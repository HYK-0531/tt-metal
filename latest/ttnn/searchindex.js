Search.setIndex({"docnames": ["index", "resources/contributing", "resources/support", "tt_metal_models/get_performance", "tt_metal_models/get_started", "ttnn/about", "ttnn/adding_new_ttnn_operation", "ttnn/api", "ttnn/api/ttnn.Conv2dConfig", "ttnn/api/ttnn.Conv2dSliceConfig", "ttnn/api/ttnn.GetDefaultDevice", "ttnn/api/ttnn.SetDefaultDevice", "ttnn/api/ttnn.abs", "ttnn/api/ttnn.abs_bw", "ttnn/api/ttnn.acos", "ttnn/api/ttnn.acos_bw", "ttnn/api/ttnn.acosh", "ttnn/api/ttnn.acosh_bw", "ttnn/api/ttnn.add", "ttnn/api/ttnn.add_bw", "ttnn/api/ttnn.addalpha", "ttnn/api/ttnn.addalpha_bw", "ttnn/api/ttnn.addcdiv", "ttnn/api/ttnn.addcdiv_bw", "ttnn/api/ttnn.addcmul", "ttnn/api/ttnn.addcmul_bw", "ttnn/api/ttnn.all_gather", "ttnn/api/ttnn.alt_complex_rotate90", "ttnn/api/ttnn.angle", "ttnn/api/ttnn.angle_bw", "ttnn/api/ttnn.arange", "ttnn/api/ttnn.argmax", "ttnn/api/ttnn.as_tensor", "ttnn/api/ttnn.asin", "ttnn/api/ttnn.asin_bw", "ttnn/api/ttnn.asinh", "ttnn/api/ttnn.asinh_bw", "ttnn/api/ttnn.assign_bw", "ttnn/api/ttnn.atan", "ttnn/api/ttnn.atan2", "ttnn/api/ttnn.atan2_bw", "ttnn/api/ttnn.atan_bw", "ttnn/api/ttnn.atanh", "ttnn/api/ttnn.atanh_bw", "ttnn/api/ttnn.batch_norm", "ttnn/api/ttnn.bias_gelu_bw", "ttnn/api/ttnn.bitwise_and", "ttnn/api/ttnn.bitwise_left_shift", "ttnn/api/ttnn.bitwise_not", "ttnn/api/ttnn.bitwise_or", "ttnn/api/ttnn.bitwise_right_shift", "ttnn/api/ttnn.bitwise_xor", "ttnn/api/ttnn.cbrt", "ttnn/api/ttnn.ceil", "ttnn/api/ttnn.ceil_bw", "ttnn/api/ttnn.celu", "ttnn/api/ttnn.celu_bw", "ttnn/api/ttnn.clamp", "ttnn/api/ttnn.clamp_bw", "ttnn/api/ttnn.clip", "ttnn/api/ttnn.clip_bw", "ttnn/api/ttnn.clone", "ttnn/api/ttnn.close_device", "ttnn/api/ttnn.concat", "ttnn/api/ttnn.concat_bw", "ttnn/api/ttnn.conj", "ttnn/api/ttnn.conj_bw", "ttnn/api/ttnn.conv1d", "ttnn/api/ttnn.conv2d", "ttnn/api/ttnn.conv_transpose2d", "ttnn/api/ttnn.cos", "ttnn/api/ttnn.cos_bw", "ttnn/api/ttnn.cosh", "ttnn/api/ttnn.cosh_bw", "ttnn/api/ttnn.create_sharded_memory_config", "ttnn/api/ttnn.deallocate", "ttnn/api/ttnn.deg2rad", "ttnn/api/ttnn.deg2rad_bw", "ttnn/api/ttnn.digamma", "ttnn/api/ttnn.digamma_bw", "ttnn/api/ttnn.div", "ttnn/api/ttnn.div_bw", "ttnn/api/ttnn.div_no_nan", "ttnn/api/ttnn.div_no_nan_bw", "ttnn/api/ttnn.downsample", "ttnn/api/ttnn.dump_tensor", "ttnn/api/ttnn.elu", "ttnn/api/ttnn.elu_bw", "ttnn/api/ttnn.embedding", "ttnn/api/ttnn.embedding_bw", "ttnn/api/ttnn.empty", "ttnn/api/ttnn.empty_like", "ttnn/api/ttnn.eq", "ttnn/api/ttnn.eq_", "ttnn/api/ttnn.eqz", "ttnn/api/ttnn.erf", "ttnn/api/ttnn.erf_bw", "ttnn/api/ttnn.erfc", "ttnn/api/ttnn.erfc_bw", "ttnn/api/ttnn.erfinv", "ttnn/api/ttnn.erfinv_bw", "ttnn/api/ttnn.exp", "ttnn/api/ttnn.exp2", "ttnn/api/ttnn.exp2_bw", "ttnn/api/ttnn.exp_bw", "ttnn/api/ttnn.experimental.all_reduce", "ttnn/api/ttnn.experimental.conv3d", "ttnn/api/ttnn.experimental.cumprod", "ttnn/api/ttnn.experimental.cumsum", "ttnn/api/ttnn.experimental.dropout", "ttnn/api/ttnn.experimental.gelu_bw", "ttnn/api/ttnn.experimental.rotary_embedding", "ttnn/api/ttnn.experimental.sort", "ttnn/api/ttnn.expm1", "ttnn/api/ttnn.expm1_bw", "ttnn/api/ttnn.fill", "ttnn/api/ttnn.fill_bw", "ttnn/api/ttnn.fill_ones_rm", "ttnn/api/ttnn.fill_rm", "ttnn/api/ttnn.fill_zero_bw", "ttnn/api/ttnn.floor", "ttnn/api/ttnn.floor_bw", "ttnn/api/ttnn.floor_div", "ttnn/api/ttnn.fmod", "ttnn/api/ttnn.fmod_bw", "ttnn/api/ttnn.format_input_tensor", "ttnn/api/ttnn.format_output_tensor", "ttnn/api/ttnn.frac", "ttnn/api/ttnn.frac_bw", "ttnn/api/ttnn.from_device", "ttnn/api/ttnn.from_torch", "ttnn/api/ttnn.full", "ttnn/api/ttnn.full_like", "ttnn/api/ttnn.gcd", "ttnn/api/ttnn.ge", "ttnn/api/ttnn.ge_", "ttnn/api/ttnn.geglu", "ttnn/api/ttnn.gelu", "ttnn/api/ttnn.gelu_bw", "ttnn/api/ttnn.gez", "ttnn/api/ttnn.global_avg_pool2d", "ttnn/api/ttnn.glu", "ttnn/api/ttnn.group_norm", "ttnn/api/ttnn.gt", "ttnn/api/ttnn.gt_", "ttnn/api/ttnn.gtz", "ttnn/api/ttnn.hardshrink", "ttnn/api/ttnn.hardshrink_bw", "ttnn/api/ttnn.hardsigmoid", "ttnn/api/ttnn.hardsigmoid_bw", "ttnn/api/ttnn.hardswish", "ttnn/api/ttnn.hardswish_bw", "ttnn/api/ttnn.hardtanh", "ttnn/api/ttnn.hardtanh_bw", "ttnn/api/ttnn.heaviside", "ttnn/api/ttnn.hypot", "ttnn/api/ttnn.hypot_bw", "ttnn/api/ttnn.i0", "ttnn/api/ttnn.i0_bw", "ttnn/api/ttnn.identity", "ttnn/api/ttnn.imag", "ttnn/api/ttnn.imag_bw", "ttnn/api/ttnn.indexed_fill", "ttnn/api/ttnn.is_imag", "ttnn/api/ttnn.is_real", "ttnn/api/ttnn.isclose", "ttnn/api/ttnn.isfinite", "ttnn/api/ttnn.isinf", "ttnn/api/ttnn.isnan", "ttnn/api/ttnn.isneginf", "ttnn/api/ttnn.isposinf", "ttnn/api/ttnn.kv_cache.fill_cache_for_user_", "ttnn/api/ttnn.kv_cache.update_cache_for_token_", "ttnn/api/ttnn.l1_loss", "ttnn/api/ttnn.layer_norm", "ttnn/api/ttnn.lcm", "ttnn/api/ttnn.ldexp", "ttnn/api/ttnn.ldexp_bw", "ttnn/api/ttnn.le", "ttnn/api/ttnn.le_", "ttnn/api/ttnn.leaky_relu", "ttnn/api/ttnn.leaky_relu_bw", "ttnn/api/ttnn.lerp", "ttnn/api/ttnn.lerp_bw", "ttnn/api/ttnn.lez", "ttnn/api/ttnn.lgamma", "ttnn/api/ttnn.lgamma_bw", "ttnn/api/ttnn.linear", "ttnn/api/ttnn.load_tensor", "ttnn/api/ttnn.log", "ttnn/api/ttnn.log10", "ttnn/api/ttnn.log10_bw", "ttnn/api/ttnn.log1p", "ttnn/api/ttnn.log1p_bw", "ttnn/api/ttnn.log2", "ttnn/api/ttnn.log2_bw", "ttnn/api/ttnn.log_bw", "ttnn/api/ttnn.log_sigmoid", "ttnn/api/ttnn.log_sigmoid_bw", "ttnn/api/ttnn.logaddexp", "ttnn/api/ttnn.logaddexp2", "ttnn/api/ttnn.logaddexp2_bw", "ttnn/api/ttnn.logaddexp_bw", "ttnn/api/ttnn.logical_and", "ttnn/api/ttnn.logical_and_", "ttnn/api/ttnn.logical_not", "ttnn/api/ttnn.logical_not_", "ttnn/api/ttnn.logical_or", "ttnn/api/ttnn.logical_or_", "ttnn/api/ttnn.logical_xor", "ttnn/api/ttnn.logical_xor_", "ttnn/api/ttnn.logit", "ttnn/api/ttnn.logit_bw", "ttnn/api/ttnn.logiteps_bw", "ttnn/api/ttnn.lt", "ttnn/api/ttnn.lt_", "ttnn/api/ttnn.ltz", "ttnn/api/ttnn.mac", "ttnn/api/ttnn.manage_device", "ttnn/api/ttnn.matmul", "ttnn/api/ttnn.max", "ttnn/api/ttnn.max_bw", "ttnn/api/ttnn.max_pool2d", "ttnn/api/ttnn.maximum", "ttnn/api/ttnn.mean", "ttnn/api/ttnn.min", "ttnn/api/ttnn.min_bw", "ttnn/api/ttnn.minimum", "ttnn/api/ttnn.mish", "ttnn/api/ttnn.model_preprocessing.preprocess_model", "ttnn/api/ttnn.model_preprocessing.preprocess_model_parameters", "ttnn/api/ttnn.moreh_sum", "ttnn/api/ttnn.mse_loss", "ttnn/api/ttnn.mul_bw", "ttnn/api/ttnn.multigammaln", "ttnn/api/ttnn.multigammaln_bw", "ttnn/api/ttnn.multiply", "ttnn/api/ttnn.ne", "ttnn/api/ttnn.ne_", "ttnn/api/ttnn.neg", "ttnn/api/ttnn.neg_bw", "ttnn/api/ttnn.nextafter", "ttnn/api/ttnn.nez", "ttnn/api/ttnn.nonzero", "ttnn/api/ttnn.normalize_global", "ttnn/api/ttnn.normalize_hw", "ttnn/api/ttnn.ones", "ttnn/api/ttnn.ones_like", "ttnn/api/ttnn.open_device", "ttnn/api/ttnn.outer", "ttnn/api/ttnn.pad", "ttnn/api/ttnn.pad_to_tile_shape", "ttnn/api/ttnn.permute", "ttnn/api/ttnn.polar", "ttnn/api/ttnn.polar_bw", "ttnn/api/ttnn.polygamma", "ttnn/api/ttnn.polygamma_bw", "ttnn/api/ttnn.polyval", "ttnn/api/ttnn.pow", "ttnn/api/ttnn.pow_bw", "ttnn/api/ttnn.prelu", "ttnn/api/ttnn.prepare_conv_bias", "ttnn/api/ttnn.prepare_conv_transpose2d_bias", "ttnn/api/ttnn.prepare_conv_transpose2d_weights", "ttnn/api/ttnn.prepare_conv_weights", "ttnn/api/ttnn.prod", "ttnn/api/ttnn.prod_bw", "ttnn/api/ttnn.rad2deg", "ttnn/api/ttnn.rad2deg_bw", "ttnn/api/ttnn.rdiv", "ttnn/api/ttnn.rdiv_bw", "ttnn/api/ttnn.real", "ttnn/api/ttnn.real_bw", "ttnn/api/ttnn.reallocate", "ttnn/api/ttnn.reciprocal", "ttnn/api/ttnn.reciprocal_bw", "ttnn/api/ttnn.reduce_scatter", "ttnn/api/ttnn.register_post_operation_hook", "ttnn/api/ttnn.register_pre_operation_hook", "ttnn/api/ttnn.reglu", "ttnn/api/ttnn.relu", "ttnn/api/ttnn.relu6", "ttnn/api/ttnn.relu6_bw", "ttnn/api/ttnn.relu_bw", "ttnn/api/ttnn.relu_max", "ttnn/api/ttnn.relu_min", "ttnn/api/ttnn.remainder", "ttnn/api/ttnn.remainder_bw", "ttnn/api/ttnn.repeat", "ttnn/api/ttnn.repeat_bw", "ttnn/api/ttnn.repeat_interleave", "ttnn/api/ttnn.reshape", "ttnn/api/ttnn.rms_norm", "ttnn/api/ttnn.round", "ttnn/api/ttnn.round_bw", "ttnn/api/ttnn.rpow", "ttnn/api/ttnn.rpow_bw", "ttnn/api/ttnn.rsqrt", "ttnn/api/ttnn.rsqrt_bw", "ttnn/api/ttnn.rsub", "ttnn/api/ttnn.rsub_bw", "ttnn/api/ttnn.scatter", "ttnn/api/ttnn.selu", "ttnn/api/ttnn.selu_bw", "ttnn/api/ttnn.set_printoptions", "ttnn/api/ttnn.sigmoid", "ttnn/api/ttnn.sigmoid_accurate", "ttnn/api/ttnn.sigmoid_bw", "ttnn/api/ttnn.sign", "ttnn/api/ttnn.sign_bw", "ttnn/api/ttnn.signbit", "ttnn/api/ttnn.silu", "ttnn/api/ttnn.silu_bw", "ttnn/api/ttnn.sin", "ttnn/api/ttnn.sin_bw", "ttnn/api/ttnn.sinh", "ttnn/api/ttnn.sinh_bw", "ttnn/api/ttnn.slice", "ttnn/api/ttnn.softmax", "ttnn/api/ttnn.softplus", "ttnn/api/ttnn.softplus_bw", "ttnn/api/ttnn.softshrink", "ttnn/api/ttnn.softshrink_bw", "ttnn/api/ttnn.softsign", "ttnn/api/ttnn.softsign_bw", "ttnn/api/ttnn.sqrt", "ttnn/api/ttnn.sqrt_bw", "ttnn/api/ttnn.square", "ttnn/api/ttnn.square_bw", "ttnn/api/ttnn.squared_difference", "ttnn/api/ttnn.squared_difference_bw", "ttnn/api/ttnn.std", "ttnn/api/ttnn.sub_bw", "ttnn/api/ttnn.subalpha", "ttnn/api/ttnn.subalpha_bw", "ttnn/api/ttnn.subtract", "ttnn/api/ttnn.sum", "ttnn/api/ttnn.swiglu", "ttnn/api/ttnn.swish", "ttnn/api/ttnn.synchronize_device", "ttnn/api/ttnn.tan", "ttnn/api/ttnn.tan_bw", "ttnn/api/ttnn.tanh", "ttnn/api/ttnn.tanh_bw", "ttnn/api/ttnn.tanhshrink", "ttnn/api/ttnn.tanhshrink_bw", "ttnn/api/ttnn.threshold", "ttnn/api/ttnn.threshold_bw", "ttnn/api/ttnn.tilize", "ttnn/api/ttnn.tilize_with_val_padding", "ttnn/api/ttnn.to_device", "ttnn/api/ttnn.to_layout", "ttnn/api/ttnn.to_memory_config", "ttnn/api/ttnn.to_torch", "ttnn/api/ttnn.topk", "ttnn/api/ttnn.transformer.attention_softmax", "ttnn/api/ttnn.transformer.attention_softmax_", "ttnn/api/ttnn.transformer.concatenate_heads", "ttnn/api/ttnn.transformer.scaled_dot_product_attention", "ttnn/api/ttnn.transformer.scaled_dot_product_attention_decode", "ttnn/api/ttnn.transformer.split_query_key_value_and_split_heads", "ttnn/api/ttnn.tril", "ttnn/api/ttnn.triu", "ttnn/api/ttnn.trunc", "ttnn/api/ttnn.trunc_bw", "ttnn/api/ttnn.unary_chain", "ttnn/api/ttnn.untilize", "ttnn/api/ttnn.untilize_with_unpadding", "ttnn/api/ttnn.upsample", "ttnn/api/ttnn.var", "ttnn/api/ttnn.where", "ttnn/api/ttnn.where_bw", "ttnn/api/ttnn.xlogy", "ttnn/api/ttnn.xlogy_bw", "ttnn/api/ttnn.zeros", "ttnn/api/ttnn.zeros_like", "ttnn/converting_torch_model_to_ttnn", "ttnn/demos", "ttnn/dependencies/examples", "ttnn/dependencies/index", "ttnn/dependencies/tensor", "ttnn/dependencies/tt_lib", "ttnn/get_started", "ttnn/installing", "ttnn/onboarding", "ttnn/profiling_ttnn_operations", "ttnn/tensor", "ttnn/tutorials", "ttnn/tutorials/graphing_torch_dit", "ttnn/tutorials/matmul", "ttnn/tutorials/multihead-attention", "ttnn/tutorials/profiling", "ttnn/tutorials/resnet-basic-block", "ttnn/tutorials/tensor_and_add_operation", "ttnn/tutorials/ttnn-tracer", "ttnn/tutorials/ttnn_tutorials/001", "ttnn/tutorials/ttnn_tutorials/002", "ttnn/tutorials/ttnn_tutorials/003", "ttnn/tutorials/ttnn_tutorials/004", "ttnn/tutorials/ttnn_tutorials/005", "ttnn/tutorials/ttnn_tutorials/006", "ttnn/tutorials/ttnn_tutorials/007", "ttnn/usage"], "filenames": ["index.rst", "resources/contributing.rst", "resources/support.rst", "tt_metal_models/get_performance.rst", "tt_metal_models/get_started.rst", "ttnn/about.rst", "ttnn/adding_new_ttnn_operation.rst", "ttnn/api.rst", "ttnn/api/ttnn.Conv2dConfig.rst", "ttnn/api/ttnn.Conv2dSliceConfig.rst", "ttnn/api/ttnn.GetDefaultDevice.rst", "ttnn/api/ttnn.SetDefaultDevice.rst", "ttnn/api/ttnn.abs.rst", "ttnn/api/ttnn.abs_bw.rst", "ttnn/api/ttnn.acos.rst", "ttnn/api/ttnn.acos_bw.rst", "ttnn/api/ttnn.acosh.rst", "ttnn/api/ttnn.acosh_bw.rst", "ttnn/api/ttnn.add.rst", "ttnn/api/ttnn.add_bw.rst", "ttnn/api/ttnn.addalpha.rst", "ttnn/api/ttnn.addalpha_bw.rst", "ttnn/api/ttnn.addcdiv.rst", "ttnn/api/ttnn.addcdiv_bw.rst", "ttnn/api/ttnn.addcmul.rst", "ttnn/api/ttnn.addcmul_bw.rst", "ttnn/api/ttnn.all_gather.rst", "ttnn/api/ttnn.alt_complex_rotate90.rst", "ttnn/api/ttnn.angle.rst", "ttnn/api/ttnn.angle_bw.rst", "ttnn/api/ttnn.arange.rst", "ttnn/api/ttnn.argmax.rst", "ttnn/api/ttnn.as_tensor.rst", "ttnn/api/ttnn.asin.rst", "ttnn/api/ttnn.asin_bw.rst", "ttnn/api/ttnn.asinh.rst", "ttnn/api/ttnn.asinh_bw.rst", "ttnn/api/ttnn.assign_bw.rst", "ttnn/api/ttnn.atan.rst", "ttnn/api/ttnn.atan2.rst", "ttnn/api/ttnn.atan2_bw.rst", "ttnn/api/ttnn.atan_bw.rst", "ttnn/api/ttnn.atanh.rst", "ttnn/api/ttnn.atanh_bw.rst", "ttnn/api/ttnn.batch_norm.rst", "ttnn/api/ttnn.bias_gelu_bw.rst", "ttnn/api/ttnn.bitwise_and.rst", "ttnn/api/ttnn.bitwise_left_shift.rst", "ttnn/api/ttnn.bitwise_not.rst", "ttnn/api/ttnn.bitwise_or.rst", "ttnn/api/ttnn.bitwise_right_shift.rst", "ttnn/api/ttnn.bitwise_xor.rst", "ttnn/api/ttnn.cbrt.rst", "ttnn/api/ttnn.ceil.rst", "ttnn/api/ttnn.ceil_bw.rst", "ttnn/api/ttnn.celu.rst", "ttnn/api/ttnn.celu_bw.rst", "ttnn/api/ttnn.clamp.rst", "ttnn/api/ttnn.clamp_bw.rst", "ttnn/api/ttnn.clip.rst", "ttnn/api/ttnn.clip_bw.rst", "ttnn/api/ttnn.clone.rst", "ttnn/api/ttnn.close_device.rst", "ttnn/api/ttnn.concat.rst", "ttnn/api/ttnn.concat_bw.rst", "ttnn/api/ttnn.conj.rst", "ttnn/api/ttnn.conj_bw.rst", "ttnn/api/ttnn.conv1d.rst", "ttnn/api/ttnn.conv2d.rst", "ttnn/api/ttnn.conv_transpose2d.rst", "ttnn/api/ttnn.cos.rst", "ttnn/api/ttnn.cos_bw.rst", "ttnn/api/ttnn.cosh.rst", "ttnn/api/ttnn.cosh_bw.rst", "ttnn/api/ttnn.create_sharded_memory_config.rst", "ttnn/api/ttnn.deallocate.rst", "ttnn/api/ttnn.deg2rad.rst", "ttnn/api/ttnn.deg2rad_bw.rst", "ttnn/api/ttnn.digamma.rst", "ttnn/api/ttnn.digamma_bw.rst", "ttnn/api/ttnn.div.rst", "ttnn/api/ttnn.div_bw.rst", "ttnn/api/ttnn.div_no_nan.rst", "ttnn/api/ttnn.div_no_nan_bw.rst", "ttnn/api/ttnn.downsample.rst", "ttnn/api/ttnn.dump_tensor.rst", "ttnn/api/ttnn.elu.rst", "ttnn/api/ttnn.elu_bw.rst", "ttnn/api/ttnn.embedding.rst", "ttnn/api/ttnn.embedding_bw.rst", "ttnn/api/ttnn.empty.rst", "ttnn/api/ttnn.empty_like.rst", "ttnn/api/ttnn.eq.rst", "ttnn/api/ttnn.eq_.rst", "ttnn/api/ttnn.eqz.rst", "ttnn/api/ttnn.erf.rst", "ttnn/api/ttnn.erf_bw.rst", "ttnn/api/ttnn.erfc.rst", "ttnn/api/ttnn.erfc_bw.rst", "ttnn/api/ttnn.erfinv.rst", "ttnn/api/ttnn.erfinv_bw.rst", "ttnn/api/ttnn.exp.rst", "ttnn/api/ttnn.exp2.rst", "ttnn/api/ttnn.exp2_bw.rst", "ttnn/api/ttnn.exp_bw.rst", "ttnn/api/ttnn.experimental.all_reduce.rst", "ttnn/api/ttnn.experimental.conv3d.rst", "ttnn/api/ttnn.experimental.cumprod.rst", "ttnn/api/ttnn.experimental.cumsum.rst", "ttnn/api/ttnn.experimental.dropout.rst", "ttnn/api/ttnn.experimental.gelu_bw.rst", "ttnn/api/ttnn.experimental.rotary_embedding.rst", "ttnn/api/ttnn.experimental.sort.rst", "ttnn/api/ttnn.expm1.rst", "ttnn/api/ttnn.expm1_bw.rst", "ttnn/api/ttnn.fill.rst", "ttnn/api/ttnn.fill_bw.rst", "ttnn/api/ttnn.fill_ones_rm.rst", "ttnn/api/ttnn.fill_rm.rst", "ttnn/api/ttnn.fill_zero_bw.rst", "ttnn/api/ttnn.floor.rst", "ttnn/api/ttnn.floor_bw.rst", "ttnn/api/ttnn.floor_div.rst", "ttnn/api/ttnn.fmod.rst", "ttnn/api/ttnn.fmod_bw.rst", "ttnn/api/ttnn.format_input_tensor.rst", "ttnn/api/ttnn.format_output_tensor.rst", "ttnn/api/ttnn.frac.rst", "ttnn/api/ttnn.frac_bw.rst", "ttnn/api/ttnn.from_device.rst", "ttnn/api/ttnn.from_torch.rst", "ttnn/api/ttnn.full.rst", "ttnn/api/ttnn.full_like.rst", "ttnn/api/ttnn.gcd.rst", "ttnn/api/ttnn.ge.rst", "ttnn/api/ttnn.ge_.rst", "ttnn/api/ttnn.geglu.rst", "ttnn/api/ttnn.gelu.rst", "ttnn/api/ttnn.gelu_bw.rst", "ttnn/api/ttnn.gez.rst", "ttnn/api/ttnn.global_avg_pool2d.rst", "ttnn/api/ttnn.glu.rst", "ttnn/api/ttnn.group_norm.rst", "ttnn/api/ttnn.gt.rst", "ttnn/api/ttnn.gt_.rst", "ttnn/api/ttnn.gtz.rst", "ttnn/api/ttnn.hardshrink.rst", "ttnn/api/ttnn.hardshrink_bw.rst", "ttnn/api/ttnn.hardsigmoid.rst", "ttnn/api/ttnn.hardsigmoid_bw.rst", "ttnn/api/ttnn.hardswish.rst", "ttnn/api/ttnn.hardswish_bw.rst", "ttnn/api/ttnn.hardtanh.rst", "ttnn/api/ttnn.hardtanh_bw.rst", "ttnn/api/ttnn.heaviside.rst", "ttnn/api/ttnn.hypot.rst", "ttnn/api/ttnn.hypot_bw.rst", "ttnn/api/ttnn.i0.rst", "ttnn/api/ttnn.i0_bw.rst", "ttnn/api/ttnn.identity.rst", "ttnn/api/ttnn.imag.rst", "ttnn/api/ttnn.imag_bw.rst", "ttnn/api/ttnn.indexed_fill.rst", "ttnn/api/ttnn.is_imag.rst", "ttnn/api/ttnn.is_real.rst", "ttnn/api/ttnn.isclose.rst", "ttnn/api/ttnn.isfinite.rst", "ttnn/api/ttnn.isinf.rst", "ttnn/api/ttnn.isnan.rst", "ttnn/api/ttnn.isneginf.rst", "ttnn/api/ttnn.isposinf.rst", "ttnn/api/ttnn.kv_cache.fill_cache_for_user_.rst", "ttnn/api/ttnn.kv_cache.update_cache_for_token_.rst", "ttnn/api/ttnn.l1_loss.rst", "ttnn/api/ttnn.layer_norm.rst", "ttnn/api/ttnn.lcm.rst", "ttnn/api/ttnn.ldexp.rst", "ttnn/api/ttnn.ldexp_bw.rst", "ttnn/api/ttnn.le.rst", "ttnn/api/ttnn.le_.rst", "ttnn/api/ttnn.leaky_relu.rst", "ttnn/api/ttnn.leaky_relu_bw.rst", "ttnn/api/ttnn.lerp.rst", "ttnn/api/ttnn.lerp_bw.rst", "ttnn/api/ttnn.lez.rst", "ttnn/api/ttnn.lgamma.rst", "ttnn/api/ttnn.lgamma_bw.rst", "ttnn/api/ttnn.linear.rst", "ttnn/api/ttnn.load_tensor.rst", "ttnn/api/ttnn.log.rst", "ttnn/api/ttnn.log10.rst", "ttnn/api/ttnn.log10_bw.rst", "ttnn/api/ttnn.log1p.rst", "ttnn/api/ttnn.log1p_bw.rst", "ttnn/api/ttnn.log2.rst", "ttnn/api/ttnn.log2_bw.rst", "ttnn/api/ttnn.log_bw.rst", "ttnn/api/ttnn.log_sigmoid.rst", "ttnn/api/ttnn.log_sigmoid_bw.rst", "ttnn/api/ttnn.logaddexp.rst", "ttnn/api/ttnn.logaddexp2.rst", "ttnn/api/ttnn.logaddexp2_bw.rst", "ttnn/api/ttnn.logaddexp_bw.rst", "ttnn/api/ttnn.logical_and.rst", "ttnn/api/ttnn.logical_and_.rst", "ttnn/api/ttnn.logical_not.rst", "ttnn/api/ttnn.logical_not_.rst", "ttnn/api/ttnn.logical_or.rst", "ttnn/api/ttnn.logical_or_.rst", "ttnn/api/ttnn.logical_xor.rst", "ttnn/api/ttnn.logical_xor_.rst", "ttnn/api/ttnn.logit.rst", "ttnn/api/ttnn.logit_bw.rst", "ttnn/api/ttnn.logiteps_bw.rst", "ttnn/api/ttnn.lt.rst", "ttnn/api/ttnn.lt_.rst", "ttnn/api/ttnn.ltz.rst", "ttnn/api/ttnn.mac.rst", "ttnn/api/ttnn.manage_device.rst", "ttnn/api/ttnn.matmul.rst", "ttnn/api/ttnn.max.rst", "ttnn/api/ttnn.max_bw.rst", "ttnn/api/ttnn.max_pool2d.rst", "ttnn/api/ttnn.maximum.rst", "ttnn/api/ttnn.mean.rst", "ttnn/api/ttnn.min.rst", "ttnn/api/ttnn.min_bw.rst", "ttnn/api/ttnn.minimum.rst", "ttnn/api/ttnn.mish.rst", "ttnn/api/ttnn.model_preprocessing.preprocess_model.rst", "ttnn/api/ttnn.model_preprocessing.preprocess_model_parameters.rst", "ttnn/api/ttnn.moreh_sum.rst", "ttnn/api/ttnn.mse_loss.rst", "ttnn/api/ttnn.mul_bw.rst", "ttnn/api/ttnn.multigammaln.rst", "ttnn/api/ttnn.multigammaln_bw.rst", "ttnn/api/ttnn.multiply.rst", "ttnn/api/ttnn.ne.rst", "ttnn/api/ttnn.ne_.rst", "ttnn/api/ttnn.neg.rst", "ttnn/api/ttnn.neg_bw.rst", "ttnn/api/ttnn.nextafter.rst", "ttnn/api/ttnn.nez.rst", "ttnn/api/ttnn.nonzero.rst", "ttnn/api/ttnn.normalize_global.rst", "ttnn/api/ttnn.normalize_hw.rst", "ttnn/api/ttnn.ones.rst", "ttnn/api/ttnn.ones_like.rst", "ttnn/api/ttnn.open_device.rst", "ttnn/api/ttnn.outer.rst", "ttnn/api/ttnn.pad.rst", "ttnn/api/ttnn.pad_to_tile_shape.rst", "ttnn/api/ttnn.permute.rst", "ttnn/api/ttnn.polar.rst", "ttnn/api/ttnn.polar_bw.rst", "ttnn/api/ttnn.polygamma.rst", "ttnn/api/ttnn.polygamma_bw.rst", "ttnn/api/ttnn.polyval.rst", "ttnn/api/ttnn.pow.rst", "ttnn/api/ttnn.pow_bw.rst", "ttnn/api/ttnn.prelu.rst", "ttnn/api/ttnn.prepare_conv_bias.rst", "ttnn/api/ttnn.prepare_conv_transpose2d_bias.rst", "ttnn/api/ttnn.prepare_conv_transpose2d_weights.rst", "ttnn/api/ttnn.prepare_conv_weights.rst", "ttnn/api/ttnn.prod.rst", "ttnn/api/ttnn.prod_bw.rst", "ttnn/api/ttnn.rad2deg.rst", "ttnn/api/ttnn.rad2deg_bw.rst", "ttnn/api/ttnn.rdiv.rst", "ttnn/api/ttnn.rdiv_bw.rst", "ttnn/api/ttnn.real.rst", "ttnn/api/ttnn.real_bw.rst", "ttnn/api/ttnn.reallocate.rst", "ttnn/api/ttnn.reciprocal.rst", "ttnn/api/ttnn.reciprocal_bw.rst", "ttnn/api/ttnn.reduce_scatter.rst", "ttnn/api/ttnn.register_post_operation_hook.rst", "ttnn/api/ttnn.register_pre_operation_hook.rst", "ttnn/api/ttnn.reglu.rst", "ttnn/api/ttnn.relu.rst", "ttnn/api/ttnn.relu6.rst", "ttnn/api/ttnn.relu6_bw.rst", "ttnn/api/ttnn.relu_bw.rst", "ttnn/api/ttnn.relu_max.rst", "ttnn/api/ttnn.relu_min.rst", "ttnn/api/ttnn.remainder.rst", "ttnn/api/ttnn.remainder_bw.rst", "ttnn/api/ttnn.repeat.rst", "ttnn/api/ttnn.repeat_bw.rst", "ttnn/api/ttnn.repeat_interleave.rst", "ttnn/api/ttnn.reshape.rst", "ttnn/api/ttnn.rms_norm.rst", "ttnn/api/ttnn.round.rst", "ttnn/api/ttnn.round_bw.rst", "ttnn/api/ttnn.rpow.rst", "ttnn/api/ttnn.rpow_bw.rst", "ttnn/api/ttnn.rsqrt.rst", "ttnn/api/ttnn.rsqrt_bw.rst", "ttnn/api/ttnn.rsub.rst", "ttnn/api/ttnn.rsub_bw.rst", "ttnn/api/ttnn.scatter.rst", "ttnn/api/ttnn.selu.rst", "ttnn/api/ttnn.selu_bw.rst", "ttnn/api/ttnn.set_printoptions.rst", "ttnn/api/ttnn.sigmoid.rst", "ttnn/api/ttnn.sigmoid_accurate.rst", "ttnn/api/ttnn.sigmoid_bw.rst", "ttnn/api/ttnn.sign.rst", "ttnn/api/ttnn.sign_bw.rst", "ttnn/api/ttnn.signbit.rst", "ttnn/api/ttnn.silu.rst", "ttnn/api/ttnn.silu_bw.rst", "ttnn/api/ttnn.sin.rst", "ttnn/api/ttnn.sin_bw.rst", "ttnn/api/ttnn.sinh.rst", "ttnn/api/ttnn.sinh_bw.rst", "ttnn/api/ttnn.slice.rst", "ttnn/api/ttnn.softmax.rst", "ttnn/api/ttnn.softplus.rst", "ttnn/api/ttnn.softplus_bw.rst", "ttnn/api/ttnn.softshrink.rst", "ttnn/api/ttnn.softshrink_bw.rst", "ttnn/api/ttnn.softsign.rst", "ttnn/api/ttnn.softsign_bw.rst", "ttnn/api/ttnn.sqrt.rst", "ttnn/api/ttnn.sqrt_bw.rst", "ttnn/api/ttnn.square.rst", "ttnn/api/ttnn.square_bw.rst", "ttnn/api/ttnn.squared_difference.rst", "ttnn/api/ttnn.squared_difference_bw.rst", "ttnn/api/ttnn.std.rst", "ttnn/api/ttnn.sub_bw.rst", "ttnn/api/ttnn.subalpha.rst", "ttnn/api/ttnn.subalpha_bw.rst", "ttnn/api/ttnn.subtract.rst", "ttnn/api/ttnn.sum.rst", "ttnn/api/ttnn.swiglu.rst", "ttnn/api/ttnn.swish.rst", "ttnn/api/ttnn.synchronize_device.rst", "ttnn/api/ttnn.tan.rst", "ttnn/api/ttnn.tan_bw.rst", "ttnn/api/ttnn.tanh.rst", "ttnn/api/ttnn.tanh_bw.rst", "ttnn/api/ttnn.tanhshrink.rst", "ttnn/api/ttnn.tanhshrink_bw.rst", "ttnn/api/ttnn.threshold.rst", "ttnn/api/ttnn.threshold_bw.rst", "ttnn/api/ttnn.tilize.rst", "ttnn/api/ttnn.tilize_with_val_padding.rst", "ttnn/api/ttnn.to_device.rst", "ttnn/api/ttnn.to_layout.rst", "ttnn/api/ttnn.to_memory_config.rst", "ttnn/api/ttnn.to_torch.rst", "ttnn/api/ttnn.topk.rst", "ttnn/api/ttnn.transformer.attention_softmax.rst", "ttnn/api/ttnn.transformer.attention_softmax_.rst", "ttnn/api/ttnn.transformer.concatenate_heads.rst", "ttnn/api/ttnn.transformer.scaled_dot_product_attention.rst", "ttnn/api/ttnn.transformer.scaled_dot_product_attention_decode.rst", "ttnn/api/ttnn.transformer.split_query_key_value_and_split_heads.rst", "ttnn/api/ttnn.tril.rst", "ttnn/api/ttnn.triu.rst", "ttnn/api/ttnn.trunc.rst", "ttnn/api/ttnn.trunc_bw.rst", "ttnn/api/ttnn.unary_chain.rst", "ttnn/api/ttnn.untilize.rst", "ttnn/api/ttnn.untilize_with_unpadding.rst", "ttnn/api/ttnn.upsample.rst", "ttnn/api/ttnn.var.rst", "ttnn/api/ttnn.where.rst", "ttnn/api/ttnn.where_bw.rst", "ttnn/api/ttnn.xlogy.rst", "ttnn/api/ttnn.xlogy_bw.rst", "ttnn/api/ttnn.zeros.rst", "ttnn/api/ttnn.zeros_like.rst", "ttnn/converting_torch_model_to_ttnn.rst", "ttnn/demos.rst", "ttnn/dependencies/examples.rst", "ttnn/dependencies/index.rst", "ttnn/dependencies/tensor.rst", "ttnn/dependencies/tt_lib.rst", "ttnn/get_started.rst", "ttnn/installing.md", "ttnn/onboarding.rst", "ttnn/profiling_ttnn_operations.rst", "ttnn/tensor.rst", "ttnn/tutorials.rst", "ttnn/tutorials/graphing_torch_dit.rst", "ttnn/tutorials/matmul.rst", "ttnn/tutorials/multihead-attention.rst", "ttnn/tutorials/profiling.rst", "ttnn/tutorials/resnet-basic-block.rst", "ttnn/tutorials/tensor_and_add_operation.rst", "ttnn/tutorials/ttnn-tracer.rst", "ttnn/tutorials/ttnn_tutorials/001.ipynb", "ttnn/tutorials/ttnn_tutorials/002.ipynb", "ttnn/tutorials/ttnn_tutorials/003.ipynb", "ttnn/tutorials/ttnn_tutorials/004.ipynb", "ttnn/tutorials/ttnn_tutorials/005.ipynb", "ttnn/tutorials/ttnn_tutorials/006.ipynb", "ttnn/tutorials/ttnn_tutorials/007.ipynb", "ttnn/usage.rst"], "titles": ["Welcome to TT-NN documentation!", "Contributing as a developer", "Support", "Performance", "Getting Started", "What is TT-NN?", "Adding New TT-NN Operation", "APIs", "ttnn.Conv2dConfig", "ttnn.Conv2dSliceConfig", "ttnn.GetDefaultDevice", "ttnn.SetDefaultDevice", "ttnn.abs", "ttnn.abs_bw", "ttnn.acos", "ttnn.acos_bw", "ttnn.acosh", "ttnn.acosh_bw", "ttnn.add", "ttnn.add_bw", "ttnn.addalpha", "ttnn.addalpha_bw", "ttnn.addcdiv", "ttnn.addcdiv_bw", "ttnn.addcmul", "ttnn.addcmul_bw", "ttnn.all_gather", "ttnn.alt_complex_rotate90", "ttnn.angle", "ttnn.angle_bw", "ttnn.arange", "ttnn.argmax", "ttnn.as_tensor", "ttnn.asin", "ttnn.asin_bw", "ttnn.asinh", "ttnn.asinh_bw", "ttnn.assign_bw", "ttnn.atan", "ttnn.atan2", "ttnn.atan2_bw", "ttnn.atan_bw", "ttnn.atanh", "ttnn.atanh_bw", "ttnn.batch_norm", "ttnn.bias_gelu_bw", "ttnn.bitwise_and", "ttnn.bitwise_left_shift", "ttnn.bitwise_not", "ttnn.bitwise_or", "ttnn.bitwise_right_shift", "ttnn.bitwise_xor", "ttnn.cbrt", "ttnn.ceil", "ttnn.ceil_bw", "ttnn.celu", "ttnn.celu_bw", "ttnn.clamp", "ttnn.clamp_bw", "ttnn.clip", "ttnn.clip_bw", "ttnn.clone", "ttnn.close_device", "ttnn.concat", "ttnn.concat_bw", "ttnn.conj", "ttnn.conj_bw", "ttnn.conv1d", "ttnn.conv2d", "ttnn.conv_transpose2d", "ttnn.cos", "ttnn.cos_bw", "ttnn.cosh", "ttnn.cosh_bw", "ttnn.create_sharded_memory_config", "ttnn.deallocate", "ttnn.deg2rad", "ttnn.deg2rad_bw", "ttnn.digamma", "ttnn.digamma_bw", "ttnn.div", "ttnn.div_bw", "ttnn.div_no_nan", "ttnn.div_no_nan_bw", "ttnn.downsample", "ttnn.dump_tensor", "ttnn.elu", "ttnn.elu_bw", "ttnn.embedding", "ttnn.embedding_bw", "ttnn.empty", "ttnn.empty_like", "ttnn.eq", "ttnn.eq_", "ttnn.eqz", "ttnn.erf", "ttnn.erf_bw", "ttnn.erfc", "ttnn.erfc_bw", "ttnn.erfinv", "ttnn.erfinv_bw", "ttnn.exp", "ttnn.exp2", "ttnn.exp2_bw", "ttnn.exp_bw", "ttnn.experimental.all_reduce", "ttnn.experimental.conv3d", "ttnn.experimental.cumprod", "ttnn.experimental.cumsum", "ttnn.experimental.dropout", "ttnn.experimental.gelu_bw", "ttnn.experimental.rotary_embedding", "ttnn.experimental.sort", "ttnn.expm1", "ttnn.expm1_bw", "ttnn.fill", "ttnn.fill_bw", "ttnn.fill_ones_rm", "ttnn.fill_rm", "ttnn.fill_zero_bw", "ttnn.floor", "ttnn.floor_bw", "ttnn.floor_div", "ttnn.fmod", "ttnn.fmod_bw", "ttnn.format_input_tensor", "ttnn.format_output_tensor", "ttnn.frac", "ttnn.frac_bw", "ttnn.from_device", "ttnn.from_torch", "ttnn.full", "ttnn.full_like", "ttnn.gcd", "ttnn.ge", "ttnn.ge_", "ttnn.geglu", "ttnn.gelu", "ttnn.gelu_bw", "ttnn.gez", "ttnn.global_avg_pool2d", "ttnn.glu", "ttnn.group_norm", "ttnn.gt", "ttnn.gt_", "ttnn.gtz", "ttnn.hardshrink", "ttnn.hardshrink_bw", "ttnn.hardsigmoid", "ttnn.hardsigmoid_bw", "ttnn.hardswish", "ttnn.hardswish_bw", "ttnn.hardtanh", "ttnn.hardtanh_bw", "ttnn.heaviside", "ttnn.hypot", "ttnn.hypot_bw", "ttnn.i0", "ttnn.i0_bw", "ttnn.identity", "ttnn.imag", "ttnn.imag_bw", "ttnn.indexed_fill", "ttnn.is_imag", "ttnn.is_real", "ttnn.isclose", "ttnn.isfinite", "ttnn.isinf", "ttnn.isnan", "ttnn.isneginf", "ttnn.isposinf", "ttnn.kv_cache.fill_cache_for_user_", "ttnn.kv_cache.update_cache_for_token_", "ttnn.l1_loss", "ttnn.layer_norm", "ttnn.lcm", "ttnn.ldexp", "ttnn.ldexp_bw", "ttnn.le", "ttnn.le_", "ttnn.leaky_relu", "ttnn.leaky_relu_bw", "ttnn.lerp", "ttnn.lerp_bw", "ttnn.lez", "ttnn.lgamma", "ttnn.lgamma_bw", "ttnn.linear", "ttnn.load_tensor", "ttnn.log", "ttnn.log10", "ttnn.log10_bw", "ttnn.log1p", "ttnn.log1p_bw", "ttnn.log2", "ttnn.log2_bw", "ttnn.log_bw", "ttnn.log_sigmoid", "ttnn.log_sigmoid_bw", "ttnn.logaddexp", "ttnn.logaddexp2", "ttnn.logaddexp2_bw", "ttnn.logaddexp_bw", "ttnn.logical_and", "ttnn.logical_and_", "ttnn.logical_not", "ttnn.logical_not_", "ttnn.logical_or", "ttnn.logical_or_", "ttnn.logical_xor", "ttnn.logical_xor_", "ttnn.logit", "ttnn.logit_bw", "ttnn.logiteps_bw", "ttnn.lt", "ttnn.lt_", "ttnn.ltz", "ttnn.mac", "ttnn.manage_device", "ttnn.matmul", "ttnn.max", "ttnn.max_bw", "ttnn.max_pool2d", "ttnn.maximum", "ttnn.mean", "ttnn.min", "ttnn.min_bw", "ttnn.minimum", "ttnn.mish", "ttnn.model_preprocessing.preprocess_model", "ttnn.model_preprocessing.preprocess_model_parameters", "ttnn.moreh_sum", "ttnn.mse_loss", "ttnn.mul_bw", "ttnn.multigammaln", "ttnn.multigammaln_bw", "ttnn.multiply", "ttnn.ne", "ttnn.ne_", "ttnn.neg", "ttnn.neg_bw", "ttnn.nextafter", "ttnn.nez", "ttnn.nonzero", "ttnn.normalize_global", "ttnn.normalize_hw", "ttnn.ones", "ttnn.ones_like", "ttnn.open_device", "ttnn.outer", "ttnn.pad", "ttnn.pad_to_tile_shape", "ttnn.permute", "ttnn.polar", "ttnn.polar_bw", "ttnn.polygamma", "ttnn.polygamma_bw", "ttnn.polyval", "ttnn.pow", "ttnn.pow_bw", "ttnn.prelu", "ttnn.prepare_conv_bias", "ttnn.prepare_conv_transpose2d_bias", "ttnn.prepare_conv_transpose2d_weights", "ttnn.prepare_conv_weights", "ttnn.prod", "ttnn.prod_bw", "ttnn.rad2deg", "ttnn.rad2deg_bw", "ttnn.rdiv", "ttnn.rdiv_bw", "ttnn.real", "ttnn.real_bw", "ttnn.reallocate", "ttnn.reciprocal", "ttnn.reciprocal_bw", "ttnn.reduce_scatter", "ttnn.register_post_operation_hook", "ttnn.register_pre_operation_hook", "ttnn.reglu", "ttnn.relu", "ttnn.relu6", "ttnn.relu6_bw", "ttnn.relu_bw", "ttnn.relu_max", "ttnn.relu_min", "ttnn.remainder", "ttnn.remainder_bw", "ttnn.repeat", "ttnn.repeat_bw", "ttnn.repeat_interleave", "ttnn.reshape", "ttnn.rms_norm", "ttnn.round", "ttnn.round_bw", "ttnn.rpow", "ttnn.rpow_bw", "ttnn.rsqrt", "ttnn.rsqrt_bw", "ttnn.rsub", "ttnn.rsub_bw", "ttnn.scatter", "ttnn.selu", "ttnn.selu_bw", "ttnn.set_printoptions", "ttnn.sigmoid", "ttnn.sigmoid_accurate", "ttnn.sigmoid_bw", "ttnn.sign", "ttnn.sign_bw", "ttnn.signbit", "ttnn.silu", "ttnn.silu_bw", "ttnn.sin", "ttnn.sin_bw", "ttnn.sinh", "ttnn.sinh_bw", "ttnn.slice", "ttnn.softmax", "ttnn.softplus", "ttnn.softplus_bw", "ttnn.softshrink", "ttnn.softshrink_bw", "ttnn.softsign", "ttnn.softsign_bw", "ttnn.sqrt", "ttnn.sqrt_bw", "ttnn.square", "ttnn.square_bw", "ttnn.squared_difference", "ttnn.squared_difference_bw", "ttnn.std", "ttnn.sub_bw", "ttnn.subalpha", "ttnn.subalpha_bw", "ttnn.subtract", "ttnn.sum", "ttnn.swiglu", "ttnn.swish", "ttnn.synchronize_device", "ttnn.tan", "ttnn.tan_bw", "ttnn.tanh", "ttnn.tanh_bw", "ttnn.tanhshrink", "ttnn.tanhshrink_bw", "ttnn.threshold", "ttnn.threshold_bw", "ttnn.tilize", "ttnn.tilize_with_val_padding", "ttnn.to_device", "ttnn.to_layout", "ttnn.to_memory_config", "ttnn.to_torch", "ttnn.topk", "ttnn.transformer.attention_softmax", "ttnn.transformer.attention_softmax_", "ttnn.transformer.concatenate_heads", "ttnn.transformer.scaled_dot_product_attention", "ttnn.transformer.scaled_dot_product_attention_decode", "ttnn.transformer.split_query_key_value_and_split_heads", "ttnn.tril", "ttnn.triu", "ttnn.trunc", "ttnn.trunc_bw", "ttnn.unary_chain", "ttnn.untilize", "ttnn.untilize_with_unpadding", "ttnn.upsample", "ttnn.var", "ttnn.where", "ttnn.where_bw", "ttnn.xlogy", "ttnn.xlogy_bw", "ttnn.zeros", "ttnn.zeros_like", "Converting PyTorch Model to TT-NN", "Building and Uplifting Demos", "Examples of Tensor and TT-LIB Use", "Dependencies", "Tensor", "TT-LIB", "Getting Started", "Install", "Onboarding New Functionality", "Profiling TT-NN Operations", "Tensor", "Tutorials", "Graphing Torch DiT_XL_2 With TTNN", "Matmul Operation", "Multi-Head Attention", "ttnn Profiling", "Resnet Basic Block", "Tensor and Add Operation", "ttnn Tracer", "Tensor and Add Operation", "Matrix Multiplication", "Multi-Head Attention", "Tracing ttnn operations and torch modules/functions", "Profiling ttnn operations", "Resnet Block", "Build a graph of a pytorch based model", "Using TT-NN"], "terms": {"what": [0, 377, 385, 401], "i": [0, 3, 4, 8, 9, 10, 11, 16, 19, 21, 31, 35, 40, 42, 44, 45, 48, 62, 63, 64, 69, 74, 80, 81, 84, 88, 89, 92, 105, 107, 108, 111, 112, 115, 117, 118, 125, 126, 128, 130, 134, 136, 140, 141, 143, 156, 158, 163, 164, 177, 178, 183, 187, 190, 194, 201, 202, 203, 214, 218, 219, 221, 222, 226, 229, 230, 233, 235, 237, 248, 250, 251, 253, 255, 257, 258, 260, 265, 266, 269, 274, 276, 277, 278, 279, 284, 285, 290, 291, 295, 300, 317, 318, 330, 332, 334, 337, 339, 340, 341, 351, 353, 354, 355, 356, 358, 359, 360, 368, 373, 376, 377, 378, 380, 381, 382, 383, 384, 385, 386, 387, 388, 391, 392, 395, 396, 397, 398, 399, 400, 401, 402], "get": [0, 8, 10, 305, 357, 376, 378, 380, 383, 387, 392, 395, 396, 397, 398, 399], "start": [0, 30, 182, 317, 376, 380, 381, 383, 385, 397, 399], "1": [0, 3, 8, 9, 12, 13, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 34, 36, 37, 39, 40, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 54, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 69, 71, 73, 77, 79, 80, 81, 82, 83, 86, 87, 88, 89, 92, 93, 95, 96, 97, 98, 100, 101, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 114, 115, 116, 117, 118, 119, 121, 122, 123, 124, 125, 126, 128, 130, 131, 133, 134, 135, 136, 137, 138, 140, 141, 143, 144, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 158, 159, 160, 161, 162, 163, 164, 165, 173, 175, 176, 177, 178, 179, 180, 181, 182, 183, 186, 191, 192, 193, 195, 196, 198, 199, 200, 201, 202, 203, 204, 207, 208, 209, 210, 211, 212, 213, 214, 215, 217, 219, 220, 221, 222, 223, 224, 225, 226, 227, 232, 233, 234, 235, 236, 237, 238, 240, 241, 243, 244, 245, 246, 247, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 265, 266, 268, 269, 270, 271, 272, 274, 275, 276, 279, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 305, 306, 307, 309, 312, 314, 316, 317, 318, 319, 320, 321, 322, 324, 326, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 340, 341, 342, 343, 345, 346, 347, 351, 358, 359, 360, 363, 364, 369, 370, 371, 372, 373, 374, 375, 378, 380, 381, 384, 386, 395, 396, 397, 398, 399, 400, 401], "instal": [0, 3, 377, 385, 387, 399, 401], "build": [0, 378, 387, 388, 391, 399], "2": [0, 8, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 64, 66, 69, 70, 71, 72, 73, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 107, 108, 109, 110, 112, 113, 114, 115, 116, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 137, 138, 139, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 161, 162, 165, 166, 167, 168, 169, 170, 173, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 246, 247, 250, 251, 252, 254, 255, 256, 257, 258, 259, 260, 265, 267, 268, 269, 270, 272, 274, 275, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 293, 294, 295, 296, 297, 298, 299, 300, 302, 303, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 338, 340, 341, 342, 343, 344, 345, 346, 347, 353, 358, 360, 361, 362, 363, 364, 365, 369, 370, 371, 372, 373, 374, 375, 379, 380, 386, 387, 388, 395, 396, 397, 398, 399, 400], "explor": 0, "our": [0, 3, 4, 377, 384, 386, 395], "demo": [0, 4, 383, 385, 398], "3": [0, 3, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 29, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 63, 70, 71, 72, 73, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 107, 108, 109, 110, 112, 113, 114, 115, 116, 118, 119, 120, 121, 122, 123, 124, 127, 128, 129, 130, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 165, 166, 167, 168, 169, 170, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 221, 222, 223, 226, 227, 228, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 250, 252, 254, 255, 256, 257, 258, 259, 260, 267, 268, 269, 270, 274, 275, 276, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 293, 294, 295, 296, 297, 298, 299, 300, 302, 303, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 332, 333, 334, 335, 337, 338, 340, 341, 342, 343, 344, 345, 346, 347, 350, 351, 353, 360, 361, 362, 363, 364, 365, 370, 371, 372, 373, 374, 375, 380, 381, 386, 395, 396, 397, 398, 399, 400, 401], "tutori": [0, 388, 391, 392, 397, 401], "multi": [0, 26, 32, 84, 85, 105, 276, 368, 386, 387, 395], "head": [0, 355, 356, 357, 358, 360, 383, 387], "attent": [0, 355, 356, 358, 359, 360, 387], "simpl": [0, 383, 399, 401], "4": [0, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 29, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 64, 70, 71, 72, 73, 76, 77, 78, 79, 80, 81, 82, 83, 86, 87, 88, 90, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 108, 109, 110, 112, 113, 114, 115, 116, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 133, 134, 135, 136, 137, 138, 139, 141, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 165, 166, 167, 168, 169, 170, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 221, 222, 223, 226, 227, 228, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 244, 245, 249, 250, 251, 254, 255, 256, 257, 258, 259, 260, 266, 267, 268, 269, 270, 274, 275, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 291, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 332, 333, 334, 335, 337, 338, 340, 341, 342, 343, 344, 345, 346, 347, 361, 362, 363, 364, 365, 370, 371, 372, 373, 374, 375, 380, 381, 386, 395, 396, 397, 398, 399, 400, 401], "optim": [0, 8, 386, 387, 390, 395], "where": [0, 2, 3, 4, 30, 69, 74, 90, 91, 125, 126, 187, 219, 243, 260, 351, 371, 377, 381, 385, 386, 401], "To": [0, 3, 378, 381, 384, 395, 396, 402], "go": [0, 395], "from": [0, 2, 3, 4, 6, 8, 30, 32, 62, 63, 67, 68, 69, 89, 130, 171, 172, 188, 261, 262, 263, 264, 299, 300, 335, 367, 376, 377, 379, 380, 385, 386, 387, 388, 392, 395, 397, 398, 399], "here": [0, 2, 6, 383, 396, 401], "prerequisit": 0, "set": [0, 3, 4, 8, 11, 63, 69, 117, 219, 235, 252, 261, 262, 265, 304, 339, 354, 357, 360, 376, 378, 380, 381, 385, 387, 395, 398, 399, 400, 401, 402], "up": [0, 5, 118, 354, 381, 385, 387, 396, 401], "hardwar": [0, 2, 6, 248, 376, 377, 382, 386, 395, 402], "driver": [0, 395, 396, 397, 398, 399, 400], "firmwar": 0, "system": [0, 16, 35, 42, 115, 136, 141, 269, 279, 284, 285, 295, 337, 385], "level": [0, 385], "depend": [0, 4, 261, 262, 263, 264, 351, 377, 385, 386, 387, 399], "kmd": 0, "updat": [0, 44, 172, 377, 384, 402], "devic": [0, 5, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 107, 108, 109, 110, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 173, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 361, 362, 363, 364, 365, 366, 367, 369, 370, 371, 372, 373, 374, 375, 376, 377, 379, 380, 385, 386, 387, 389, 390, 393, 398, 399, 400, 401], "flash": [0, 359], "manag": [0, 218, 277, 278, 377], "interfac": [0, 381], "smi": 0, "option": [0, 8, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 107, 108, 109, 110, 111, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 136, 137, 138, 139, 140, 141, 142, 143, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 162, 163, 164, 165, 166, 167, 168, 169, 170, 173, 174, 175, 176, 177, 178, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 205, 206, 207, 209, 211, 212, 213, 214, 216, 217, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 255, 256, 257, 258, 259, 260, 265, 266, 267, 268, 269, 270, 271, 273, 274, 275, 276, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 379, 380, 385, 401, 402], "card": 0, "configur": [0, 3, 4, 9, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 76, 77, 78, 79, 80, 81, 82, 83, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 127, 128, 130, 131, 132, 133, 134, 136, 137, 138, 139, 140, 141, 142, 143, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 173, 174, 175, 176, 177, 178, 180, 181, 182, 183, 184, 185, 186, 187, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 205, 206, 207, 209, 211, 212, 213, 214, 216, 217, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243, 244, 245, 246, 247, 249, 250, 252, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 351, 352, 354, 355, 356, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 377, 380, 382, 387, 389, 390, 398], "topologi": [0, 26, 105, 276], "metalium": [0, 3, 4, 6], "There": [0, 6, 219, 376, 380, 381, 386], "ar": [0, 3, 4, 8, 9, 11, 19, 26, 39, 47, 50, 61, 63, 81, 105, 108, 118, 187, 219, 222, 243, 252, 258, 276, 277, 278, 295, 317, 339, 354, 360, 368, 376, 377, 378, 380, 381, 382, 385, 386, 387, 388, 391, 392, 395, 396, 397, 402], "three": 0, "sourc": [0, 3, 4, 5, 171, 387, 391], "step": [0, 30, 317, 377, 378, 384, 395, 401], "clone": [0, 159, 360, 387, 388, 395, 396, 397, 398], "repositori": [0, 1, 382], "invok": [0, 6], "script": [0, 3, 4, 377, 385], "docker": 0, "releas": [0, 75], "imag": [0, 69, 163, 253, 377, 381, 385, 386, 401], "wheel": [0, 399], "download": [0, 387, 388, 398, 399], "latest": [0, 385], "For": [0, 6, 8, 13, 23, 32, 45, 68, 81, 108, 112, 124, 130, 138, 156, 183, 191, 193, 195, 196, 198, 201, 202, 212, 213, 219, 259, 266, 270, 275, 287, 320, 326, 360, 373, 376, 377, 380, 384, 385, 386], "user": [0, 4, 6, 159, 187, 219, 229, 230, 248, 349, 377, 382, 384, 395, 396, 397, 398, 399, 400, 401], "onli": [0, 6, 8, 9, 19, 21, 22, 24, 26, 31, 40, 45, 57, 58, 59, 60, 63, 64, 74, 80, 81, 85, 89, 118, 130, 134, 136, 141, 143, 156, 177, 178, 182, 183, 190, 194, 201, 202, 203, 214, 217, 219, 221, 226, 233, 250, 255, 263, 264, 274, 276, 279, 287, 300, 330, 332, 334, 337, 339, 359, 360, 370, 373, 376, 377, 380, 381, 385, 386, 387, 388, 391, 392, 400, 402], "environ": [0, 3, 4, 381, 382, 397, 398, 399, 401, 402], "you": [0, 1, 2, 3, 4, 6, 319, 377, 378, 380, 381, 382, 385, 387, 398, 401, 402], "all": [0, 6, 8, 26, 31, 105, 140, 219, 229, 230, 250, 257, 265, 266, 276, 339, 376, 377, 380, 381, 384, 385, 386, 395, 397, 399], "verifi": [0, 384], "your": [0, 377, 378, 381, 382, 385, 387], "try": [0, 8, 377, 395, 399], "execut": [0, 3, 8, 277, 278, 377, 378, 381, 385, 396, 397, 399, 402], "program": [0, 3, 5, 6, 26, 187, 219, 276, 355, 356, 379, 385, 387, 389, 390, 395, 398, 399, 400], "exampl": [0, 108, 112, 129, 252, 265, 290, 339, 350, 377, 379, 381, 382, 384, 385, 386, 395, 401], "interest": 0, "contribut": [0, 2, 382], "us": [0, 3, 4, 5, 6, 8, 9, 10, 11, 26, 32, 45, 63, 67, 68, 69, 74, 81, 88, 89, 91, 95, 97, 101, 105, 109, 110, 111, 112, 118, 130, 132, 137, 138, 156, 159, 187, 219, 222, 229, 230, 247, 248, 250, 261, 262, 269, 270, 276, 277, 278, 289, 297, 304, 305, 318, 319, 348, 349, 351, 352, 353, 354, 359, 360, 366, 367, 375, 376, 377, 379, 380, 381, 382, 383, 384, 385, 386, 387, 389, 390, 393, 394, 399, 401], "basic": [0, 382, 383, 387], "convert": [0, 5, 6, 8, 32, 61, 130, 229, 230, 261, 262, 263, 264, 352, 353, 378, 379, 387, 390, 393, 396], "torch": [0, 6, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 86, 87, 88, 89, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 107, 108, 109, 110, 112, 113, 114, 115, 116, 119, 120, 121, 122, 123, 124, 127, 128, 129, 130, 133, 134, 135, 136, 137, 138, 139, 140, 141, 143, 144, 145, 146, 147, 148, 149, 150, 151, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 173, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 249, 252, 253, 254, 255, 256, 257, 258, 259, 260, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 340, 341, 342, 343, 344, 345, 346, 347, 350, 351, 352, 353, 354, 360, 361, 362, 363, 364, 365, 369, 370, 371, 372, 373, 375, 376, 378, 380, 386, 387, 389, 390, 392, 393, 394, 399, 401], "tensor": [0, 4, 5, 6, 8, 9, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 379, 385, 387, 389, 397, 398, 400, 401], "run": [0, 5, 6, 26, 44, 105, 130, 229, 276, 277, 278, 377, 379, 381, 382, 383, 384, 385, 387, 390, 392, 395, 396, 399], "an": [0, 2, 3, 5, 6, 26, 63, 67, 68, 69, 88, 105, 106, 118, 119, 123, 140, 218, 219, 253, 260, 265, 276, 286, 350, 353, 377, 380, 381, 382, 383, 384, 385, 386, 395, 397, 401], "oper": [0, 5, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 249, 250, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 377, 378, 379, 380, 382, 384, 386, 387, 391, 394, 396, 397], "__getitem__": 0, "slice": [0, 9, 381], "enabl": [0, 3, 5, 8, 381, 384, 385, 387, 389, 390, 395, 398, 399, 400, 401], "cach": [0, 3, 5, 32, 62, 111, 171, 172, 229, 230, 379, 385, 386, 387, 389, 390, 395, 398, 399, 400, 401], "5": [0, 30, 44, 55, 58, 60, 74, 78, 88, 90, 108, 112, 132, 146, 147, 148, 150, 165, 211, 234, 260, 270, 321, 322, 380, 383, 395, 396, 397, 398, 399, 400, 401], "debug": [0, 5, 6, 381, 384, 395, 396, 397, 398, 400], "intermedi": 0, "6": [0, 30, 112, 234, 293, 380, 386, 395, 396, 397, 398, 399, 400, 401], "trace": [0, 5, 248, 387, 392, 394, 401], "graph": [0, 5, 229, 379, 387, 392, 398], "7": [0, 88, 131, 293, 380, 395, 396, 397, 398, 399, 400, 401], "tt_lib": [0, 117, 118, 350, 378, 379], "8": [0, 26, 30, 74, 88, 105, 152, 250, 276, 380, 385, 386, 395, 396, 397, 398, 399, 400, 401], "log": [0, 8, 198, 372, 379, 385, 396, 398, 399], "9": [0, 57, 59, 72, 88, 90, 315, 380, 383, 395, 396, 397, 398, 399, 400, 401], "support": [0, 1, 5, 8, 9, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 29, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 63, 64, 70, 71, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 86, 87, 90, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 108, 109, 110, 112, 113, 114, 115, 116, 119, 120, 121, 122, 123, 124, 127, 128, 133, 134, 135, 136, 137, 138, 139, 141, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 165, 166, 167, 168, 169, 170, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 221, 222, 223, 226, 227, 228, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 244, 245, 249, 250, 254, 255, 256, 257, 258, 259, 260, 263, 264, 265, 266, 267, 268, 269, 270, 274, 275, 279, 280, 281, 282, 283, 284, 285, 286, 287, 289, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 332, 333, 334, 335, 337, 338, 340, 341, 342, 343, 344, 345, 346, 347, 359, 361, 362, 363, 364, 365, 370, 371, 372, 373, 374, 375, 378, 380, 381, 386, 395, 400], "python": [0, 4, 5, 381, 383, 384, 385, 399, 401], "10": [0, 30, 75, 88, 129, 140, 158, 187, 219, 255, 273, 346, 350, 351, 352, 378, 380, 383, 395, 396, 397, 398, 399, 401], "chang": [0, 291, 348, 349, 366, 367, 381, 395, 399], "string": [0, 8, 45, 80, 138, 229, 230, 269, 270, 304, 381], "represent": [0, 319, 381, 386], "11": [0, 90, 395, 396, 397, 398, 399, 400], "visual": [0, 5, 398, 400, 401], "web": 0, "browser": [0, 387], "12": [0, 64, 142, 174, 250, 292, 376, 383, 395, 396, 397, 399, 400, 401], "regist": [0, 5, 6, 277, 278], "pre": [0, 6, 222, 278, 383, 387, 388, 390, 399], "post": [0, 277, 385], "hook": [0, 277, 278, 399], "13": [0, 395, 396, 397, 399, 401], "queri": [0, 360, 381, 397], "14": [0, 3, 64, 395, 396, 397, 399], "fall": [0, 177], "back": [0, 6, 357, 378, 385, 395], "15": [0, 397, 399], "captur": [0, 401], "c": [0, 5, 9, 44, 67, 68, 69, 84, 106, 117, 118, 222, 305, 368, 381, 385, 386, 395, 396, 397, 398, 399, 400], "function": [0, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 380, 381, 387, 394, 395], "buffer": [0, 6, 8, 26, 75, 105, 248, 276, 379, 380, 381, 385, 386], "alloc": [0, 6, 30, 90, 91, 131, 132, 246, 247, 374, 375, 386, 395, 396, 397, 398, 399, 400], "etc": [0, 5, 6, 380], "shape": [0, 30, 44, 63, 69, 74, 90, 91, 107, 108, 115, 125, 126, 131, 132, 140, 187, 219, 222, 243, 246, 247, 250, 251, 252, 265, 288, 289, 290, 291, 317, 349, 357, 360, 367, 374, 375, 376, 378, 380, 381, 385, 395, 396, 397, 400, 401], "layout": [0, 8, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 64, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 108, 109, 110, 113, 114, 115, 116, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 165, 166, 167, 168, 169, 170, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 219, 221, 223, 226, 227, 228, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 249, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 266, 267, 268, 269, 270, 274, 275, 276, 279, 280, 281, 282, 283, 284, 285, 286, 287, 289, 290, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 332, 333, 334, 335, 337, 338, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 351, 353, 354, 361, 362, 363, 364, 365, 366, 367, 370, 371, 372, 373, 374, 375, 376, 378, 380, 381, 385, 387, 389, 393, 397, 400, 402], "data": [0, 5, 8, 18, 30, 31, 32, 61, 84, 88, 89, 90, 91, 92, 117, 118, 130, 131, 132, 133, 134, 140, 143, 175, 176, 178, 187, 199, 200, 203, 207, 209, 214, 219, 223, 227, 236, 237, 246, 247, 269, 299, 329, 335, 348, 349, 351, 352, 353, 354, 366, 367, 368, 374, 375, 376, 378, 380, 381, 385, 387, 393, 399, 402], "type": [0, 5, 6, 8, 9, 18, 30, 31, 32, 45, 61, 67, 68, 69, 81, 88, 89, 90, 91, 92, 106, 107, 108, 117, 118, 125, 130, 131, 132, 133, 134, 138, 140, 143, 175, 176, 178, 187, 199, 200, 203, 207, 209, 214, 219, 223, 227, 236, 237, 246, 247, 248, 252, 261, 262, 263, 264, 269, 273, 291, 299, 318, 329, 335, 339, 348, 349, 351, 352, 353, 354, 366, 367, 374, 375, 376, 378, 379, 380, 385, 387, 393, 399, 401], "limit": [0, 13, 23, 45, 81, 124, 138, 156, 187, 191, 193, 195, 196, 198, 201, 202, 212, 213, 259, 266, 270, 275, 287, 320, 326, 373, 377, 381], "bfloat8_b": [0, 8, 12, 13, 14, 18, 19, 20, 21, 22, 23, 24, 25, 27, 33, 36, 37, 38, 39, 40, 41, 45, 53, 55, 64, 70, 71, 72, 76, 77, 78, 79, 81, 83, 86, 92, 93, 94, 95, 97, 99, 101, 102, 103, 104, 113, 114, 115, 120, 124, 127, 128, 130, 134, 135, 136, 137, 138, 139, 141, 143, 144, 145, 146, 147, 148, 150, 152, 153, 154, 155, 156, 157, 158, 159, 166, 167, 168, 169, 170, 176, 177, 178, 179, 180, 181, 182, 183, 184, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 212, 213, 214, 215, 216, 217, 221, 226, 228, 233, 236, 237, 238, 239, 240, 241, 242, 257, 258, 259, 260, 266, 267, 268, 270, 274, 275, 279, 280, 281, 283, 284, 287, 293, 296, 297, 298, 299, 300, 302, 303, 306, 307, 308, 311, 312, 313, 314, 315, 320, 321, 322, 325, 326, 327, 328, 329, 330, 332, 333, 334, 335, 337, 338, 340, 341, 342, 344, 345, 353, 361, 362, 363, 365, 370, 371, 373, 376, 380, 385, 397], "storag": [0, 8, 379, 381, 387, 393], "shard": [0, 8, 19, 63, 74, 187, 219, 222, 261, 262, 263, 264, 350, 352, 360], "memori": [0, 8, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 76, 77, 78, 79, 80, 81, 82, 83, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 136, 137, 138, 139, 140, 141, 142, 143, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 173, 174, 175, 176, 177, 178, 180, 181, 182, 183, 184, 185, 186, 187, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 205, 206, 207, 209, 211, 212, 213, 214, 216, 217, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243, 244, 245, 246, 247, 249, 250, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 380, 385, 395, 397], "config": [0, 8, 26, 65, 88, 89, 105, 106, 125, 126, 219, 253, 261, 262, 276, 291, 317, 355, 356, 357, 376, 380, 383, 387, 389, 395, 397, 398, 399, 400, 402], "api": [0, 4, 6, 358, 376, 379, 382, 383, 384, 397, 402], "rank": [0, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 29, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 64, 70, 71, 72, 73, 76, 77, 78, 79, 80, 81, 82, 83, 86, 87, 90, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 108, 109, 110, 113, 114, 115, 116, 119, 120, 121, 122, 123, 124, 127, 128, 133, 134, 135, 136, 137, 138, 139, 141, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 165, 166, 167, 168, 169, 170, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 221, 223, 226, 227, 228, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 244, 245, 249, 250, 254, 255, 256, 257, 258, 259, 260, 266, 267, 268, 269, 270, 274, 275, 279, 280, 281, 282, 283, 284, 285, 286, 287, 289, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 332, 333, 334, 335, 337, 338, 340, 341, 342, 343, 344, 345, 346, 347, 353, 361, 362, 363, 364, 365, 370, 371, 372, 373, 374, 375, 380, 386], "to_rank": [0, 386], "open_devic": [0, 11, 62, 75, 88, 89, 129, 188, 273, 339, 350, 351, 352, 380, 395, 396, 397, 398, 402], "close_devic": [0, 395, 396, 397, 398, 400, 402], "manage_devic": [0, 402], "synchronize_devic": 0, "setdefaultdevic": [0, 378, 381], "getdefaultdevic": 0, "format_input_tensor": 0, "format_output_tensor": 0, "pad_to_tile_shap": 0, "create_sharded_memory_config": [0, 386], "core": [0, 6, 8, 74, 187, 219, 248, 351, 352, 366, 376, 381, 385, 386, 396, 397, 399], "as_tensor": [0, 395, 396, 397, 398, 400], "from_torch": [0, 6, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 86, 87, 88, 89, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 107, 108, 109, 110, 113, 114, 115, 116, 119, 120, 121, 122, 123, 124, 127, 128, 129, 133, 134, 135, 136, 137, 138, 139, 140, 141, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 173, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 249, 252, 253, 254, 255, 256, 257, 258, 259, 260, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 340, 341, 342, 343, 344, 345, 346, 347, 350, 351, 352, 353, 361, 362, 363, 364, 365, 369, 370, 371, 372, 373, 375, 376, 395, 396, 397, 398, 400, 402], "to_torch": [0, 6, 250, 376, 380, 395, 396, 397, 398, 400, 402], "to_devic": [0, 26, 28, 29, 65, 66, 75, 83, 88, 129, 160, 161, 162, 163, 164, 173, 187, 219, 220, 222, 224, 225, 232, 243, 252, 253, 254, 256, 271, 272, 273, 296, 318, 331, 336, 351, 352, 369, 395, 396, 397, 398, 400], "from_devic": [0, 395, 396, 397, 398, 399, 400], "to_layout": [0, 75, 130, 353, 386, 395, 396, 397], "dump_tensor": [0, 395, 396, 397, 398, 400], "load_tensor": [0, 395, 396, 397, 398, 399, 400], "dealloc": [0, 8, 273, 386, 395, 396, 397, 398, 400, 402], "realloc": [0, 395, 396, 397, 398, 400], "to_memory_config": [0, 386, 400], "creation": [0, 30, 90, 91, 130, 131, 132, 246, 247, 374, 375, 380], "arang": [0, 380, 395, 396, 397, 398], "empti": [0, 6, 8, 381, 401, 402], "empty_lik": 0, "zero": [0, 63, 67, 68, 69, 80, 109, 119, 132, 218, 243, 252, 261, 262, 263, 264, 274, 317, 318, 380, 381, 386, 398, 402], "zeros_lik": [0, 107], "ones": [0, 85, 125, 381], "ones_lik": 0, "full": [0, 377, 379, 381, 385, 402], "full_lik": 0, "matrix": [0, 5, 8, 88, 187, 219, 383, 386, 387, 389, 395, 402], "multipl": [0, 5, 6, 8, 61, 75, 118, 131, 136, 141, 175, 219, 229, 246, 261, 262, 263, 264, 279, 337, 351, 374, 380, 381, 385, 387, 389], "matmul": [0, 187, 387, 395, 396, 397, 398, 399], "linear": [0, 26, 105, 276, 319, 376, 379, 381, 395, 396, 397, 398], "pointwis": 0, "unari": [0, 6, 12, 14, 16, 27, 33, 35, 38, 42, 48, 52, 53, 55, 57, 59, 70, 72, 76, 78, 86, 94, 95, 97, 99, 101, 102, 113, 115, 120, 127, 136, 137, 139, 141, 145, 146, 148, 150, 152, 154, 157, 159, 166, 167, 168, 169, 170, 180, 184, 185, 189, 190, 192, 194, 197, 205, 206, 211, 216, 228, 234, 239, 242, 244, 245, 255, 267, 269, 270, 274, 279, 280, 281, 284, 285, 293, 295, 297, 302, 305, 306, 308, 310, 311, 313, 315, 319, 321, 323, 325, 327, 337, 338, 340, 342, 344, 346, 361, 362, 363, 365], "ab": [0, 13], "aco": [0, 15], "acosh": [0, 17], "asin": [0, 34], "asinh": [0, 36], "atan": [0, 41], "atanh": [0, 43], "bitwise_not": [0, 379, 381], "bitwise_left_shift": 0, "bitwise_right_shift": 0, "cbrt": 0, "ceil": [0, 54, 222, 379, 381], "celu": [0, 56, 395, 396, 397, 398], "clamp": [0, 58], "clip": [0, 60, 395, 396, 397, 398], "co": 0, "cosh": [0, 73], "deg2rad": [0, 77], "digamma": [0, 79], "experiment": [0, 4, 377, 379, 399], "dropout": 0, "gelu_bw": 0, "elu": [0, 87], "eqz": 0, "erf": [0, 96], "erfc": [0, 98], "erfinv": [0, 100], "exp": [0, 365, 378, 381, 398, 402], "exp2": [0, 103], "expm1": [0, 114], "fill": [0, 116, 118, 119, 131, 132, 246, 247, 288, 317, 374, 375, 381, 385], "floor": [0, 80, 81, 121, 122, 269, 270, 379, 381], "frac": [0, 39, 80, 128, 381], "geglu": [0, 395, 396, 397, 398], "gelu": [0, 8, 110, 136, 138, 376, 381], "glu": [0, 395, 396, 397, 398], "gez": 0, "gtz": 0, "hardshrink": [0, 147, 181, 395, 396, 397, 398], "hardsigmoid": [0, 149], "hardswish": [0, 151], "hardtanh": [0, 153], "heavisid": 0, "i0": [0, 158], "ident": [0, 108, 400], "isfinit": 0, "isinf": 0, "isnan": 0, "isneginf": 0, "isposinf": 0, "leaky_relu": [0, 181], "lez": 0, "lgamma": [0, 186], "log10": [0, 191], "log1p": [0, 193], "log2": [0, 195], "log_sigmoid": 0, "logical_not": [0, 206], "logical_not_": 0, "logit": [0, 212, 395, 396, 397, 398], "ltz": 0, "mish": [0, 8], "multigammaln": 0, "neg": [0, 240, 259, 381], "nez": 0, "normalize_glob": 0, "normalize_hw": 0, "polygamma": [0, 256, 395, 396, 397, 398], "prelu": 0, "rad2deg": [0, 268], "rdiv": [0, 270], "reciproc": [0, 275, 298, 386], "reglu": [0, 395, 396, 397, 398], "relu": [0, 8, 18, 180, 236, 279, 283, 284, 285, 319, 335, 365, 378, 400], "relu_max": 0, "relu_min": 0, "relu6": [0, 282], "remaind": [0, 287], "round": [0, 37, 80, 81, 269, 270, 294, 354, 386], "rsqrt": 0, "selu": [0, 303], "sigmoid": [0, 8, 198, 307, 381], "sigmoid_accur": 0, "sign": [0, 309, 381], "signbit": 0, "silu": [0, 8, 312, 337, 378, 379, 381, 402], "sin": [0, 314], "sinh": [0, 316], "softmax": [0, 355, 356, 379, 381, 397], "softplu": [0, 8, 320], "softshrink": [0, 322, 395, 396, 397, 398], "softsign": [0, 324], "sqrt": [0, 8, 155, 381], "squar": [0, 8, 232, 298, 326, 328, 329, 355, 356, 386], "swiglu": [0, 395, 396, 397, 398], "swish": [0, 381], "tan": [0, 341], "tanh": [0, 8, 45, 110, 138, 343], "tanhshrink": [0, 345], "threshold": [0, 319, 320, 347, 395, 396, 397, 398], "tril": 0, "triu": 0, "trunc": [0, 80, 81, 269, 270, 379, 381], "unary_chain": 0, "clamp_bw": 0, "clip_bw": 0, "hardtanh_bw": 0, "threshold_bw": 0, "softplus_bw": 0, "rdiv_bw": 0, "pow_bw": 0, "exp_bw": 0, "tanh_bw": 0, "sqrt_bw": 0, "multigammaln_bw": 0, "lgamma_bw": 0, "fill_bw": 0, "hardsigmoid_bw": 0, "cos_bw": 0, "acosh_bw": 0, "acos_bw": 0, "atan_bw": 0, "rad2deg_bw": 0, "frac_bw": 0, "trunc_bw": 0, "log_sigmoid_bw": 0, "fill_zero_bw": 0, "i0_bw": 0, "tan_bw": 0, "sigmoid_bw": 0, "rsqrt_bw": 0, "neg_bw": 0, "relu_bw": 0, "logit_bw": 0, "hardshrink_bw": 0, "softshrink_bw": 0, "leaky_relu_bw": 0, "elu_bw": 0, "celu_bw": 0, "rpow_bw": 0, "floor_bw": 0, "round_bw": 0, "log_bw": 0, "relu6_bw": 0, "abs_bw": 0, "silu_bw": 0, "selu_bw": 0, "square_bw": 0, "prod_bw": 0, "hardswish_bw": 0, "tanhshrink_bw": 0, "atanh_bw": 0, "asin_bw": 0, "asinh_bw": 0, "sin_bw": 0, "sinh_bw": 0, "log10_bw": 0, "log1p_bw": 0, "erfc_bw": 0, "ceil_bw": 0, "softsign_bw": 0, "cosh_bw": 0, "logiteps_bw": 0, "log2_bw": 0, "sign_bw": 0, "div_no_nan_bw": 0, "exp2_bw": 0, "expm1_bw": 0, "reciprocal_bw": 0, "digamma_bw": 0, "erfinv_bw": 0, "erf_bw": 0, "deg2rad_bw": 0, "polygamma_bw": 0, "repeat_bw": 0, "real": [0, 164, 253, 272, 377, 380], "angl": [0, 29], "is_imag": 0, "is_real": 0, "polar_bw": 0, "imag_bw": 0, "real_bw": 0, "angle_bw": 0, "conj_bw": 0, "conj": [0, 66], "polar": [0, 254], "alt_complex_rotate90": 0, "binari": [0, 18, 20, 39, 46, 47, 49, 50, 51, 80, 82, 92, 93, 122, 123, 133, 134, 135, 143, 144, 155, 165, 175, 176, 178, 179, 199, 200, 203, 204, 207, 208, 209, 210, 214, 215, 223, 227, 236, 237, 238, 241, 249, 257, 258, 260, 286, 299, 301, 329, 333, 335, 372, 381], "add": [0, 19, 250, 355, 356, 377, 381, 383, 384, 387, 397, 399, 400, 402], "addalpha": [0, 21], "subalpha": [0, 334], "multipli": [0, 8, 20, 22, 24, 187, 219, 233, 333, 368, 381, 387, 389, 402], "subtract": [0, 3, 299, 300, 332, 360, 402], "div": 0, "div_no_nan": [0, 83], "floor_div": 0, "fmod": [0, 124], "gcd": 0, "lcm": 0, "logical_and_": 0, "logical_or_": 0, "logical_xor_": 0, "rpow": [0, 296], "rsub": 0, "ldexp": [0, 177], "logical_and": 0, "logical_or": 0, "logical_xor": [0, 395, 396, 397, 398], "bitwise_and": [0, 47, 50], "bitwise_or": 0, "bitwise_xor": 0, "logaddexp": [0, 202], "logaddexp2": [0, 201], "hypot": [0, 156, 395, 396, 397, 398], "xlogi": [0, 373, 395, 396, 397, 398], "squared_differ": [0, 330], "gt": [0, 395, 396, 397, 398, 399, 400, 401], "gt_": 0, "lt_": 0, "ge_": 0, "le_": 0, "eq_": 0, "ne_": 0, "ge": 0, "lt": [0, 395, 396, 397, 398, 399, 400, 401], "le": 0, "eq": 0, "ne": 0, "isclos": [0, 395, 396, 397, 398], "nextaft": [0, 395, 396, 397, 398], "maximum": [0, 8, 31, 57, 58, 59, 60, 153, 221, 222, 381, 395, 396, 397, 398], "minimum": [0, 6, 57, 58, 59, 60, 153, 226, 381, 386, 395, 396, 397, 398], "outer": 0, "pow": [0, 378], "polyv": [0, 395, 396, 397, 398], "scatter": [0, 276], "atan2": [0, 40, 395, 396, 397, 398], "add_bw": 0, "assign_bw": 0, "atan2_bw": 0, "bias_gelu_bw": 0, "div_bw": 0, "embedding_bw": 0, "fmod_bw": 0, "remainder_bw": 0, "addalpha_bw": 0, "subalpha_bw": 0, "xlogy_bw": 0, "hypot_bw": 0, "ldexp_bw": 0, "logaddexp_bw": 0, "logaddexp2_bw": 0, "mul_bw": 0, "sub_bw": 0, "squared_difference_bw": 0, "concat_bw": 0, "rsub_bw": 0, "min_bw": 0, "max_bw": 0, "ternari": [0, 22, 24, 182, 217, 370], "addcdiv": [0, 23, 395, 396, 397, 398], "addcmul": [0, 25, 395, 396, 397, 398], "mac": [0, 395, 396, 397, 398], "lerp": [0, 183, 395, 396, 397, 398], "addcmul_bw": 0, "addcdiv_bw": 0, "where_bw": 0, "lerp_bw": 0, "loss": [0, 173, 232], "l1_loss": [0, 395, 396, 397, 398], "mse_loss": [0, 395, 396, 397, 398], "reduct": [0, 5, 8, 31, 173, 220, 224, 225, 232, 265, 331, 336, 354, 369], "cumprod": 0, "max": [0, 57, 58, 59, 60, 152, 153, 222, 284, 381, 399, 401], "mean": [0, 8, 44, 173, 232, 378, 379, 381, 386], "min": [0, 57, 58, 59, 60, 152, 153, 284, 285, 401], "std": [0, 6, 252, 304, 354, 380, 381, 400], "sum": [0, 108, 231, 381], "var": [0, 381], "argmax": [0, 381], "prod": [0, 74, 266], "topk": 0, "sort": [0, 354], "cumsum": 0, "movement": 0, "concat": [0, 64, 379, 381, 384], "nonzero": 0, "pad": [0, 61, 67, 68, 69, 88, 118, 125, 126, 130, 222, 251, 252, 261, 262, 263, 264, 291, 317, 349, 351, 378, 379, 380, 381, 386, 395, 400], "permut": [0, 222, 360, 397, 400], "reshap": [0, 222, 360, 378, 379, 380, 381, 395, 396, 397, 398, 399, 400], "repeat": [0, 5, 289, 290, 379, 381], "repeat_interleav": [0, 379, 381], "tiliz": [0, 32, 44, 381, 396, 397], "tilize_with_val_pad": 0, "fill_rm": [0, 117], "fill_ones_rm": 0, "until": [0, 107, 353, 367], "untilize_with_unpad": 0, "indexed_fil": 0, "normal": [0, 44, 142, 159, 174, 292, 318, 381, 399, 401], "group_norm": [0, 379, 381], "layer_norm": [0, 379, 381], "rms_norm": 0, "batch_norm": 0, "moreh": [0, 231, 381], "moreh_sum": 0, "transform": [0, 5, 187, 253, 376, 397, 398, 399], "split_query_key_value_and_split_head": [0, 397], "concatenate_head": [0, 397], "attention_softmax": 0, "attention_softmax_": [0, 397], "rotary_embed": 0, "scaled_dot_product_attent": 0, "scaled_dot_product_attention_decod": 0, "ccl": [0, 5, 26, 276], "all_gath": [0, 395, 396, 397, 398], "reduce_scatt": 0, "all_reduc": 0, "embed": [0, 89, 111, 395], "convolut": [0, 5, 63, 67, 68, 69, 106, 222, 261, 262, 263, 264, 381, 400], "conv1d": [0, 8, 400], "conv2d": [0, 8, 9, 69, 261, 262, 263, 264, 379, 381, 395, 396, 397, 398, 400], "conv3d": 0, "conv_transpose2d": [0, 8], "prepare_conv_weight": 0, "prepare_conv_bia": 0, "prepare_conv_transpose2d_weight": 0, "prepare_conv_transpose2d_bia": 0, "conv2dconfig": [0, 67, 68, 69, 261, 262, 263, 264, 400], "act_block_h_overrid": [0, 8, 400], "act_block_w_div": [0, 8], "activ": [0, 4, 8, 18, 92, 133, 134, 143, 153, 175, 176, 178, 187, 199, 200, 203, 207, 209, 214, 219, 223, 227, 236, 237, 299, 329, 335, 376, 383, 387, 390, 400], "always_preprocess_weight": [0, 8], "core_grid": [0, 8, 74, 142, 187, 219, 376, 396, 397], "deallocate_activ": [0, 8, 400], "dtype": [0, 6, 8, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 107, 108, 109, 110, 113, 114, 115, 116, 118, 119, 120, 121, 122, 123, 124, 125, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 173, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 249, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 279, 280, 281, 282, 283, 284, 285, 286, 287, 289, 290, 291, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 361, 362, 363, 364, 365, 369, 370, 371, 372, 373, 374, 375, 376, 380, 381, 386, 395, 396, 397, 398, 400, 402], "enable_act_double_buff": [0, 8], "enable_split_read": [0, 8], "enable_subblock_pad": [0, 8], "enable_weights_double_buff": [0, 8], "in_plac": [0, 8, 222], "output_layout": [0, 8, 142, 381], "override_sharding_config": [0, 8], "preprocess_weights_on_devic": [0, 8], "reallocate_halo_output": [0, 8], "reshard_if_not_optim": [0, 8], "shard_layout": [0, 8, 400], "transpose_shard": [0, 8], "weights_dtyp": [0, 8, 261, 262, 400], "conv2dsliceconfig": 0, "slicetypeenum": [0, 9], "sliceheight": [0, 9], "slicewidth": [0, 9], "name": [0, 3, 6, 9, 32, 85, 188, 229, 230, 358, 376, 380, 381, 384, 385, 399, 400, 401, 402], "valu": [0, 8, 9, 20, 21, 22, 23, 24, 25, 30, 31, 44, 55, 56, 57, 58, 59, 60, 63, 65, 78, 83, 87, 90, 91, 108, 112, 115, 117, 118, 125, 130, 131, 132, 140, 146, 147, 148, 150, 152, 153, 154, 162, 163, 164, 165, 171, 172, 181, 185, 213, 220, 222, 224, 225, 235, 246, 247, 248, 250, 252, 255, 258, 259, 269, 284, 285, 291, 295, 296, 302, 317, 319, 320, 321, 322, 331, 333, 334, 336, 346, 347, 349, 354, 360, 361, 362, 369, 374, 375, 377, 378, 380, 381, 386, 387, 389, 393, 397, 402], "num_slic": [0, 9], "slice_typ": [0, 9], "pool": [0, 84, 140, 222, 368, 381], "global_avg_pool2d": 0, "max_pool2d": 0, "vision": 0, "upsampl": [0, 381], "downsampl": [0, 400], "kv": 0, "kv_cach": 0, "fill_cache_for_user_": 0, "update_cache_for_token_": 0, "convers": [0, 77, 268, 351, 378, 380, 395], "model_preprocess": [0, 376, 397, 398, 399, 400], "preprocess_model": [0, 398, 399, 400], "preprocess_model_paramet": [0, 376, 398, 400], "report": [0, 68, 219, 377, 395, 396, 397, 398, 399, 400, 402], "set_printopt": [0, 402], "register_pre_operation_hook": [0, 402], "register_post_operation_hook": [0, 402], "creat": [0, 4, 6, 8, 30, 61, 74, 90, 91, 108, 112, 115, 131, 132, 246, 247, 374, 375, 378, 380, 381, 384, 386, 387, 392, 393, 397, 401], "host": [0, 8, 32, 67, 68, 129, 250, 317, 339, 351, 378, 380, 381, 383, 385, 386, 387, 393, 396, 397, 398, 399, 400], "borrow": [0, 380, 386, 387, 393], "v": [0, 8, 32, 118, 359, 383, 387, 393], "own": [0, 8, 380, 386, 387, 393], "open": [0, 5, 218, 248, 387, 393, 396, 397, 398, 399, 400, 401, 402], "initi": [0, 91, 229, 230, 376, 378, 381, 387, 389, 390, 393, 398, 399, 400], "b": [0, 6, 111, 290, 358, 359, 387, 389, 393], "random": [0, 252, 378, 387, 389, 393], "inspect": [0, 387, 389, 393], "output": [0, 3, 6, 8, 9, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 126, 127, 128, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 249, 250, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 274, 275, 276, 277, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 351, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 375, 376, 377, 378, 379, 380, 385, 386, 387, 389, 390, 393, 398, 402], "attribut": [0, 6, 376, 381, 385, 386, 387, 393, 399], "close": [0, 62, 218, 378, 383, 387, 389, 390, 393, 398, 399, 400], "result": [0, 3, 30, 32, 44, 118, 130, 132, 159, 187, 219, 222, 247, 261, 262, 263, 264, 265, 269, 274, 375, 378, 381, 385, 386, 387, 389], "more": [0, 1, 5, 6, 13, 23, 45, 68, 81, 124, 138, 156, 191, 193, 195, 196, 198, 201, 202, 212, 213, 259, 266, 270, 274, 275, 287, 320, 326, 373, 381, 382, 383, 385, 386, 387, 389, 397, 399, 401], "perform": [0, 8, 13, 15, 16, 17, 19, 21, 23, 25, 26, 28, 29, 34, 35, 36, 37, 40, 41, 42, 43, 45, 46, 47, 49, 50, 51, 52, 54, 55, 56, 57, 58, 59, 60, 64, 66, 71, 72, 73, 76, 77, 78, 79, 81, 83, 87, 93, 96, 98, 100, 103, 104, 105, 114, 116, 119, 121, 123, 124, 127, 128, 135, 136, 138, 140, 141, 144, 146, 147, 148, 149, 150, 151, 152, 153, 156, 158, 159, 160, 161, 177, 179, 181, 183, 185, 186, 191, 193, 195, 196, 198, 201, 202, 206, 211, 212, 213, 215, 218, 219, 221, 222, 226, 233, 234, 235, 238, 240, 244, 245, 250, 253, 254, 255, 256, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 275, 276, 279, 282, 283, 286, 287, 289, 294, 295, 296, 298, 300, 302, 303, 305, 307, 309, 312, 314, 315, 316, 317, 320, 321, 322, 323, 324, 326, 328, 330, 332, 334, 337, 338, 341, 343, 344, 345, 346, 347, 361, 362, 364, 371, 373, 376, 377, 380, 381, 382, 383, 384, 385, 387, 389, 397], "write": [0, 1, 3, 4, 6, 74, 187, 219, 376, 386, 387, 390, 399], "weight": [0, 8, 44, 67, 68, 69, 88, 89, 106, 142, 174, 182, 187, 260, 261, 262, 263, 264, 292, 376, 381, 387, 390, 398, 399, 400], "first": [0, 3, 130, 136, 141, 187, 219, 243, 250, 279, 337, 376, 378, 380, 381, 383, 385, 387, 390, 396, 402], "iter": [0, 387, 390], "subsequ": [0, 387, 390, 396, 402], "version": [0, 9, 229, 230, 359, 383, 385, 387, 390, 392, 395, 396, 398, 399], "process": [0, 63, 108, 261, 262, 385, 387, 390, 399], "paramet": [0, 5, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 249, 251, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 381, 387, 390, 392, 398], "check": [0, 2, 6, 219, 377, 380, 383, 387, 390, 399], "match": [0, 61, 219, 229, 230, 291, 380, 381, 386, 387, 390, 395, 396, 398, 399, 400], "origin": [0, 112, 229, 230, 251, 265, 377, 384, 387, 390, 395], "implement": [0, 3, 8, 67, 107, 219, 250, 358, 359, 360, 376, 381, 384, 385, 387, 390, 392], "tracer": [0, 387, 398, 400, 401, 402], "modul": [0, 6, 69, 229, 230, 376, 381, 382, 387, 392, 394, 395, 396, 397, 401], "written": [0, 6, 171, 172, 187, 219, 387, 394, 397], "profil": [0, 3, 159, 304, 379, 383, 387, 402], "resnet": [0, 385, 387, 399], "block": [0, 8, 67, 68, 69, 74, 218, 219, 222, 261, 262, 263, 264, 274, 381, 386, 387], "torchvis": [0, 387, 392, 399, 401], "preprocess": [0, 6, 8, 32, 67, 68, 69, 229, 230, 261, 262, 263, 264, 386, 387, 392], "displai": [0, 383, 387, 388, 392], "pass": [0, 6, 8, 67, 68, 110, 111, 118, 277, 278, 360, 376, 378, 381, 384, 387, 392, 395, 398, 399], "constructor": [0, 380, 387, 392], "dit_xl_2": [0, 387, 401], "With": [0, 108, 387], "pytorch": [0, 3, 5, 8, 31, 67, 68, 69, 112, 219, 261, 262, 263, 264, 354, 358, 360, 379, 381, 387, 388, 399], "base": [0, 3, 4, 6, 8, 9, 44, 63, 74, 219, 251, 381, 383, 386, 387, 388, 397], "librari": [0, 4, 5, 378, 379, 380, 387, 388], "http": [0, 26, 276, 382, 383, 387, 388, 399], "github": [0, 2, 26, 276, 382, 383, 387, 388], "com": [0, 26, 276, 382, 383, 387, 388], "facebookresearch": [0, 387, 388], "dit": [0, 387, 388], "git": [0, 229, 230, 383, 387, 388, 399], "xl": [0, 387, 388], "sampl": [0, 381, 387, 388], "train": [0, 44, 381, 387, 388], "onboard": 0, "new": [0, 91, 132, 247, 288, 291, 375, 377, 379, 385, 395, 398], "rewrit": 0, "switch": [0, 250, 319], "ad": [0, 67, 68, 69, 187, 261, 262, 263, 264, 380, 381, 384, 395], "faq": 0, "need": [0, 1, 2, 9, 187, 219, 377, 378, 381, 385, 386, 395, 396, 397, 402], "bind": [0, 399], "golden": [0, 376, 402], "perf": [0, 395, 396, 397, 398, 399, 400], "header": [0, 3], "profile_thi": [0, 399], "descript": [0, 117, 118, 380, 381, 384], "lib": [0, 4, 379, 398, 399, 401], "overview": [0, 379], "infrastructur": [0, 379], "member": [0, 2, 9, 379, 380], "input": [0, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 127, 128, 130, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 249, 250, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 375, 377, 378, 379, 380, 385, 386, 396, 397, 400, 401, 402], "fast": [0, 95, 97, 101, 137, 297, 305, 379], "dispatch": [0, 248, 379, 380, 385], "cpu": [0, 3, 378, 379, 380, 381, 383, 385, 399, 401], "through": [0, 379, 383, 401], "primari": [0, 379], "softmax_backward": [0, 379, 381], "softmin": [0, 379, 381], "softmin_backward": [0, 379, 381], "logsoftmax": [0, 379, 381], "logsoftmax_backward": [0, 379, 381], "mean_backward": [0, 379, 381], "group_norm_backward": [0, 379, 381], "norm": [0, 44, 379, 381], "norm_backward": [0, 379, 381], "enum": [0, 379], "bcastopmath": [0, 379, 381], "bcastopdim": [0, 379, 381], "fallback": [0, 378, 379, 384], "tensor_slic": [0, 379, 381], "chunk": [0, 8, 105, 276, 358, 359, 379, 381, 396, 401], "interpol": [0, 182, 379, 381], "batchnorm2d": [0, 379, 381, 400], "groupnorm": [0, 379, 381], "layernorm": [0, 379, 381], "maxpool2d": [0, 379, 381], "adaptiveavgpool2d": [0, 379, 381], "unary_fmod": [0, 379, 381], "binary_fmod": [0, 379, 381], "unary_bitwise_or": [0, 379, 381], "unary_bitwise_and": [0, 379, 381], "unary_bitwise_xor": [0, 379, 381], "binary_bitwise_or": [0, 379, 381], "binary_bitwise_and": [0, 379, 381], "binary_bitwise_xor": [0, 379, 381], "unary_bitwise_left_shift": [0, 379, 381], "unary_bitwise_right_shift": [0, 379, 381], "binary_bitwise_left_shift": [0, 379, 381], "binary_bitwise_right_shift": [0, 379, 381], "torch_argmax": [0, 379, 381], "torch_argmin": [0, 379, 381], "fuse": [0, 5, 8, 376, 379, 397], "mini": [0, 379], "addandnorm": [0, 379, 381], "complex": [0, 28, 29, 65, 66, 160, 161, 253, 254, 271, 272, 379], "__init__": [0, 376, 379, 380, 400], "get_dtyp": [0, 6, 379, 380], "get_layout": [0, 6, 379, 380], "pad_to_til": [0, 379, 380], "storage_typ": [0, 379, 380], "unpad": [0, 61, 126, 351, 367, 378, 379, 380, 381], "unpad_from_til": [0, 379, 380], "memoryconfig": [0, 6, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 109, 110, 111, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 136, 137, 138, 139, 140, 141, 142, 143, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 173, 174, 175, 176, 177, 178, 180, 181, 182, 183, 184, 185, 186, 187, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 205, 206, 207, 209, 211, 212, 213, 214, 216, 217, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243, 244, 245, 246, 247, 249, 250, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 354, 355, 356, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 379, 381], "between": [0, 6, 30, 44, 67, 68, 69, 219, 261, 262, 263, 264, 352, 379, 381], "one": [0, 6, 57, 58, 59, 60, 265, 379, 381, 382], "op": [0, 6, 8, 9, 10, 31, 67, 68, 69, 74, 89, 222, 318, 339, 358, 359, 365, 379, 381, 384, 385, 399, 402], "acceler": [0, 348, 349, 366, 367, 379, 380, 381, 396, 399], "odd": [0, 379], "size": [0, 8, 9, 30, 67, 68, 69, 84, 88, 89, 108, 219, 222, 248, 260, 261, 262, 263, 264, 265, 317, 358, 359, 360, 368, 379, 380, 381, 386, 395, 402], "last": [0, 9, 27, 31, 61, 112, 131, 136, 141, 246, 279, 291, 337, 351, 354, 360, 374, 379, 380, 381, 385, 386], "dim": [0, 26, 31, 63, 64, 74, 105, 107, 108, 112, 136, 141, 162, 220, 224, 225, 252, 265, 266, 276, 279, 290, 317, 318, 331, 336, 337, 354, 360, 369, 379, 380, 381, 386, 397, 401], "uplift": 0, "next": [0, 8, 241, 381, 386], "file": [0, 2, 4, 6, 32, 85, 188, 377, 383, 385, 399, 401, 402], "bug": 0, "featur": [0, 5, 381, 384, 391, 402], "propos": [0, 384], "request": [0, 351, 384, 399, 401], "troubleshoot": [0, 377], "tip": 0, "commun": 0, "develop": [0, 4, 5, 382, 383, 384, 385, 399], "index": [0, 171, 172, 354, 359, 380, 385, 399, 401], "search": 0, "page": [0, 383, 387], "If": [1, 2, 6, 8, 9, 31, 61, 67, 68, 69, 74, 108, 112, 187, 219, 229, 230, 248, 250, 252, 265, 266, 317, 318, 339, 353, 354, 359, 360, 380, 381, 382, 383, 384, 385, 398, 402], "would": [1, 8, 384, 385, 386], "like": [1, 119, 319, 376, 378, 386, 395, 402], "thi": [1, 3, 4, 6, 8, 9, 10, 11, 31, 44, 61, 63, 67, 68, 69, 89, 108, 110, 111, 112, 115, 125, 126, 140, 159, 177, 222, 251, 261, 262, 263, 264, 265, 284, 285, 319, 358, 376, 377, 378, 380, 381, 383, 384, 385, 386, 387, 388, 391, 392, 395, 396, 397, 401, 402], "project": [1, 2, 4, 5, 382], "pleas": [1, 2, 219, 377, 381, 382, 384, 387, 402], "review": [1, 382, 384], "standard": [1, 2, 285, 377, 382], "gain": 1, "access": [1, 2, 5, 383, 401], "read": [1, 74, 381, 382, 386], "section": [1, 2, 377, 381, 386], "detail": [1, 6, 13, 23, 45, 81, 124, 138, 156, 191, 193, 195, 196, 198, 201, 202, 212, 213, 259, 266, 270, 275, 287, 320, 326, 373, 382, 401, 402], "contact": 1, "u": [1, 384], "have": [2, 3, 4, 6, 31, 75, 89, 108, 126, 182, 219, 261, 262, 265, 269, 348, 349, 354, 366, 367, 377, 380, 381, 383, 385, 386, 387, 395, 401], "formal": 2, "permiss": 2, "cloud": 2, "issu": [2, 8, 219, 319, 377, 384, 385, 395, 396, 397, 398, 399, 400], "can": [2, 3, 4, 5, 6, 8, 9, 30, 45, 57, 58, 59, 60, 67, 68, 69, 80, 81, 91, 132, 138, 219, 247, 250, 261, 262, 263, 264, 269, 270, 277, 278, 319, 339, 375, 376, 378, 380, 381, 383, 385, 386, 387, 395, 396, 397, 398, 400, 401, 402], "out": [2, 6, 8, 107, 109, 112, 285, 317, 354, 381, 383, 395, 397, 400], "relev": [2, 377], "ever": 2, "help": [2, 384, 387], "we": [2, 3, 4, 126, 130, 219, 353, 377, 378, 381, 383, 384, 386, 388, 391, 392, 395, 396, 401, 402], "offici": 2, "discord": 2, "channel": [2, 8, 26, 44, 67, 68, 69, 84, 105, 117, 118, 140, 222, 261, 262, 263, 264, 276, 368, 381, 385], "repres": [2, 5, 380, 385, 386, 395], "both": [2, 8, 19, 67, 68, 69, 108, 219, 258, 261, 262, 263, 264, 376, 377, 380, 381, 385, 386, 395, 400], "tenstorr": [2, 6, 8, 26, 276, 376, 377, 382, 383, 387, 395, 396, 401, 402], "metal": [2, 5, 26, 276, 380, 382, 383, 387, 395, 396, 397, 398, 399, 400, 401], "join": [2, 399], "discuss": [2, 377], "board": 2, "bounc": 2, "idea": [2, 377], "off": [2, 284, 293, 376, 386], "each": [2, 3, 9, 44, 63, 118, 140, 222, 250, 288, 290, 317, 380, 381, 385, 386], "other": [2, 6, 8, 219, 376, 377, 381, 382, 386, 388, 391, 392, 402], "refer": [2, 3, 4, 5, 13, 23, 45, 68, 75, 81, 91, 115, 124, 138, 156, 191, 193, 195, 196, 198, 201, 202, 212, 213, 219, 235, 259, 266, 270, 275, 287, 320, 326, 373, 380, 384, 386, 402], "code": [2, 6, 31, 112, 277, 278, 354, 360, 378, 381, 382, 383, 384, 385, 395, 399, 402], "conduct": 2, "when": [2, 6, 8, 9, 10, 11, 44, 63, 67, 68, 69, 74, 81, 111, 156, 203, 218, 219, 229, 252, 258, 265, 270, 351, 354, 377, 380, 381, 384, 386, 395, 397, 398, 400, 402], "interact": 2, "ensur": [3, 4, 61, 112, 339, 377, 383, 386], "tt": [3, 4, 11, 26, 276, 339, 348, 349, 366, 367, 368, 379, 387, 388, 391, 392, 395, 396, 397, 398, 399, 400, 401], "requir": [3, 6, 19, 21, 37, 44, 64, 74, 81, 117, 118, 131, 229, 230, 233, 246, 300, 332, 334, 371, 374, 377, 380, 381, 383, 385, 391, 399, 401], "model": [3, 5, 229, 230, 377, 384, 385, 387, 388, 390, 394, 395, 396, 399, 400], "follow": [3, 6, 69, 112, 118, 219, 291, 376, 378, 380, 381, 382, 383, 384, 385, 386, 387, 401, 402], "instruct": [3, 4, 377, 382, 383, 387, 402], "readi": [3, 4, 360, 377], "come": [3, 385], "typic": [3, 140, 386], "found": [3, 6, 376, 387, 399, 401], "under": [3, 4, 377, 378, 384, 385, 387, 402], "your_model": 3, "perf_model": 3, "py": [3, 6, 376, 377, 385, 398, 399, 402], "pytest": [3, 4, 376, 377, 385, 399, 402], "test": [3, 4, 6, 376, 377, 384, 385, 397, 399, 402], "python_api_test": 3, "perf_your_model": 3, "csv": [3, 385, 399], "perf_your_model_d": 3, "contain": [3, 4, 6, 8, 30, 88, 108, 222, 274, 380, 386, 395], "tabl": [3, 383, 399], "two": [3, 61, 63, 136, 141, 219, 279, 291, 337, 351, 360, 376, 380, 381, 386], "row": [3, 8, 54, 106, 116, 118, 119, 121, 128, 243, 294, 309, 364, 380, 385, 386, 395, 396, 399], "batch": [3, 44, 67, 68, 69, 84, 117, 118, 162, 187, 219, 222, 261, 262, 263, 264, 359, 380, 381, 385], "sec": 3, "second": [3, 130, 136, 141, 187, 219, 250, 260, 279, 291, 337, 380, 381, 385, 397, 399, 402], "compil": [3, 381, 396, 399, 402], "time": [3, 107, 165, 229, 288, 377, 381, 385, 396, 397, 399, 402], "infer": [3, 44, 377, 385, 399], "g": [3, 69, 219, 381, 383, 385, 386], "throughput": 3, "inf": [3, 228, 234], "vit": 3, "patch16": 3, "30": [3, 64, 90, 399], "51": [3, 398], "16": [3, 317, 386, 397, 399, 401], "05": [3, 44, 381, 401], "46": [3, 399], "0": [3, 6, 8, 9, 11, 12, 14, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 37, 38, 44, 46, 47, 48, 49, 50, 51, 53, 57, 58, 59, 60, 62, 63, 64, 65, 69, 70, 75, 78, 80, 81, 86, 88, 89, 90, 91, 92, 94, 95, 97, 99, 101, 102, 104, 107, 108, 109, 110, 113, 115, 116, 117, 118, 120, 125, 129, 130, 131, 132, 133, 134, 137, 138, 139, 143, 145, 146, 147, 148, 150, 154, 157, 159, 160, 162, 163, 164, 165, 166, 167, 168, 169, 170, 173, 175, 176, 178, 180, 181, 182, 184, 185, 188, 189, 190, 192, 194, 197, 199, 200, 203, 205, 207, 209, 211, 213, 214, 216, 218, 222, 223, 227, 228, 232, 233, 236, 237, 239, 240, 242, 243, 246, 247, 248, 250, 252, 253, 257, 258, 259, 265, 266, 269, 270, 271, 273, 274, 276, 280, 281, 284, 285, 290, 291, 293, 297, 298, 299, 300, 305, 306, 308, 310, 311, 312, 313, 317, 318, 319, 321, 322, 325, 326, 327, 329, 332, 333, 334, 335, 339, 340, 342, 343, 346, 347, 348, 349, 350, 351, 352, 353, 354, 358, 359, 360, 361, 362, 363, 365, 366, 367, 370, 371, 374, 375, 376, 378, 380, 381, 383, 385, 386, 395, 396, 397, 398, 399, 400, 401, 402], "0623": 3, "29": [3, 399], "4960": 3, "includ": [3, 5, 6, 61, 377, 386], "without": [3, 91, 378, 380, 381, 383], "ani": [3, 8, 117, 118, 250, 377, 378, 381, 399], "abovement": 3, "grayskul": [3, 6, 32, 376, 383, 387, 388, 391, 392, 397, 398, 402], "It": [3, 5, 6, 8, 9, 67, 68, 69, 229, 230, 353, 380, 381, 395], "sinc": [3, 159, 378], "dure": [3, 63, 130, 165, 353, 386], "do": [3, 377, 378, 397], "pai": 3, "suggest": 3, "calcul": [3, 9, 74, 107, 269, 385], "comput": [3, 5, 6, 20, 22, 24, 39, 61, 67, 68, 69, 80, 82, 106, 108, 122, 133, 140, 142, 155, 165, 174, 175, 176, 182, 187, 199, 200, 203, 204, 207, 208, 209, 210, 217, 219, 223, 227, 241, 249, 257, 261, 262, 263, 264, 265, 292, 301, 318, 329, 333, 355, 356, 359, 360, 370, 372, 381, 385, 395, 396], "": [3, 4, 6, 8, 61, 67, 68, 69, 107, 229, 230, 358, 359, 377, 380, 381, 383, 386, 395, 396, 397, 401], "also": [3, 6, 8, 67, 68, 69, 108, 235, 377, 378, 380, 381, 385, 401], "maintain": [3, 384], "run_perform": [3, 377], "sh": [3, 4, 377, 383, 385, 399], "facilit": 3, "easi": [3, 395], "wai": [3, 5, 229, 230, 376, 381, 383, 395], "attempt": [3, 381, 399], "fastest": 3, "command": [3, 12, 14, 18, 19, 21, 27, 31, 33, 37, 38, 44, 46, 47, 48, 49, 50, 51, 53, 63, 64, 70, 80, 81, 86, 88, 89, 92, 94, 95, 97, 99, 101, 102, 104, 107, 109, 110, 113, 115, 116, 117, 118, 120, 130, 131, 132, 133, 134, 137, 138, 139, 143, 145, 154, 157, 159, 162, 166, 167, 168, 169, 170, 173, 175, 176, 178, 180, 184, 189, 190, 192, 194, 197, 199, 200, 203, 205, 207, 209, 214, 216, 223, 227, 228, 232, 233, 236, 237, 239, 240, 242, 243, 247, 250, 252, 258, 259, 269, 274, 280, 281, 284, 285, 291, 293, 297, 298, 299, 300, 305, 306, 308, 310, 311, 312, 313, 319, 325, 326, 327, 329, 332, 334, 335, 339, 340, 342, 343, 348, 349, 353, 354, 358, 359, 363, 365, 366, 367, 370, 371, 375, 380, 383, 385, 402], "merg": [3, 384], "built": [4, 5, 273, 383, 399, 401], "now": [4, 112, 130, 353, 368, 386, 395, 397], "root": [4, 298, 326, 355, 356], "provid": [4, 26, 31, 44, 107, 115, 187, 219, 229, 230, 265, 266, 276, 318, 339, 342, 349, 354, 377, 380, 381, 384, 385, 386, 395, 397, 402], "virtual": [4, 383], "which": [4, 8, 47, 50, 74, 107, 108, 112, 131, 132, 187, 219, 222, 229, 230, 246, 247, 269, 317, 318, 358, 359, 374, 375, 376, 380, 381, 385, 386], "ll": 4, "work": [4, 219, 317, 376, 377, 387, 388, 391, 392, 402], "python_env": [4, 383, 398, 399, 401], "bin": [4, 85, 188, 383, 399], "python_env_dir": 4, "variabl": [4, 6, 381, 383, 398, 400, 402], "create_venv": [4, 383], "control": [4, 8, 381, 395], "pythonpath": [4, 383, 401], "common": [4, 133, 175, 377, 381], "practic": 4, "export": [4, 381, 383, 402], "pwd": [4, 383], "folder": [4, 377, 385, 399], "split": [4, 63, 136, 141, 279, 337, 360, 381, 386], "them": [4, 360, 381, 383, 385, 395], "sub": [4, 339, 366, 381], "In": [4, 6, 109, 219, 291, 351, 356, 376, 378, 381, 385, 386, 395, 401], "find": [4, 383, 395, 396, 397, 398, 399, 400], "prepar": [4, 8, 377, 399], "readm": [4, 377, 383, 399, 401], "md": [4, 26, 276, 377, 383], "give": [4, 385], "how": [4, 6, 9, 377, 380, 381, 385, 386, 396, 397, 402], "progress": [4, 401], "yet": 4, "mani": [4, 6, 376, 396, 401], "part": [4, 136, 141, 279, 337, 377, 381, 385, 397], "entir": [4, 140, 339], "path_to_test_fil": 4, "test_in_fil": 4, "ttnn": [4, 6, 380, 381, 383, 384, 386, 387, 390, 392, 393, 396, 401, 402], "friendli": [4, 382], "top": [4, 354, 380, 387], "doc": [4, 6, 399], "document": [4, 6, 377, 384, 399], "neural": 5, "network": [5, 381], "design": 5, "feel": 5, "familiar": 5, "experienc": 5, "kei": [5, 360, 376, 386, 395, 397, 399], "than": [5, 8, 9, 63, 78, 117, 118, 134, 135, 143, 144, 178, 179, 185, 214, 215, 235, 265, 354, 381, 383, 385, 397, 402], "200": 5, "A": [5, 6, 8, 30, 131, 132, 219, 246, 247, 319, 359, 374, 375, 377, 378, 380, 381, 383, 384, 386], "differ": [5, 8, 69, 219, 305, 329, 354, 380, 386, 395, 402], "distribut": [5, 74, 85, 187, 219, 386], "The": [5, 6, 8, 9, 10, 11, 26, 27, 30, 32, 61, 62, 67, 68, 69, 75, 84, 85, 86, 88, 89, 90, 91, 105, 110, 112, 115, 118, 130, 131, 132, 140, 154, 180, 187, 218, 219, 222, 235, 246, 247, 248, 251, 261, 262, 263, 264, 276, 277, 278, 284, 285, 288, 339, 350, 353, 354, 358, 359, 368, 374, 375, 376, 377, 378, 380, 381, 382, 384, 385, 386, 395, 396, 401, 402], "abil": [5, 381], "custom": [5, 6, 277, 278, 397], "nativ": 5, "mesh": [5, 26, 130, 276, 351, 353, 383], "tool": [5, 383, 385, 399], "util": [5, 380, 383, 386, 395, 396, 401], "significantli": [5, 8, 396], "speed": [5, 396], "load": [5, 188, 398, 399, 401], "comparison": [5, 165, 384], "mode": [5, 37, 44, 45, 81, 95, 97, 101, 137, 138, 173, 222, 232, 270, 297, 305, 381, 395, 396, 397, 398, 399, 400], "long": [5, 384], "sequenc": [5, 6, 358, 359], "against": [5, 377, 402], "known": [5, 69, 381], "meant": 6, "contributor": 6, "Not": [6, 238, 376, 402], "mai": [6, 75, 81, 156, 177, 219, 270, 274, 350, 354, 376, 381, 386, 402], "wormhol": [6, 32, 376, 383, 387, 402], "take": [6, 265, 357, 377, 380, 381, 382, 386, 395], "produc": [6, 274, 377, 378, 381, 395, 396], "call": [6, 8, 107, 130, 277, 278, 353, 378, 380, 381, 384, 385, 386, 395, 397, 399, 402], "optiona": 6, "composit": 6, "struct": [6, 381], "specifi": [6, 8, 30, 61, 85, 90, 91, 108, 112, 131, 132, 187, 219, 229, 230, 246, 247, 250, 251, 252, 265, 288, 339, 349, 358, 359, 374, 375, 376, 380, 381, 396, 397], "simpli": [6, 351, 380, 381, 395], "defin": [6, 32, 380, 381, 384, 386], "method": [6, 273, 381, 383, 399], "register_oper": 6, "exist": [6, 248, 380, 399, 401], "bind_registered_oper": 6, "auto": [6, 381], "attach": [6, 229, 230, 399], "attach_golden_funct": 6, "let": [6, 386, 395, 397], "just": [6, 261, 262, 263, 264, 397, 401], "copi": [6, 61, 129, 159, 350, 380, 381, 395], "order": [6, 74, 112, 219, 256, 300, 354, 380, 381, 385, 386, 387, 395, 397, 402], "directori": [6, 387, 401], "structur": [6, 8, 9, 376], "shown": [6, 386], "below": [6, 107, 219, 377, 381, 385, 386, 387], "cpp": 6, "categori": 6, "operation_nam": 6, "_device_oper": 6, "hpp": 6, "program_factory_0": 6, "_program_factori": 6, "factori": 6, "But": [6, 378], "concret": [6, 219], "example_device_oper": 6, "spdx": [6, 402], "filecopyrighttext": [6, 402], "2023": [6, 399, 401], "inc": [6, 402], "licens": [6, 402], "identifi": [6, 399, 402], "apach": [6, 402], "pragma": 6, "onc": [6, 261, 262, 263, 264, 383, 396], "variant": 6, "device_oper": 6, "decor": [6, 384, 395, 396, 397, 398, 399, 400], "namespac": [6, 401], "exampledeviceoper": 6, "store": [6, 44, 380, 385, 386, 395], "aren": [6, 10], "t": [6, 10, 105, 159, 219, 229, 230, 276, 378, 380, 381, 385, 395, 397, 399], "operation_attributes_t": 6, "bool": [6, 19, 21, 31, 32, 37, 44, 64, 67, 68, 69, 74, 75, 80, 81, 95, 97, 101, 137, 142, 165, 173, 187, 219, 222, 229, 230, 232, 233, 250, 263, 264, 265, 297, 300, 305, 332, 334, 348, 349, 354, 355, 356, 358, 359, 360, 366, 367, 371, 381], "int": [6, 12, 14, 18, 19, 21, 26, 27, 30, 31, 33, 37, 38, 44, 46, 47, 48, 49, 50, 51, 53, 63, 64, 67, 68, 69, 70, 74, 80, 81, 86, 88, 89, 90, 92, 94, 95, 97, 99, 101, 102, 104, 105, 107, 108, 109, 110, 111, 113, 115, 116, 117, 118, 120, 126, 130, 131, 132, 133, 134, 136, 137, 139, 141, 142, 143, 145, 154, 157, 159, 162, 166, 167, 168, 169, 170, 171, 172, 173, 175, 176, 178, 180, 184, 187, 189, 190, 192, 194, 197, 199, 200, 203, 205, 207, 209, 214, 216, 218, 219, 222, 223, 227, 228, 232, 233, 236, 237, 239, 240, 242, 243, 247, 248, 250, 251, 252, 255, 258, 259, 261, 262, 263, 264, 265, 266, 269, 274, 276, 279, 280, 281, 284, 285, 289, 293, 297, 299, 300, 305, 306, 308, 310, 311, 312, 313, 317, 318, 319, 325, 326, 327, 329, 332, 334, 335, 337, 339, 340, 342, 343, 348, 349, 353, 354, 355, 356, 358, 359, 360, 363, 365, 366, 367, 368, 370, 371, 375, 380, 381, 386, 400, 401], "some_other_attribut": 6, "argument": [6, 8, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 70, 71, 72, 73, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89, 91, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 127, 128, 130, 133, 134, 136, 137, 138, 139, 140, 141, 142, 143, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 173, 174, 175, 176, 177, 178, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 205, 206, 207, 209, 211, 212, 213, 214, 216, 217, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243, 244, 245, 248, 249, 253, 254, 255, 256, 257, 258, 259, 260, 265, 266, 267, 268, 269, 270, 271, 272, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 380, 381, 395, 398, 400], "don": [6, 219, 381, 395], "thei": [6, 8, 9, 219, 377, 381, 396, 397], "tensor_args_t": 6, "const": [6, 304, 381], "input_tensor": [6, 12, 13, 14, 15, 16, 17, 26, 27, 28, 29, 31, 33, 34, 35, 36, 37, 38, 41, 42, 43, 44, 45, 48, 52, 53, 54, 55, 56, 57, 58, 59, 60, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 76, 77, 78, 79, 81, 83, 84, 86, 87, 88, 89, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 109, 110, 111, 112, 113, 114, 115, 116, 119, 120, 121, 125, 127, 128, 135, 136, 137, 138, 139, 140, 141, 142, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 157, 158, 159, 160, 161, 163, 164, 166, 167, 168, 169, 170, 171, 174, 179, 180, 181, 184, 185, 186, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 205, 206, 211, 212, 213, 215, 216, 222, 228, 234, 235, 238, 239, 240, 242, 243, 244, 245, 247, 250, 252, 253, 254, 255, 256, 257, 258, 259, 265, 266, 267, 268, 269, 270, 271, 272, 274, 275, 276, 279, 280, 281, 282, 283, 284, 285, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 302, 303, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 337, 338, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 354, 355, 356, 357, 360, 361, 362, 363, 364, 365, 366, 367, 368, 375, 381, 398, 400, 402], "howev": [6, 8, 250, 381, 395], "show": [6, 219, 380, 386, 396, 397], "els": [6, 80, 380, 398, 400, 401], "done": [6, 8, 377, 380, 383, 385, 395, 399], "io_tensor": 6, "optional_output_tensor": [6, 187, 219], "vector": [6, 219, 257, 305, 380, 381], "vector_of_tensor": 6, "tupl": [6, 67, 68, 69, 74, 250, 261, 262, 263, 264, 354, 360, 381], "tuple_of_tensor": 6, "vector_of_optional_tensor": 6, "some_crazy_tuple_of_tensor": 6, "return": [6, 10, 12, 13, 14, 15, 16, 17, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 333, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 378, 380, 381, 397, 398, 399, 400, 401], "spec": [6, 74, 380, 381], "singl": [6, 359, 376, 384, 385, 386, 395], "tensorspec": [6, 381], "spec_return_value_t": 6, "tensor_return_value_t": 6, "note": [6, 66, 107, 161, 219, 246, 247, 272, 291, 354, 378, 380, 381, 383, 385, 386, 395, 397, 401, 402], "should": [6, 8, 69, 136, 141, 159, 165, 182, 219, 222, 243, 250, 258, 279, 337, 351, 377, 380, 384, 385, 395, 396, 397, 398, 400], "same": [6, 8, 18, 20, 22, 24, 39, 46, 47, 49, 50, 51, 67, 68, 80, 82, 89, 91, 92, 93, 107, 108, 115, 117, 122, 132, 133, 134, 135, 143, 144, 155, 159, 165, 175, 176, 178, 179, 182, 187, 199, 200, 203, 204, 207, 208, 209, 210, 214, 215, 217, 219, 223, 227, 229, 230, 236, 237, 238, 241, 243, 247, 249, 257, 258, 261, 262, 263, 264, 299, 301, 329, 333, 335, 351, 354, 358, 370, 372, 375, 381, 385, 386, 395, 397], "pattern": [6, 63, 229, 384], "e": [6, 219, 381, 385, 386, 402], "singlecor": 6, "share": [6, 354, 380, 386], "override_runtime_argu": 6, "shared_variables_t": 6, "tt_metal": [6, 339, 368, 380, 381, 383, 385, 399], "kernelhandl": 6, "unary_reader_kernel_id": [6, 381], "unary_writer_kernel_id": [6, 381], "cached_program_t": 6, "cachedprogram": 6, "static": 6, "operation_attribut": 6, "tensor_arg": 6, "tensor_return_valu": 6, "void": [6, 381], "cached_program": 6, "multicor": [6, 250, 348, 349, 366, 367], "size_t": 6, "num_cor": [6, 8], "num_cores_i": 6, "program_factory_t": 6, "mandatori": [6, 381], "select": [6, 8, 44, 380, 384], "arg": [6, 118, 252, 261, 262, 263, 264, 277, 278, 339, 380, 402], "select_program_factori": 6, "valid": [6, 8, 26, 105, 117, 118, 219, 229, 230, 276, 376, 377, 380, 381, 385, 386, 399], "usual": 6, "validate_on_program_cache_miss": 6, "reus": [6, 261, 262, 263, 264], "less": [6, 178, 179, 214, 215, 265, 381, 383, 385, 402], "validate_on_program_cache_hit": 6, "compute_output_spec": [6, 381], "create_output_tensor": [6, 381], "map": [6, 32, 395, 396, 397, 398, 399, 400], "abl": 6, "prim": 6, "after": [6, 8, 106, 250, 277, 378, 384, 385, 395, 402], "keep": [6, 31, 265, 376, 386], "mind": [6, 397], "overload": [6, 339, 380, 395], "queue_id": [6, 12, 14, 18, 19, 21, 27, 31, 33, 37, 38, 44, 46, 47, 48, 49, 50, 51, 53, 63, 64, 69, 70, 80, 81, 86, 88, 89, 92, 94, 95, 97, 99, 101, 102, 104, 106, 107, 109, 110, 113, 115, 116, 117, 118, 120, 131, 132, 133, 134, 137, 138, 139, 143, 145, 154, 157, 159, 162, 166, 167, 168, 169, 170, 173, 175, 176, 178, 180, 184, 189, 190, 192, 194, 197, 199, 200, 203, 205, 207, 209, 214, 216, 222, 223, 227, 228, 232, 233, 236, 237, 239, 240, 242, 243, 247, 250, 252, 258, 259, 269, 274, 280, 281, 284, 285, 291, 293, 297, 298, 299, 300, 305, 306, 308, 310, 311, 312, 313, 317, 319, 325, 326, 327, 329, 332, 334, 335, 340, 342, 343, 348, 349, 354, 358, 359, 363, 365, 366, 367, 370, 371, 375], "automat": [6, 218, 351, 377, 378, 381, 385, 386, 395, 396], "primit": 6, "so": [6, 8, 118, 376, 378, 380, 381, 395, 401], "case": [6, 80, 130, 165, 219, 241, 260, 351, 376, 377, 381, 386, 395, 402], "hash": [6, 229, 230, 385], "stl": 6, "hash_t": 6, "compute_program_hash": 6, "create_op_performance_model": 6, "opperformancemodel": 6, "make": [6, 229, 230, 319, 354, 360, 376, 386, 399, 402], "avail": [6, 8, 368, 378, 380, 387, 388, 391, 392, 402], "constexpr": 6, "some_condition_based_on_operation_attributes_and_or_tensor_arg": 6, "true": [6, 8, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 127, 128, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 249, 250, 252, 253, 254, 255, 256, 257, 258, 259, 260, 265, 266, 267, 268, 269, 270, 271, 272, 274, 275, 276, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 351, 352, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 380, 381, 395, 396, 397, 398, 399, 400, 401, 402], "get_logical_shap": 6, "tensorlayout": 6, "pageconfig": 6, "output_spec": 6, "create_device_tensor": 6, "42": [6, 396, 399, 401], "single_core_program_factori": 6, "work_split": 6, "output_tensor": [6, 12, 14, 18, 27, 31, 33, 38, 46, 47, 48, 49, 50, 51, 53, 63, 70, 80, 86, 88, 89, 92, 94, 95, 97, 99, 101, 102, 104, 109, 113, 115, 116, 120, 126, 131, 132, 133, 134, 137, 138, 139, 143, 145, 154, 157, 159, 166, 167, 168, 169, 170, 173, 175, 176, 178, 180, 184, 189, 190, 192, 194, 197, 199, 200, 203, 205, 207, 209, 214, 216, 223, 227, 228, 232, 236, 237, 239, 240, 242, 247, 258, 259, 269, 274, 280, 281, 284, 285, 293, 297, 298, 299, 305, 306, 308, 310, 311, 312, 313, 319, 325, 326, 327, 329, 335, 340, 342, 343, 354, 363, 365, 370, 371, 375, 381, 395, 400, 402], "src_buffer": 6, "dst_buffer": 6, "dataformat": 6, "cb_data_format": 6, "datatype_to_dataformat_convert": 6, "uint32_t": [6, 109], "single_tile_s": 6, "tiles": 6, "cb_data_format_output": 6, "single_tile_size_output": 6, "num_til": 6, "volum": 6, "constant": [6, 381, 386], "tile_hw": 6, "idevic": [6, 67, 68, 69, 261, 262, 339, 380], "corecoord": [6, 381], "compute_with_storage_grid_s": 6, "num_cores_x": [6, 376, 397], "x": [6, 39, 69, 187, 219, 253, 358, 359, 380, 381, 383, 385, 386, 396, 397, 400, 401], "y": [6, 39, 253, 380, 381, 385, 386, 396, 397, 401], "all_cor": 6, "core_group_1": 6, "core_group_2": 6, "num_tiles_per_core_group_1": 6, "num_tiles_per_core_group_2": 6, "split_work_to_cor": 6, "src0_cb_index": 6, "cbindex": 6, "c_0": 6, "num_input_til": 6, "circularbufferconfig": 6, "cb_src0_config": 6, "set_page_s": 6, "cb_src0": 6, "createcircularbuff": 6, "output_cb_index": 6, "c_2": 6, "num_output_til": 6, "cb_output_config": 6, "cb_output": 6, "src_is_dram": 6, "buffer_typ": [6, 380, 381], "buffertyp": [6, 380, 381], "dram": [6, 9, 350, 352, 380, 381, 386, 395], "reader_compile_time_arg": 6, "dst_is_dram": 6, "writer_compile_time_arg": 6, "createkernel": 6, "eltwis": [6, 123, 260, 286, 381], "kernel": [6, 8, 61, 67, 68, 69, 106, 110, 187, 219, 222, 261, 262, 263, 264, 318, 381, 383, 385, 396], "dataflow": 6, "reader_unary_interleaved_start_id": 6, "readerdatamovementconfig": 6, "writer_unary_interleaved_start_id": 6, "writerdatamovementconfig": 6, "compute_kernel_args_group_1": 6, "per_core_block_cnt": 6, "per_core_block_s": 6, "math_approx_mod": 6, "fals": [6, 8, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 249, 250, 252, 253, 254, 255, 256, 257, 258, 259, 260, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 381, 395, 396, 397, 398, 399, 400, 401, 402], "eltwise_unary_kernel_group_1_id": 6, "eltwise_sfpu": 6, "computeconfig": 6, "math_fidel": [6, 400], "mathfidel": [6, 400], "hifi4": [6, 385, 399], "compile_arg": 6, "rang": [6, 18, 30, 46, 47, 48, 49, 50, 51, 72, 117, 118, 133, 158, 175, 177, 192, 228, 234, 236, 255, 293, 295, 315, 335, 340, 341, 342, 380, 381, 386], "compute_kernel_args_group_2": 6, "eltwise_unary_kernel_group_2_id": 6, "num_tiles_written": 6, "num_tiles_per_cor": 6, "tt_assert": 6, "setruntimearg": 6, "address": [6, 381], "move": [6, 8, 125, 126, 377, 378, 380, 381, 395, 397, 398, 399], "shared_vari": 6, "runtime_arg": [6, 381], "getruntimearg": [6, 381], "multi_core_program_factori": 6, "compositeexampleoper": 6, "composite_exampl": 6, "another_copi": 6, "_pybind": 6, "example_pybind": 6, "pybind11": 6, "h": [6, 9, 44, 67, 68, 69, 84, 106, 117, 118, 222, 368, 381, 386], "pybind": 6, "bind_example_oper": 6, "r": [6, 253, 383, 399], "expos": 6, "logic": [6, 203, 204, 207, 208, 210, 219, 229, 230], "self": [6, 376, 380, 381, 386, 400], "correct": 6, "specif": [6, 8, 31, 112, 219, 248, 359, 377, 381, 383, 386, 387, 399], "pybind_overload_t": 6, "decltyp": 6, "examples_pybind": 6, "py_modul": 6, "final": [6, 219, 376, 377, 378, 384, 395], "wherev": 6, "want": [6, 378, 383, 396, 398, 402], "compar": [6, 92, 134, 143, 178, 214, 219, 237, 396], "its": [6, 61, 69, 91, 250, 317, 376, 377, 380, 381, 384, 385, 386, 402], "equival": [6, 31, 112, 354, 360, 386], "import": [6, 108, 112, 222, 376, 377, 378, 383, 385, 395, 396, 397, 398, 399, 400, 401, 402], "signatur": 6, "And": [6, 376, 380, 381, 386, 395, 396], "ignor": 6, "kwarg": [6, 261, 262, 263, 264, 277, 278, 339, 380, 402], "def": [6, 376, 397, 398, 399, 400, 401, 402], "golden_funct": [6, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 249, 250, 252, 253, 254, 255, 256, 257, 258, 259, 260, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375], "befor": [6, 8, 32, 108, 250, 261, 262, 263, 264, 278, 319, 377, 381, 386], "some": [6, 339, 354, 381, 402], "postprocess": 6, "manual": [6, 376, 402], "pack": [6, 366, 367], "preprocess_golden_function_input": [6, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 249, 250, 252, 253, 254, 255, 256, 257, 258, 259, 260, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375], "ttnn_input_tensor": 6, "postprocess_golden_function_output": [6, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 249, 250, 252, 253, 254, 255, 256, 257, 258, 259, 260, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375], "torch_output_tensor": [6, 402], "becaus": [6, 386, 395, 396, 397], "wa": [6, 107, 377, 385, 386, 395, 401], "class": [8, 9, 376, 377, 380, 381, 385, 386, 399, 400, 401], "pybind11_object": [8, 9, 386], "flag": [8, 32, 381, 399], "properti": [8, 9, 386], "height": [8, 9, 61, 63, 67, 68, 69, 74, 117, 118, 131, 140, 219, 222, 246, 261, 262, 263, 264, 351, 374, 381, 385, 386], "gener": [8, 88, 109, 118, 219, 253, 377, 381, 385, 395, 396, 397, 398, 399, 400], "l1": [8, 9, 74, 187, 219, 248, 350, 352, 380, 386, 397], "divid": [8, 9, 81, 355, 356], "among": 8, "further": [8, 396], "subdivid": 8, "within": [8, 30, 47, 50, 222, 377, 380, 381, 386, 387], "possibl": [8, 219, 353, 376, 381, 398], "equal": [8, 92, 93, 112, 134, 135, 165, 178, 179, 237, 238, 381, 402], "output_matrix_height_per_cor": 8, "lead": 8, "larg": [8, 9, 319, 376, 386, 398], "temporari": 8, "circular": [8, 381], "oom": 8, "must": [8, 27, 31, 44, 67, 68, 89, 106, 108, 235, 259, 261, 262, 269, 291, 317, 348, 349, 350, 354, 366, 367, 378, 380, 381, 383, 386], "32": [8, 26, 61, 63, 64, 75, 106, 118, 129, 131, 136, 140, 141, 187, 219, 243, 244, 245, 246, 249, 250, 252, 260, 266, 273, 279, 289, 290, 301, 317, 318, 337, 350, 351, 352, 365, 374, 378, 380, 381, 386, 395, 396, 398, 399, 402], "evenli": [8, 30], "reduc": [8, 9, 31, 265, 276, 354, 381], "width": [8, 9, 61, 67, 68, 69, 74, 117, 118, 131, 140, 219, 222, 246, 261, 262, 263, 264, 351, 357, 374, 381, 385], "prevent": 8, "greater": [8, 63, 78, 117, 118, 134, 135, 143, 144, 185, 235, 381, 402], "n150": 8, "thats": 8, "64": [8, 63, 74, 75, 129, 136, 141, 187, 219, 252, 273, 279, 317, 318, 337, 350, 351, 352, 380, 386, 397, 398, 399, 400, 402], "2048": [8, 402], "divisor": [8, 133, 270], "halv": 8, "appli": [8, 9, 12, 14, 18, 27, 33, 38, 44, 48, 53, 61, 67, 68, 69, 70, 86, 92, 94, 95, 97, 99, 101, 102, 106, 108, 109, 110, 111, 113, 115, 120, 133, 134, 136, 137, 139, 140, 141, 143, 145, 154, 157, 166, 167, 168, 169, 170, 175, 176, 178, 180, 184, 187, 189, 190, 192, 194, 197, 199, 200, 203, 205, 207, 209, 214, 216, 219, 222, 223, 227, 228, 236, 237, 239, 242, 250, 261, 262, 263, 264, 274, 279, 280, 281, 284, 285, 293, 297, 299, 305, 306, 308, 310, 311, 313, 319, 325, 327, 329, 335, 337, 340, 342, 363, 365, 377, 381], "sigmoid_approx": 8, "determin": [8, 9, 69, 219, 229, 230, 381, 386, 395, 396, 397, 398, 399, 400], "examin": 8, "assum": [8, 84, 111, 126, 339, 368, 387], "alreadi": [8, 69, 229, 230, 248, 378, 387, 399, 401], "alwai": [8, 387, 395, 398, 399], "even": [8, 27, 75, 218, 219, 378, 381], "format": [8, 9, 32, 39, 67, 68, 69, 106, 125, 126, 130, 222, 261, 262, 263, 264, 274, 384, 386], "out_channel": [8, 67, 68, 69, 261, 262, 263, 264, 381, 400], "in_channel": [8, 67, 68, 69, 261, 262, 263, 264, 381, 400], "kernel_height": [8, 67, 68, 263, 264], "kernel_width": [8, 67, 68, 263, 264], "grid": [8, 187, 219, 358, 359, 366, 376, 386], "boolean": [8, 163, 164, 342, 354, 381], "indic": [8, 31, 88, 89, 112, 243, 317, 367, 380, 381], "whether": [8, 31, 32, 75, 187, 219, 222, 229, 230, 263, 264, 348, 349, 354, 359, 360, 366, 367, 381], "conv": [8, 67, 68, 69, 84, 400], "halo": [8, 9, 222], "micro": 8, "anoth": [8, 107], "float32": [8, 27, 30, 44, 90, 91, 108, 115, 123, 133, 159, 175, 223, 227, 258, 374, 375, 376, 380, 385, 386, 395, 400, 402], "bfloat16": [8, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 107, 108, 109, 110, 113, 114, 115, 116, 119, 120, 121, 122, 123, 124, 125, 127, 128, 129, 130, 131, 132, 134, 135, 136, 137, 138, 139, 140, 141, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 173, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 249, 252, 253, 254, 255, 256, 257, 258, 259, 260, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 279, 280, 281, 282, 283, 284, 285, 286, 287, 289, 290, 291, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 361, 362, 363, 364, 365, 366, 367, 369, 370, 371, 372, 373, 374, 375, 376, 378, 380, 381, 385, 386, 395, 396, 397, 399, 400, 402], "doubl": 8, "allow": [8, 219, 377, 380, 381, 384], "stall": 8, "reader": [8, 229], "improv": [8, 319, 377], "increas": 8, "usag": [8, 377, 383], "writer": 8, "carri": [8, 285], "bottleneck": 8, "re": [8, 376, 381, 382, 396], "overwrit": 8, "either": [8, 9, 74, 219, 351, 377, 381, 383, 386, 387], "tile": [8, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 29, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 64, 70, 71, 72, 73, 76, 77, 78, 79, 80, 81, 82, 83, 86, 87, 90, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 108, 109, 110, 113, 114, 115, 116, 118, 119, 120, 121, 122, 123, 124, 127, 128, 130, 133, 134, 135, 136, 137, 138, 139, 141, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 165, 166, 167, 168, 169, 170, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 219, 221, 223, 226, 227, 228, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 244, 245, 249, 251, 252, 254, 255, 256, 257, 258, 259, 260, 266, 267, 268, 269, 270, 274, 275, 279, 280, 281, 282, 283, 284, 285, 286, 287, 289, 291, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 332, 333, 334, 335, 337, 338, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 353, 358, 359, 361, 362, 363, 364, 365, 366, 367, 370, 371, 372, 373, 374, 375, 380, 381, 385, 386, 395, 396, 399], "row_major": [8, 30, 31, 74, 90, 91, 130, 131, 132, 244, 245, 246, 348, 349, 366, 367, 374, 375, 378, 380, 381, 385, 395, 396, 400], "expect": [8, 69, 106, 118, 222, 360, 377, 380, 381, 384, 395], "recommend": [8, 177, 376, 383, 395, 402], "faster": [8, 385, 397], "fragment": 8, "ideal": [8, 386], "face": 8, "reshard": [8, 400], "current": [8, 61, 63, 74, 229, 230, 263, 264, 350, 359, 381, 383, 386, 387, 395, 402], "anywai": 8, "dimens": [8, 9, 26, 27, 31, 61, 63, 64, 108, 112, 131, 136, 140, 141, 162, 219, 220, 224, 225, 246, 250, 252, 260, 265, 266, 276, 279, 288, 290, 291, 317, 318, 331, 336, 337, 351, 353, 354, 357, 358, 359, 360, 369, 374, 378, 380, 381, 386], "previou": [8, 89, 378], "due": [8, 274, 385, 386, 395], "stride": [8, 67, 68, 69, 84, 222, 261, 262, 263, 264, 381, 395, 400], "dilat": [8, 67, 68, 69, 222, 261, 262, 263, 264, 381, 400], "tensormemorylayout": [8, 222, 380, 381, 400], "heurist": 8, "height_shard": [8, 400], "block_shard": [8, 222, 400], "width_shard": 8, "orient": [8, 74, 386], "major": [8, 54, 106, 116, 118, 119, 121, 128, 243, 294, 309, 364, 380, 386, 395, 396], "column": [8, 385, 386, 399], "bia": [8, 44, 45, 67, 68, 69, 106, 142, 174, 187, 261, 262, 263, 264, 292, 376, 381, 397, 400], "respons": [8, 399], "unspecifi": 8, "place": [9, 44, 93, 135, 144, 171, 172, 179, 215, 222, 238, 293, 350, 356, 380, 386], "too": [9, 402], "fit": [9, 290], "conv2d_dram": 9, "happen": 9, "number": [9, 18, 19, 22, 24, 26, 45, 63, 67, 68, 69, 80, 81, 82, 92, 93, 105, 117, 118, 122, 123, 124, 134, 135, 143, 144, 176, 178, 179, 183, 199, 200, 203, 207, 209, 214, 215, 217, 219, 220, 222, 223, 224, 225, 227, 233, 236, 237, 238, 243, 250, 252, 259, 260, 261, 262, 263, 264, 276, 286, 287, 288, 290, 291, 299, 329, 331, 332, 335, 336, 349, 354, 355, 356, 369, 370, 380, 381, 384, 385, 386, 397, 401], "along": [9, 107, 108, 112, 266, 317, 318, 354, 357, 380, 381, 386], "correspond": [9, 26, 88, 89, 276, 359, 380, 381], "divis": [9, 122, 136, 141, 269, 279, 337, 380, 381, 386], "smaller": [9, 386], "rest": [9, 118], "n": [9, 44, 63, 67, 68, 69, 84, 106, 108, 117, 118, 222, 243, 256, 257, 368, 381, 383, 385, 395, 396, 399, 401], "w": [9, 44, 67, 68, 69, 84, 106, 117, 118, 222, 368, 380, 381, 385], "prefer": 9, "much": [9, 381, 385, 396], "larger": 9, "_ttnn": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 252, 253, 254, 255, 256, 257, 258, 259, 260, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 351, 352, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 380, 381, 386], "multi_devic": [10, 11, 125, 126, 248, 339, 380], "meshdevic": [10, 11, 26, 62, 90, 91, 125, 126, 131, 132, 218, 246, 247, 248, 276, 339, 351, 374, 375, 380], "default": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 63, 64, 65, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 107, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 136, 137, 138, 139, 140, 141, 142, 143, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 162, 163, 164, 165, 166, 167, 168, 169, 170, 173, 174, 175, 176, 177, 178, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 205, 206, 207, 209, 211, 212, 213, 214, 216, 217, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 252, 253, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 273, 274, 275, 276, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 378, 380, 381, 385, 396, 402], "plan": [10, 11, 125, 126, 251], "deprec": [10, 11, 125, 126, 251, 398], "futur": [10, 11, 125, 126, 251], "arg0": [11, 380, 386], "none": [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 62, 63, 64, 65, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 91, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 136, 137, 138, 139, 140, 141, 142, 143, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 205, 206, 207, 209, 211, 212, 213, 214, 216, 217, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243, 244, 245, 246, 247, 249, 250, 252, 253, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 380, 381, 395, 396, 397, 398, 399, 400], "device_id": [11, 62, 75, 88, 89, 129, 218, 248, 273, 339, 350, 351, 352, 380, 395, 396, 397, 398, 400, 402], "python_fully_qualified_nam": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 249, 250, 252, 253, 254, 255, 256, 257, 258, 259, 260, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375], "abs_t": 12, "object": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 127, 128, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 252, 253, 254, 255, 256, 257, 258, 259, 260, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 351, 352, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 380, 384, 401], "default_preprocess_golden_function_input": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 249, 252, 253, 254, 255, 256, 257, 258, 259, 260, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375], "_golden_funct": [12, 15, 18, 23, 25, 28, 29, 30, 35, 38, 42, 52, 53, 61, 63, 65, 66, 68, 70, 72, 76, 78, 81, 88, 90, 91, 92, 94, 95, 97, 99, 101, 102, 111, 113, 116, 120, 129, 130, 132, 134, 137, 139, 142, 143, 145, 148, 150, 153, 157, 159, 160, 161, 163, 164, 166, 167, 168, 169, 170, 174, 176, 178, 183, 184, 185, 187, 189, 190, 192, 194, 197, 199, 200, 204, 205, 208, 210, 214, 216, 219, 221, 228, 233, 234, 236, 237, 239, 242, 246, 247, 250, 252, 253, 254, 256, 259, 267, 271, 272, 273, 274, 280, 281, 288, 289, 290, 291, 292, 297, 305, 306, 308, 310, 311, 313, 315, 318, 319, 320, 323, 325, 327, 335, 338, 340, 342, 350, 351, 352, 353, 355, 356, 357, 360, 361, 362, 368, 371, 374, 375], "default_postprocess_golden_function_output": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 249, 252, 253, 254, 255, 256, 257, 258, 259, 260, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375], "is_cpp_oper": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 249, 250, 252, 253, 254, 255, 256, 257, 258, 259, 260, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375], "is_experiment": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 249, 250, 252, 253, 254, 255, 256, 257, 258, 259, 260, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375], "element": [12, 14, 27, 31, 33, 38, 48, 53, 63, 67, 68, 69, 70, 86, 94, 95, 97, 99, 101, 102, 108, 109, 112, 113, 115, 120, 136, 137, 139, 140, 141, 145, 154, 157, 166, 167, 168, 169, 170, 180, 184, 189, 190, 192, 194, 197, 205, 216, 228, 239, 242, 243, 250, 257, 258, 261, 262, 263, 264, 265, 269, 274, 279, 280, 281, 284, 285, 290, 293, 297, 305, 306, 308, 310, 311, 313, 319, 325, 327, 337, 340, 342, 354, 363, 365, 367, 381, 386, 395], "wise": [12, 14, 27, 33, 38, 48, 53, 70, 86, 94, 95, 97, 99, 101, 102, 109, 113, 115, 120, 136, 137, 139, 141, 145, 154, 157, 166, 167, 168, 169, 170, 180, 184, 189, 190, 192, 194, 197, 205, 216, 228, 239, 242, 258, 269, 274, 279, 280, 281, 284, 285, 293, 297, 305, 306, 308, 310, 311, 313, 319, 325, 327, 337, 340, 342, 363, 365, 381], "mathrm": [12, 14, 18, 20, 27, 33, 38, 39, 46, 47, 48, 49, 50, 51, 53, 70, 80, 82, 86, 92, 93, 94, 95, 97, 99, 101, 102, 109, 113, 115, 120, 122, 123, 133, 134, 135, 136, 137, 139, 140, 141, 143, 144, 145, 154, 155, 157, 159, 165, 166, 167, 168, 169, 170, 175, 176, 178, 179, 180, 182, 184, 189, 190, 192, 194, 197, 199, 200, 203, 204, 205, 207, 208, 209, 210, 214, 215, 216, 228, 236, 237, 238, 239, 241, 242, 249, 257, 258, 260, 274, 279, 280, 281, 284, 285, 286, 293, 297, 299, 301, 305, 306, 308, 310, 311, 313, 319, 325, 327, 329, 333, 335, 337, 340, 342, 363, 365, 372, 381], "_tensor": [12, 14, 18, 20, 27, 33, 38, 39, 46, 47, 48, 49, 50, 51, 53, 70, 80, 82, 86, 92, 93, 94, 95, 97, 99, 101, 102, 109, 113, 115, 120, 122, 123, 133, 134, 135, 136, 137, 139, 140, 141, 143, 144, 145, 154, 155, 157, 159, 165, 166, 167, 168, 169, 170, 175, 176, 178, 179, 180, 182, 184, 189, 190, 192, 194, 197, 199, 200, 203, 204, 205, 207, 208, 209, 210, 214, 215, 216, 228, 236, 237, 238, 239, 241, 242, 249, 257, 258, 260, 274, 279, 280, 281, 284, 285, 286, 293, 297, 299, 301, 305, 306, 308, 310, 311, 313, 319, 325, 327, 329, 333, 335, 337, 340, 342, 363, 365, 372], "_i": [12, 14, 18, 33, 38, 39, 46, 47, 48, 49, 50, 51, 53, 70, 80, 86, 92, 94, 95, 97, 99, 101, 102, 109, 113, 115, 120, 133, 134, 136, 137, 139, 140, 141, 143, 145, 154, 155, 157, 159, 166, 167, 168, 169, 170, 175, 178, 180, 184, 189, 190, 192, 194, 197, 203, 204, 205, 207, 208, 209, 210, 214, 216, 228, 236, 237, 239, 241, 242, 257, 258, 274, 279, 280, 281, 284, 285, 293, 297, 299, 301, 305, 306, 308, 310, 311, 313, 319, 325, 327, 335, 337, 340, 342, 363, 365, 372], "verb": [12, 14, 33, 38, 46, 47, 48, 49, 50, 51, 53, 70, 82, 86, 99, 102, 109, 113, 115, 120, 122, 123, 133, 136, 141, 154, 157, 159, 166, 167, 168, 169, 170, 175, 176, 180, 182, 189, 190, 192, 194, 197, 199, 200, 228, 239, 260, 274, 279, 280, 281, 284, 285, 286, 293, 301, 305, 306, 308, 310, 311, 313, 325, 327, 329, 337, 340, 365], "complextensor": [12, 13, 19, 28, 29, 65, 66, 80, 81, 160, 161, 163, 164, 233, 253, 254, 271, 272, 274, 275, 332], "keyword": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 70, 71, 72, 73, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89, 91, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 127, 128, 130, 133, 134, 136, 137, 138, 139, 140, 141, 142, 143, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 173, 174, 175, 176, 177, 178, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 205, 206, 207, 209, 211, 212, 213, 214, 216, 217, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243, 244, 245, 248, 249, 250, 252, 253, 254, 255, 256, 257, 258, 259, 260, 265, 266, 267, 268, 269, 270, 271, 272, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373], "memory_config": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 76, 77, 78, 79, 80, 81, 82, 83, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 127, 128, 130, 131, 132, 133, 134, 136, 137, 138, 139, 140, 141, 142, 143, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 173, 174, 175, 176, 177, 178, 180, 181, 182, 183, 184, 185, 186, 187, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 205, 206, 207, 209, 211, 212, 213, 214, 216, 217, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243, 244, 245, 246, 247, 249, 250, 252, 253, 254, 255, 256, 257, 258, 259, 260, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 380, 381, 396, 397, 400, 402], "prealloc": [12, 14, 18, 19, 21, 27, 31, 33, 37, 38, 44, 46, 47, 48, 49, 50, 51, 53, 63, 64, 70, 80, 81, 86, 88, 89, 92, 94, 95, 97, 99, 101, 102, 104, 107, 108, 109, 110, 112, 113, 115, 116, 120, 131, 132, 133, 134, 137, 138, 139, 143, 145, 154, 157, 159, 166, 167, 168, 169, 170, 173, 175, 176, 178, 180, 184, 189, 190, 192, 194, 197, 199, 200, 203, 205, 207, 209, 214, 216, 223, 227, 228, 232, 233, 236, 237, 239, 240, 242, 247, 258, 259, 269, 274, 280, 281, 284, 285, 293, 297, 298, 299, 300, 305, 306, 308, 310, 311, 312, 313, 319, 325, 326, 327, 329, 332, 334, 335, 340, 342, 343, 354, 363, 365, 370, 371, 375], "queue": [12, 14, 18, 19, 21, 27, 31, 33, 37, 38, 44, 46, 47, 48, 49, 50, 51, 53, 63, 64, 69, 70, 80, 81, 86, 88, 89, 92, 94, 95, 97, 99, 101, 102, 104, 106, 107, 109, 110, 113, 115, 116, 117, 118, 120, 130, 131, 132, 133, 134, 137, 138, 139, 143, 145, 154, 157, 159, 162, 166, 167, 168, 169, 170, 173, 175, 176, 178, 180, 184, 189, 190, 192, 194, 197, 199, 200, 203, 205, 207, 209, 214, 216, 222, 223, 227, 228, 232, 233, 236, 237, 239, 240, 242, 243, 247, 250, 252, 258, 259, 269, 274, 280, 281, 284, 285, 291, 293, 297, 298, 299, 300, 305, 306, 308, 310, 311, 312, 313, 319, 325, 326, 327, 329, 332, 334, 335, 339, 340, 342, 343, 348, 349, 353, 354, 358, 359, 363, 365, 366, 367, 370, 371, 375, 380], "id": [12, 14, 18, 19, 21, 27, 31, 33, 37, 38, 44, 46, 47, 48, 49, 50, 51, 53, 63, 64, 69, 70, 80, 81, 86, 88, 89, 92, 94, 95, 97, 99, 101, 102, 104, 106, 107, 109, 110, 113, 115, 116, 117, 118, 120, 130, 131, 132, 133, 134, 137, 138, 139, 143, 145, 154, 157, 159, 162, 166, 167, 168, 169, 170, 173, 175, 176, 178, 180, 184, 189, 190, 192, 194, 197, 199, 200, 203, 205, 207, 209, 214, 216, 218, 222, 223, 227, 228, 232, 233, 236, 237, 239, 240, 242, 243, 247, 248, 250, 252, 258, 259, 269, 274, 280, 281, 284, 285, 291, 293, 297, 298, 299, 300, 305, 306, 308, 310, 311, 312, 313, 319, 325, 326, 327, 329, 332, 334, 335, 339, 340, 342, 343, 348, 349, 353, 354, 358, 359, 363, 365, 366, 367, 370, 371, 375, 380, 385], "tile_layout": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 64, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 86, 87, 89, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 109, 110, 113, 114, 115, 116, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 133, 134, 135, 136, 137, 138, 139, 141, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 165, 166, 167, 168, 169, 170, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 219, 221, 223, 226, 227, 228, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 244, 245, 246, 247, 255, 257, 258, 259, 266, 267, 268, 269, 270, 274, 275, 279, 280, 281, 282, 283, 284, 285, 286, 287, 289, 290, 293, 294, 295, 297, 298, 299, 300, 301, 302, 303, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 332, 333, 334, 335, 337, 338, 340, 341, 342, 343, 344, 345, 346, 347, 351, 354, 361, 362, 363, 364, 365, 370, 371, 372, 373, 374, 375, 376, 380, 386, 395, 396, 397, 400, 402], "unary_backward": [13, 15, 17, 34, 36, 41, 43, 54, 56, 58, 60, 71, 73, 77, 79, 83, 87, 96, 98, 100, 103, 104, 114, 116, 119, 121, 128, 138, 147, 149, 151, 153, 158, 181, 186, 191, 193, 195, 196, 198, 212, 213, 235, 240, 256, 259, 266, 268, 270, 275, 282, 283, 289, 294, 296, 298, 303, 307, 309, 312, 314, 316, 320, 322, 324, 326, 328, 341, 343, 345, 347, 364], "abs_bw_t": 13, "_golden_function_ab": 13, "backward": [13, 15, 17, 19, 21, 23, 25, 29, 34, 36, 37, 40, 41, 43, 45, 54, 56, 58, 60, 64, 66, 71, 73, 77, 79, 81, 83, 87, 89, 96, 98, 100, 103, 104, 110, 114, 116, 119, 121, 124, 128, 138, 147, 149, 151, 153, 156, 158, 161, 177, 181, 183, 186, 191, 193, 195, 196, 198, 201, 202, 212, 213, 221, 226, 233, 235, 240, 254, 256, 259, 266, 268, 270, 272, 275, 282, 283, 287, 289, 294, 296, 298, 300, 303, 307, 309, 312, 314, 316, 320, 322, 324, 326, 328, 330, 332, 334, 341, 343, 345, 347, 364, 371, 373, 381], "given": [13, 15, 17, 19, 21, 23, 25, 29, 31, 34, 36, 37, 40, 41, 43, 45, 54, 56, 58, 60, 64, 66, 69, 71, 73, 77, 79, 81, 83, 84, 87, 91, 96, 98, 100, 103, 104, 107, 108, 114, 116, 119, 121, 124, 128, 138, 147, 149, 151, 153, 156, 158, 161, 177, 181, 183, 186, 191, 193, 195, 196, 198, 201, 202, 212, 213, 221, 226, 229, 230, 233, 235, 240, 248, 251, 254, 256, 259, 266, 268, 270, 272, 275, 282, 283, 287, 289, 290, 294, 296, 298, 300, 303, 307, 309, 312, 314, 316, 320, 322, 324, 326, 328, 330, 332, 334, 341, 343, 345, 347, 354, 359, 364, 368, 371, 373, 376, 380, 381, 385, 395], "grad_tensor": [13, 15, 17, 19, 21, 23, 25, 29, 34, 36, 37, 40, 41, 43, 45, 54, 56, 58, 60, 64, 66, 71, 73, 77, 79, 81, 83, 87, 89, 96, 98, 100, 103, 104, 110, 114, 116, 119, 121, 124, 128, 138, 147, 149, 151, 153, 156, 158, 161, 177, 181, 183, 186, 191, 193, 195, 196, 198, 201, 202, 212, 213, 221, 226, 233, 235, 240, 254, 256, 259, 266, 268, 270, 272, 275, 282, 283, 287, 289, 294, 296, 298, 300, 303, 307, 309, 312, 314, 316, 320, 322, 324, 326, 328, 330, 332, 334, 341, 343, 345, 347, 364, 371, 373], "gradient": [13, 15, 17, 19, 21, 23, 25, 34, 36, 37, 40, 41, 43, 45, 54, 56, 58, 60, 64, 69, 71, 73, 77, 79, 81, 83, 87, 89, 96, 98, 100, 103, 104, 110, 114, 116, 119, 121, 124, 128, 138, 147, 149, 151, 153, 156, 158, 177, 181, 183, 186, 191, 193, 195, 196, 198, 201, 202, 212, 213, 221, 226, 233, 235, 240, 256, 259, 266, 268, 270, 275, 282, 283, 287, 289, 294, 296, 298, 300, 303, 307, 309, 312, 314, 316, 320, 322, 324, 326, 328, 330, 332, 334, 341, 343, 345, 347, 364, 371, 373], "list": [13, 15, 17, 18, 19, 21, 23, 25, 29, 31, 34, 36, 37, 40, 41, 43, 45, 54, 56, 58, 60, 63, 64, 66, 71, 73, 74, 77, 79, 81, 83, 84, 87, 88, 90, 92, 96, 98, 100, 103, 104, 114, 116, 119, 121, 124, 126, 128, 133, 134, 138, 143, 147, 149, 151, 153, 156, 158, 161, 175, 176, 177, 178, 181, 183, 186, 187, 191, 193, 195, 196, 198, 199, 200, 201, 202, 203, 207, 209, 212, 213, 214, 219, 221, 222, 223, 226, 227, 233, 235, 236, 237, 240, 243, 250, 251, 252, 254, 256, 259, 260, 265, 266, 268, 270, 272, 275, 282, 283, 287, 289, 294, 296, 298, 299, 300, 303, 307, 309, 312, 314, 316, 317, 320, 322, 324, 326, 328, 329, 330, 332, 334, 335, 339, 341, 343, 345, 347, 354, 359, 364, 365, 366, 367, 371, 373, 380, 381, 382], "about": [13, 23, 45, 124, 138, 191, 193, 195, 196, 198, 201, 202, 212, 213, 259, 266, 274, 275, 287, 320, 326, 373, 386, 396], "requires_grad": [13, 15, 17, 19, 21, 23, 25, 34, 36, 37, 40, 41, 43, 45, 54, 56, 58, 60, 64, 71, 73, 77, 79, 81, 87, 89, 96, 98, 100, 103, 104, 110, 114, 116, 119, 121, 124, 128, 138, 147, 149, 151, 153, 156, 158, 177, 181, 183, 186, 191, 193, 195, 196, 198, 201, 202, 212, 213, 221, 226, 233, 235, 240, 259, 266, 268, 270, 275, 282, 283, 287, 289, 294, 298, 300, 303, 307, 309, 312, 314, 316, 320, 322, 324, 326, 328, 330, 332, 334, 341, 343, 345, 347, 364, 371, 373], "acos_t": 14, "_golden_function_aco": 14, "rand": [14, 16, 27, 33, 35, 38, 42, 44, 48, 52, 53, 64, 70, 72, 76, 88, 94, 99, 102, 108, 113, 120, 127, 136, 139, 141, 145, 157, 166, 167, 168, 169, 170, 184, 185, 189, 190, 192, 194, 197, 205, 206, 216, 228, 239, 242, 244, 245, 249, 260, 266, 267, 279, 280, 281, 289, 290, 301, 308, 310, 311, 313, 315, 323, 325, 327, 337, 338, 340, 344, 361, 362, 395, 398, 400, 402], "acos_bw_t": 15, "invers": [15, 17, 34, 36, 41, 43], "cosin": [15, 17, 71, 73, 111], "input_tensor_a": [15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 34, 36, 37, 39, 40, 41, 43, 45, 46, 47, 49, 50, 51, 54, 56, 64, 71, 73, 77, 79, 80, 81, 82, 87, 92, 93, 96, 98, 100, 103, 114, 119, 121, 122, 123, 124, 128, 133, 134, 135, 143, 144, 147, 149, 151, 155, 156, 158, 162, 165, 175, 176, 177, 178, 179, 181, 183, 186, 187, 191, 193, 195, 196, 198, 199, 200, 201, 202, 203, 204, 207, 208, 209, 210, 212, 213, 214, 215, 217, 219, 221, 222, 223, 226, 227, 233, 235, 236, 237, 238, 241, 249, 257, 259, 260, 268, 270, 275, 282, 283, 286, 287, 294, 299, 300, 301, 303, 307, 309, 314, 316, 322, 324, 328, 329, 330, 332, 333, 334, 335, 341, 345, 364, 370, 371, 372, 373, 395, 402], "acosh_t": 16, "_golden_function_acosh": [16, 17], "acosh_bw_t": 17, "hyperbol": [17, 36, 43, 73, 316, 343], "add_t": 18, "input_tensor_b": [18, 19, 20, 21, 22, 23, 24, 25, 37, 39, 40, 45, 46, 47, 49, 50, 51, 64, 80, 81, 82, 92, 93, 122, 123, 124, 133, 134, 135, 143, 144, 155, 156, 162, 165, 175, 176, 177, 178, 179, 183, 187, 199, 200, 201, 202, 203, 204, 207, 208, 209, 210, 214, 215, 217, 219, 221, 223, 226, 227, 233, 236, 237, 238, 241, 249, 260, 286, 287, 299, 300, 301, 329, 330, 332, 333, 334, 335, 370, 371, 372, 373, 395, 402], "_a": [18, 20, 39, 46, 47, 49, 50, 51, 80, 82, 92, 93, 122, 123, 133, 134, 135, 143, 144, 155, 165, 175, 176, 178, 179, 199, 200, 203, 204, 207, 208, 209, 210, 214, 215, 236, 237, 238, 241, 249, 260, 286, 299, 301, 329, 333, 335, 372], "_b": [18, 20, 39, 46, 47, 49, 50, 51, 80, 82, 92, 93, 122, 123, 133, 134, 135, 143, 144, 155, 165, 175, 176, 178, 179, 199, 200, 203, 204, 207, 208, 209, 210, 214, 215, 236, 237, 238, 241, 249, 260, 286, 299, 301, 329, 333, 335, 372], "datatyp": [18, 30, 32, 61, 84, 88, 89, 90, 91, 92, 107, 108, 130, 131, 132, 133, 134, 140, 142, 143, 175, 176, 178, 187, 199, 200, 203, 207, 209, 214, 219, 223, 227, 236, 237, 246, 247, 299, 329, 335, 351, 352, 374, 375, 378, 380, 381, 395, 396, 400], "str": [18, 32, 37, 81, 85, 92, 110, 133, 134, 143, 175, 176, 178, 187, 188, 199, 200, 203, 207, 209, 214, 219, 223, 227, 229, 230, 236, 237, 299, 304, 329, 335, 380, 381, 401], "broadcast": [18, 19, 81, 92, 134, 143, 176, 178, 187, 199, 200, 203, 207, 209, 214, 219, 236, 237, 290, 329, 332, 335, 358, 402], "int32": [18, 46, 47, 48, 49, 50, 51, 92, 94, 108, 133, 134, 139, 143, 145, 175, 178, 184, 203, 214, 216, 223, 227, 237, 242, 335, 398], "uint32": [18, 31, 88, 89, 108, 159, 162, 380, 385, 386, 395], "4294967295": 18, "uint16": [18, 46, 49, 51, 131, 159, 236, 246, 335, 354, 374, 386], "65535": [18, 46, 49, 51, 236, 335], "tensor1": [18, 19, 20, 21, 22, 23, 24, 25, 37, 39, 40, 45, 46, 47, 49, 50, 51, 63, 64, 80, 81, 82, 92, 93, 122, 123, 124, 133, 134, 135, 143, 144, 155, 156, 162, 165, 175, 176, 177, 178, 179, 182, 183, 199, 200, 201, 202, 203, 204, 207, 208, 209, 210, 214, 215, 217, 219, 221, 223, 226, 227, 233, 236, 237, 238, 241, 249, 260, 286, 287, 299, 300, 301, 329, 330, 332, 333, 334, 335, 370, 371, 372, 373], "tensor2": [18, 19, 20, 21, 22, 23, 24, 25, 37, 39, 40, 45, 46, 47, 49, 50, 51, 63, 64, 80, 81, 82, 92, 93, 122, 123, 124, 133, 134, 135, 143, 144, 155, 156, 162, 165, 175, 176, 177, 178, 179, 182, 183, 199, 200, 201, 202, 203, 204, 207, 208, 209, 210, 214, 215, 217, 219, 221, 223, 226, 227, 233, 236, 237, 238, 241, 249, 260, 286, 287, 299, 300, 301, 329, 330, 332, 333, 334, 335, 370, 371, 372, 373], "scalar": [18, 19, 22, 24, 46, 47, 49, 50, 51, 80, 81, 82, 83, 92, 93, 122, 123, 124, 131, 132, 134, 135, 143, 144, 176, 178, 179, 182, 183, 199, 200, 203, 207, 209, 214, 215, 217, 223, 227, 233, 236, 237, 238, 256, 260, 269, 270, 286, 287, 299, 329, 332, 335, 378, 381, 402], "binary_backward": [19, 21, 37, 40, 45, 64, 81, 124, 156, 177, 201, 202, 221, 226, 233, 287, 300, 330, 332, 334, 373], "add_bw_t": 19, "_golden_function_bw": [19, 21, 37, 40, 45, 64, 124, 156, 177, 201, 202, 226, 287, 300, 330, 332, 334, 373], "are_required_output": [19, 21, 37, 64, 81, 233, 300, 332, 334, 371], "input_grad": [19, 21, 37, 64, 81, 110, 233, 300, 332, 334], "other_grad": [19, 21, 37, 64, 81, 233, 300, 332, 334], "bfloat4_b": [19, 21, 22, 24, 27, 40, 45, 64, 81, 130, 156, 177, 182, 183, 201, 202, 217, 221, 226, 233, 300, 330, 332, 334, 353, 370, 373, 380], "addalpha_t": 20, "_golden_function_addalpha": 20, "alpha": [20, 21, 23, 25, 55, 56, 81, 86, 87, 302, 333, 334], "float": [20, 21, 22, 23, 24, 25, 44, 55, 56, 57, 58, 59, 60, 83, 86, 87, 109, 115, 118, 125, 130, 131, 132, 142, 146, 147, 148, 150, 152, 153, 154, 165, 174, 180, 181, 182, 211, 213, 250, 252, 256, 257, 258, 260, 269, 270, 284, 285, 292, 295, 296, 302, 319, 320, 321, 322, 333, 334, 346, 347, 358, 359, 361, 362, 378, 380, 381, 386], "addalpha_bw_t": 21, "addcdiv_t": 22, "_golden_function_addcdiv": 22, "input_tensor_c": [22, 23, 24, 25, 183, 217, 370, 371], "tensor3": [22, 23, 24, 25, 182, 183, 217, 370, 371], "ternary_backward": [23, 25, 183, 371], "addcdiv_bw_t": 23, "addcmul_t": 24, "_golden_function_addcmul": 24, "addcmul_bw_t": 25, "all_gather_t": 26, "gather": [26, 105], "across": [26, 105, 140, 265, 276, 380, 385, 386], "cluster_axi": [26, 276], "meshtensor": [26, 276], "axi": [26, 107, 276, 290, 381], "line": [26, 276, 399], "mesh_devic": [26, 105, 276, 380], "applic": [26, 85, 276, 386], "guid": [26, 276, 382, 383], "blob": [26, 276, 401], "main": [26, 276, 383, 384, 387, 399, 401, 402], "tech_report": [26, 276], "20mesh": [26, 276], "20of": [26, 276], "20devic": [26, 276], "20with": [26, 276], "20tt": [26, 276], "nn": [26, 229, 230, 276, 380, 387, 388, 391, 392, 400], "num_link": [26, 105, 276], "link": [26, 105, 276], "num_work": [26, 105, 276], "worker": [26, 105, 248, 276, 351, 380, 397, 398], "num_buffers_per_channel": [26, 105, 276], "per": [26, 105, 276, 381, 383, 399], "ring": [26, 105, 276], "full_tensor": [26, 105, 276], "randn": [26, 32, 75, 89, 91, 105, 129, 130, 140, 187, 219, 222, 247, 273, 276, 350, 351, 352, 353, 365, 375, 378, 380, 396, 397, 401], "256": [26, 105, 222, 276, 400, 401], "physical_device_id": [26, 105, 276, 395, 396, 397, 398, 399, 400], "get_t3k_physical_device_ids_r": [26, 105, 276], "open_mesh_devic": [26, 105, 276], "meshshap": [26, 105, 276], "ttnn_tensor": [26, 353, 395], "input_dtyp": [26, 105, 276], "mem_config": [26, 105, 276, 380], "mesh_mapp": [26, 32, 130], "shardtensor2dmesh": 26, "mesh_shap": 26, "alt_complex_rotate90_t": 27, "_golden_function_alt_complex_rotate90": 27, "_": [27, 376, 385, 397, 401], "2i": 27, "complex_unari": [28, 65, 160, 163, 164, 253, 271], "angle_t": 28, "complex_unary_backward": [29, 66, 161, 254, 272], "angle_bw_t": 29, "arange_t": 30, "inclus": [30, 380, 386], "end": [30, 80, 165, 182, 241, 317, 367, 377, 380, 381, 385, 397, 399], "exclus": [30, 250], "consecut": [30, 386], "dram_memory_config": [30, 61, 90, 91, 187, 219, 350, 386], "space": [30, 67, 68, 69, 261, 262, 263, 264, 381, 386], "print": [30, 32, 63, 90, 91, 129, 130, 131, 132, 187, 218, 219, 246, 247, 248, 252, 288, 290, 304, 317, 318, 350, 351, 353, 374, 375, 378, 380, 381, 385, 386, 395, 396, 397, 399, 401, 402], "00000": [30, 395], "argmax_t": 31, "_create_golden_funct": [31, 220, 224, 225, 331, 336, 369], "local": [31, 35, 38, 42, 52, 53, 70, 72, 76, 78, 94, 95, 97, 99, 101, 102, 113, 120, 137, 139, 145, 148, 150, 157, 159, 166, 167, 168, 169, 170, 184, 185, 189, 190, 192, 194, 197, 205, 216, 220, 224, 225, 228, 234, 239, 242, 267, 280, 281, 297, 305, 306, 308, 310, 311, 313, 315, 319, 323, 325, 327, 331, 336, 338, 340, 342, 354, 361, 362, 369, 376, 377, 386], "keepdim": [31, 265, 381], "currenli": 31, "row_major_layout": [32, 61, 85, 88, 125, 126, 130, 131, 246, 351, 374, 386, 395, 396, 397], "cache_file_nam": 32, "pathlib": [32, 85, 188], "path": [32, 85, 188, 385, 399, 401, 402], "callabl": [32, 229, 230], "serial": 32, "tensortomesh": [32, 130], "use_device_til": 32, "toggl": 32, "truncat": [32, 364, 381], "mantissa": 32, "bit": [32, 381], "bfp": [32, 386], "rais": [32, 353, 400], "runtim": 32, "error": [32, 173, 218, 219, 232, 353, 377, 383], "rte": 32, "bfp8": [32, 395], "bfp4": 32, "375": [32, 130, 398], "30469": [32, 130], "714844": [32, 130], "761719": [32, 130], "53125": [32, 130], "652344": [32, 130], "asin_t": 33, "_golden_function_asin": 33, "asin_bw_t": 34, "lambda": [34, 36, 43, 54, 56, 58, 60, 73, 83, 87, 98, 147, 181, 191, 193, 195, 213, 270, 296, 309, 314, 316, 322, 324, 376, 398, 400], "sine": [34, 36, 111, 316], "asinh_t": 35, "register_ttnn_cpp_unary_funct": [35, 38, 42, 52, 53, 70, 72, 76, 78, 94, 95, 97, 99, 101, 102, 113, 120, 137, 139, 145, 148, 150, 157, 159, 166, 167, 168, 169, 170, 184, 185, 189, 190, 192, 194, 197, 205, 216, 228, 234, 239, 242, 267, 280, 281, 297, 305, 306, 308, 310, 311, 313, 315, 319, 323, 325, 327, 338, 340, 342, 361, 362], "asinh_bw_t": 36, "assign_bw_t": 37, "assign": [37, 378], "other_tensor": [37, 81], "round_mod": [37, 80, 81, 269, 270], "atan_t": 38, "atan2_t": 39, "_golden_function_atan2": 39, "arctan": 39, "left": [39, 80, 133, 175, 301, 380, 381, 384], "right": [39, 80, 133, 175, 301, 380, 381], "atan2_bw_t": 40, "atan_bw_t": 41, "_golden_function_atan": 41, "tangenr": 41, "atanh_t": 42, "atanh_bw_t": 43, "tangent": [43, 343], "batch_norm_t": 44, "spatial": [44, 84, 140, 368, 381], "over": [44, 67, 68, 69, 106, 140, 142, 174, 266, 292, 318, 358, 359, 381], "interleav": [44, 63, 106, 350, 352, 360, 380, 381, 386], "ep": [44, 211, 213, 381], "epsilon": [44, 142, 174, 292, 381], "1e": [44, 142, 165, 174, 292, 381], "momentum": [44, 381], "running_mean": [44, 381], "running_var": [44, 381], "varianc": [44, 381, 386], "gamma": [44, 235, 381], "beta": [44, 319, 320, 381], "evalu": [44, 399], "bias_gelu_bw_t": 45, "bias_gelu": 45, "approxim": [45, 95, 97, 101, 110, 137, 138, 297, 305], "bitwise_and_t": 46, "_golden_function_bitwise_and": 46, "integ": [46, 47, 49, 50, 51, 250, 359, 381], "bitwise_left_shift_t": 47, "_golden_function_bitwise_left_shift": 47, "ha": [47, 50, 69, 219, 229, 230, 263, 264, 339, 360, 376, 377, 378, 380, 381, 383, 385, 386, 395, 401, 402], "shift_bit": [47, 50], "31": [47, 50, 378, 380, 399, 401], "bitwise_not_t": 48, "_golden_function_bitwise_not": 48, "2147483647": [48, 133], "bitwise_or_t": 49, "_golden_function_bitwise_or": 49, "bitwise_right_shift_t": 50, "_golden_function_bitwise_right_shift": 50, "bitwise_xor_t": 51, "_golden_function_bitwise_xor": 51, "cbrt_t": 52, "ceil_t": 53, "ceil_bw_t": 54, "celu_t": 55, "_golden_function_celu": 55, "celu_bw_t": 56, "formula": [56, 87, 147, 181, 182, 213, 320, 322], "clamp_t": 57, "_golden_function_clamp": 57, "min_tensor": [57, 59], "max_tensor": [57, 59], "clamp_bw_t": 58, "clip_t": 59, "_golden_function_clip": 59, "clip_bw_t": 60, "data_mov": [61, 63, 117, 118, 162, 243, 250, 252, 288, 290, 291, 317, 348, 349, 366, 367], "clone_t": 61, "doe": [61, 130, 353, 377, 381, 385], "alter": 61, "adjust": [61, 319], "necessari": [61, 377], "param": [61, 84, 129, 250, 318, 350, 381, 401], "target": [61, 125, 126, 265, 377, 380, 381], "l1_memory_config": [61, 350, 376, 386, 396, 397, 402], "compute_kernel_config": [61, 106, 111, 174, 187, 219, 292, 318, 358, 359], "remov": [62, 353, 367, 380, 395, 398, 399, 401], "success": 62, "concat_t": 63, "concaten": [63, 64, 357, 360, 381], "group": [63, 67, 68, 69, 261, 262, 263, 264, 381, 386, 400], "partit": 63, "independ": 63, "altern": [63, 219, 383, 402], "recombin": 63, "residu": 63, "concat_bw_t": 64, "conj_t": 65, "conjug": 65, "conj_bw_t": 66, "conv1d_t": 67, "1d": [67, 219, 243, 260, 399], "signal": [67, 68, 106, 140, 381, 399], "compos": [67, 68, 69, 106, 140, 353, 381], "sever": [67, 68, 69, 106, 140, 381], "plane": [67, 68, 69, 106, 140, 381, 400], "2d": [67, 68, 69, 84, 140, 219, 368, 381, 386], "input_length": 67, "weight_tensor": [67, 68, 69, 106, 263, 264, 400], "bias_tensor": [67, 68, 69, 106, 400], "batch_siz": [67, 68, 69, 89, 140, 222, 261, 262, 263, 264, 357, 360, 376, 397, 398, 399, 400], "length": [67, 260, 358, 359], "kernel_s": [67, 68, 69, 222, 261, 262, 263, 264, 381, 400], "convolv": [67, 68, 69, 222, 261, 262, 263, 264, 381], "cross": [67, 68, 69, 261, 262, 263, 264, 381], "correl": [67, 68, 69, 261, 262, 263, 264, 377, 381], "side": [67, 68, 69, 261, 262, 263, 264, 381, 385, 395, 396, 397, 398, 399, 400], "pad_length": 67, "pad_left": [67, 68, 69, 261, 262, 263, 264], "pad_right": [67, 68, 69, 261, 262, 263, 264], "connect": [67, 68, 69, 261, 262, 263, 264, 381], "conv_config": [67, 68, 69, 261, 262, 263, 264, 400], "compute_config": [67, 68, 69, 261, 262, 263, 264], "devicecomputekernelconfig": [67, 68, 69, 106, 111, 174, 187, 219, 261, 262, 263, 264, 292, 318, 358, 359], "return_output_dim": [67, 68, 69], "return_weights_and_bia": [67, 68, 69], "bias": [67, 376, 381, 397], "conv2d_t": 68, "inform": [68, 274, 381, 383, 387], "tech": 68, "input_height": [68, 69, 261, 262, 263, 264, 400], "input_width": [68, 69, 261, 262, 263, 264, 400], "pad_height": [68, 69, 261, 262, 263, 264], "pad_width": [68, 69, 261, 262, 263, 264], "pad_top": [68, 69, 261, 262, 263, 264], "pad_bottom": [68, 69, 261, 262, 263, 264], "conv_transpose2d_t": 69, "transpos": [69, 111, 187, 219, 252, 360], "seen": [69, 74, 395, 396], "respect": [69, 89, 219, 386], "fraction": 69, "deconvolut": 69, "o": [69, 383, 385, 395, 396, 397, 398, 399, 401], "k_h": 69, "k_w": 69, "equat": 69, "h_out": 69, "h_in": 69, "output_pad": 69, "w_out": 69, "w_in": 69, "mirror_kernel": 69, "mirror": [69, 401], "intern": [69, 381, 382], "been": [69, 229, 230, 339, 377, 383, 401], "cos_t": 70, "cos_bw_t": 71, "_golden_function_co": 71, "cosh_t": 72, "cosh_bw_t": 73, "coregrid": [74, 142, 187, 219, 396, 397], "corerang": 74, "strategi": [74, 85, 219, 380, 381, 385, 386, 399], "shardstrategi": 74, "shardorient": 74, "use_height_and_width_as_shard_shap": 74, "travers": 74, "math": [74, 385, 399], "320": [74, 381], "pycapsul": [75, 129, 273, 350], "resourc": [75, 399], "explicitli": [75, 395], "whose": [75, 91, 351], "forc": [75, 398], "deg2rad_t": 76, "deg2rad_bw_t": 77, "_golden_function_deg2rad": 77, "degre": [77, 268], "radian": [77, 268], "digamma_t": 78, "digamma_bw_t": 79, "_golden_function_digamma": 79, "div_t": 80, "_golden_function_div": 80, "begin": [80, 165, 241, 381, 385], "text": [80, 165, 241, 249, 377, 381], "_mode": 80, "accurate_mod": 80, "non": [80, 219, 222, 243, 259, 274, 295], "div_bw_t": 81, "pcc": [81, 156, 177, 270, 377, 384], "degrad": [81, 156, 177, 270], "div_no_nan_t": 82, "_golden_function_div_no_nan": 82, "div_no_nan_bw_t": 83, "denomin": [83, 269, 381], "downsample_t": 84, "form": [84, 368], "downsample_param": 84, "dump": [85, 229, 398, 401, 402], "file_nam": [85, 188, 401, 402], "dict": [85, 229, 230, 380, 401], "save": [85, 399, 401], "elu_t": 86, "_golden_function_elu": 86, "elu_bw_t": 87, "embedding_t": 88, "retriev": 88, "word": 88, "padding_idx": 88, "token": [88, 172, 359, 399], "embeddings_typ": 88, "embeddingstyp": 88, "106445": 88, "988281": 88, "59375": 88, "212891": 88, "964844": 88, "199219": 88, "996094": 88, "78362e": 88, "38": [88, 386, 399], "89785e": 88, "39": [88, 395, 396, 397, 398, 399, 400, 401], "04479e": 88, "25815e": 88, "71833e": 88, "59995e": 88, "60398e": 88, "83671e": 88, "22242e": 88, "88263e": 88, "35917e": 88, "49994e": 88, "embedding_backward": 89, "embedding_bw_t": 89, "extract": 89, "vocabulari": 89, "output_gradient_tensor": 89, "seq_len": [89, 111], "embedding_dim": 89, "num_embed": 89, "1024": [89, 396], "4096": [89, 380], "3200": 89, "input_shap": [89, 222, 400], "input_index": 89, "randint": [89, 378, 398], "weights_shap": 89, "weights_ttnn": 89, "grad_shap": 89, "grad_data": 89, "empty_t": 90, "uniniti": [90, 91], "bfloat_8": 90, "21": [90, 399, 401], "67": 90, "empty_like_t": 91, "desir": [91, 108, 117, 118, 125, 126, 130, 351, 352, 353], "87": 91, "45": [91, 340, 341, 399], "22": [91, 383, 399], "60": [91, 399], "75": [91, 129], "25": [91, 351, 385, 399, 401], "eq_t": 92, "eq__t": 93, "_golden_function_eq_": 93, "input_a": [93, 135, 144, 162, 179, 215, 220, 224, 225, 238, 331, 336, 369], "input_b": [93, 135, 144, 162, 179, 215, 238], "eqz_t": 94, "_tensor_i": [94, 139, 145, 184, 205, 216, 242], "erf_t": 95, "fast_and_approximate_mod": [95, 97, 101, 137, 297, 305], "erf_bw_t": 96, "_golden_function_erf": 96, "erfc_t": 97, "erfc_bw_t": 98, "erfinv_t": 99, "erfinv_bw_t": 100, "_golden_function_erfinv": 100, "exp_t": 101, "exp2_t": 102, "exp2_bw_t": 103, "_golden_function_exp2": 103, "exp_bw_t": 104, "_golden_function_exp": 104, "exponenti": 104, "ccl_experiment": 105, "all_reduce_t": 105, "num_devic": [105, 276], "tt_input_tensor": [105, 276], "enumer": [105, 276], "append": [105, 276, 385, 401], "get_devic": [105, 276], "input_tensor_mesh": [105, 276], "aggregate_as_tensor": [105, 276], "conv3d_t": 106, "3d": 106, "d": [106, 399], "kd": 106, "kh": [106, 381], "kw": [106, 381], "c_in": 106, "c_out": 106, "conv3dconfig": 106, "cumprod_t": 107, "witth": 107, "cumul": [107, 108], "product": [107, 219, 265, 358, 359], "underli": [107, 380], "ref": [107, 383], "fed": 107, "being": [107, 277, 278, 385, 386], "actual": [107, 269, 378, 386, 395], "assert": [107, 250, 397, 402], "dtyoe": 107, "uint8": [107, 138, 159, 298, 317], "tensor_copi": 107, "cumsum_t": 108, "fundament": 108, "cast": 108, "torch_input": 108, "tensor_input": 108, "tensor_output": 108, "preallocated_output": 108, "dropout_t": 109, "seed": [109, 401], "rng": 109, "probabl": 109, "averag": [109, 140, 381, 385, 399], "total_elem": 109, "scale": [109, 148, 150, 302, 319, 358, 359, 381], "124": 109, "prob": 109, "gelu_bw_t": [110, 138], "_golden_function_gelu": [110, 138], "algorithm": [110, 368, 381], "rotary_embedding_t": 111, "rotari": 111, "cos_cach": 111, "sin_cach": 111, "token_idx": 111, "head_dim": 111, "cod_cach": 111, "token_index": 111, "sort_t": 112, "ascend": 112, "descend": 112, "stabl": 112, "preserv": 112, "addit": [112, 219, 376, 381], "info": [112, 395, 396, 397, 398, 399, 400, 401], "sorted_tensor": 112, "sorted_tensor_desc": 112, "indices_desc": 112, "input_tensor_2d": 112, "sorted_tensor_dim": 112, "indices_dim": 112, "expm1_t": 113, "expm1_bw_t": 114, "_golden_function_expm1": 114, "fill_t": 115, "_golden_function_fil": 115, "fill_valu": [115, 131, 132, 381], "wormhole_b0": [115, 383], "fill_bw_t": 116, "fill_ones_rm_t": 117, "val_hi": [117, 118], "val_lo": [117, 118], "count": [117, 118, 380, 385, 399], "ye": [117, 118, 380, 381], "hone": [117, 118], "high": [117, 118, 383, 386, 402], "region": [117, 118, 248], "wone": [117, 118], "fill_rm_t": 118, "nchw": 118, "hw": [118, 381], "hfill": 118, "wfill": 118, "hi": 118, "lo": 118, "low": 118, "fill_zero_bw_t": 119, "_golden_function_fill_zero": 119, "floor_t": 120, "floor_bw_t": 121, "_golden_function_floor": 121, "floor_div_t": 122, "_golden_function_floor_div": 122, "fmod_t": 123, "_golden_function_fmod": 123, "fmod_bw_t": 124, "padded_shap": [125, 251], "pad_valu": [125, 130, 252, 291, 317, 349, 380], "target_layout": [125, 126, 380, 381], "target_mem_config": [125, 126], "padded_tensor": 125, "output_mem_config": [125, 126, 399], "unpadded_tensor": 126, "frac_t": 127, "_golden_function_frac": [127, 128], "frac_bw_t": 128, "tensor_on_devic": [129, 350], "tensor_on_host": [129, 350], "365234": 129, "130859": 129, "itself": [130, 380], "twice": [130, 385], "purpos": [130, 377, 384, 386], "mapper": 130, "cq_id": [130, 339, 353, 380, 381], "full_t": 131, "_golden_function_ful": 131, "filled_tensor": [131, 132], "full_like_t": 132, "templat": [132, 247, 375, 385], "gcd_t": 133, "_golden_function_gcd": 133, "greatest": 133, "2147483648": 133, "tensorint32default": [133, 175], "ge_t": 134, "ge__t": 135, "_golden_function_ge_": 135, "geglu_t": 136, "_golden_function_geglu": 136, "gelu_t": 137, "gez_t": 139, "global_avg_pool2d_t": 140, "golden_global_avg_pool2d": 140, "adapt": [140, 381], "glu_t": 141, "_golden_function_glu": 141, "group_norm_t": 142, "_postprocess_golden_function_output": [142, 250], "num_group": [142, 381], "input_mask": 142, "inplac": [142, 204, 206, 208, 210, 400], "gt_t": 143, "gt__t": 144, "_golden_function_gt_": 144, "gtz_t": 145, "hardshrink_t": 146, "_golden_function_hardshrink": 146, "lambd": [146, 147, 321, 322], "hardshrink_bw_t": 147, "hardsigmoid_t": 148, "shift": [148, 150, 381], "16666667": [148, 150], "hardsigmoid_bw_t": 149, "_golden_function_hardsigmoid": 149, "hardswish_t": 150, "hardswish_bw_t": 151, "_golden_function_hardswish": 151, "hardtanh_t": 152, "_golden_function_hardtanh": 152, "min_val": 152, "max_val": 152, "hardtanh_bw_t": 153, "heaviside_t": 154, "_golden_function_heavisid": 154, "hypot_t": 155, "_golden_function_hypot": 155, "hypot_bw_t": 156, "i0_t": 157, "i0_bw_t": 158, "_golden_function_i0": 158, "identity_t": 159, "sfpu": 159, "shouldn": 159, "instead": [159, 285, 381, 395, 397, 401], "lower": [159, 250, 383], "float16": [159, 395], "imag_t": 160, "imag_bw_t": 161, "imaginari": 161, "indexed_fill_t": 162, "replac": [162, 346], "denot": [162, 381], "batch_id": 162, "is_imag_t": 163, "is_real_t": 164, "isclose_t": 165, "_golden_function_isclos": 165, "leq": 165, "atol": 165, "rtol": 165, "otherwis": [165, 265, 339, 354, 380, 397, 402], "rel": 165, "toler": 165, "05f": 165, "absolut": [165, 173, 381], "08f": 165, "equal_nan": 165, "nan": [165, 399], "treat": [165, 219], "isfinite_t": 166, "isinf_t": 167, "isnan_t": 168, "isneginf_t": 169, "isposinf_t": 170, "fill_cache_for_user__t": 171, "popul": [171, 229, 385], "batch_index": 171, "update_cache_for_token__t": 172, "update_index": 172, "batch_offset": 172, "l1_loss_t": 173, "_golden_function_l1_loss": 173, "input_refer": [173, 232], "input_predict": [173, 232], "layer_norm_t": 174, "residual_input_tensor": [174, 292], "program_config": [174, 187, 219, 292, 355, 356, 358, 359], "programconfig": [174, 292], "lcm_t": 175, "_golden_function_lcm": 175, "least": [175, 219, 385], "32767": 175, "32768": 175, "ldexp_t": 176, "ldexp_bw_t": 177, "80": [177, 383], "outsid": 177, "le_t": 178, "le__t": 179, "_golden_function_le_": 179, "leaky_relu_t": 180, "_golden_function_leaky_relu": 180, "negative_slop": [180, 181], "slope": 180, "leaki": 180, "leaky_relu_bw_t": 181, "01": 181, "lerp_t": 182, "_golden_function_lerp": 182, "point": [182, 319, 378, 381, 385, 386], "lerp_bw_t": 183, "lez_t": 184, "lgamma_t": 185, "lgamma_bw_t": 186, "_golden_function_lgamma": 186, "linear_t": 187, "behaviour": [187, 219], "transpose_a": [187, 219], "transpose_b": [187, 219], "matmulprogramconfig": [187, 219], "output_til": [187, 219], "128": [187, 219, 402], "log_t": 189, "log10_t": 190, "whb0": [190, 194, 287], "log10_bw_t": 191, "log1p_t": 192, "1e7": 192, "log1p_bw_t": 193, "log2_t": 194, "log2_bw_t": 195, "log_bw_t": 196, "_golden_function_log": 196, "logarithm": [196, 235], "log_sigmoid_t": 197, "log_sigmoid_bw_t": 198, "_golden_function_log_sigmoid": 198, "logaddexp_t": 199, "logaddexp2_t": 200, "logaddexp2_bw_t": 201, "logaddexp_bw_t": 202, "logical_and_t": 203, "_golden_function_logical_and": 203, "AND": [203, 204, 381], "use_legaci": 203, "logical_and__t": 204, "logical_not_t": 205, "logical_not__t": 206, "_golden_function_logical_not_": 206, "logical_or_t": 207, "_golden_function_logical_or": 207, "OR": [207, 208, 291, 381], "logical_or__t": 208, "logical_xor_t": 209, "_golden_function_logical_xor": 209, "land": [209, 210], "lnot": [209, 210], "lor": [209, 210], "logical_xor__t": 210, "xor": [210, 381], "logit_t": 211, "_golden_function_logit": [211, 212], "logit_bw_t": 212, "logiteps_bw_t": 213, "logitep": 213, "lt_t": 214, "lt__t": 215, "_golden_function_lt_": 215, "ltz_t": 216, "mac_t": 217, "_golden_function_mac": 217, "context": [218, 277, 278], "exit": 218, "occur": 218, "matmul_t": 219, "dimension": [219, 235, 381, 386, 395], "dot": [219, 358, 359], "although": 219, "combin": 219, "most": [219, 395], "variou": 219, "align": [219, 380, 381, 383, 385], "appropri": [219, 377, 383], "abov": [219, 383], "criteria": 219, "messag": [219, 385], "unexpect": [219, 381], "obviou": [219, 395], "except": [219, 381, 395, 397, 399], "These": [219, 377, 378, 381, 383, 387, 402], "scenario": [219, 386], "relat": [219, 381], "swap": 219, "j": 219, "implicitli": 219, "extend": 219, "patch": 219, "leverag": [219, 381], "accord": [219, 252, 288, 383], "look": [219, 354, 377, 382, 386, 399, 401], "n_size": 219, "m_size": 219, "k_size": 219, "p": [219, 386], "though": [219, 378], "constraint": [219, 386, 395], "chosen": [219, 385, 401], "carefulli": 219, "fix": 219, "problem": 219, "max_t": 220, "max_bw_t": 221, "max_pool2d_t": 222, "golden_maxpool2d": 222, "window": [222, 381], "nhw": 222, "scheme": [222, 261, 262, 263, 264], "input_h": 222, "input_w": 222, "applied_shard_schem": 222, "ceil_mod": [222, 381], "createdevic": [222, 378, 380, 400], "l1_small_siz": [222, 248, 397, 398, 400], "8192": [222, 397, 398], "kernel_h": 222, "kernel_w": 222, "stride_h": 222, "stride_w": 222, "pad_h": 222, "pad_w": 222, "dilation_h": 222, "dilation_w": 222, "nchw_shape": 222, "40": [222, 399], "in_n": 222, "in_c": 222, "in_h": 222, "in_w": 222, "input_perm": 222, "input_reshap": 222, "tt_input": 222, "tt_input_dev": 222, "tt_output": [222, 378, 380], "in_place_halo": 222, "maximum_t": 223, "_golden_function_maximum": 223, "mean_t": 224, "min_t": 225, "min_bw_t": 226, "minimum_t": 227, "_golden_function_minimum": 227, "mish_t": 228, "20": [228, 250, 319, 320, 385, 399, 401], "initialize_model": [229, 230, 376, 398, 400], "model_nam": [229, 230, 376, 398, 399, 400], "convert_to_ttnn": [229, 230, 376], "custom_preprocessor": [229, 230, 376, 398, 400], "parameterdict": [229, 230], "prefix": [229, 230], "run_model": [229, 400], "reader_patterns_cach": 229, "disabl": [229, 230, 381, 395, 396, 397, 398, 399, 400, 402], "doesn": [229, 230, 395], "invalid": [229, 230], "preprocessor": [229, 230], "put": [229, 230, 376, 395, 397, 400], "submodul": [229, 230, 383], "appear": [229, 230, 383], "ttnn_module_arg": [229, 400], "tmp": [229, 395, 396, 397, 398, 400, 401], "model_graph": 229, "svg": [229, 400, 401, 402], "avoid": [229, 319, 385, 386], "recomput": [229, 381], "moreh_sum_t": 231, "mse_loss_t": 232, "_golden_function_mse_loss": 232, "mul_bw_t": 233, "multigammaln_t": 234, "multigammaln_bw_t": 235, "_golden_function_mvlgamma": 235, "multivari": 235, "mvlgamma": 235, "5f": 235, "multiply_t": 236, "ne_t": 237, "ne__t": 238, "_golden_function_ne_": 238, "neg_t": 239, "neg_bw_t": 240, "_golden_function_neg": 240, "nextafter_t": 241, "_golden_function_nextaft": 241, "_float": 241, "neq": 241, "nez_t": 242, "nonzero_t": 243, "well": [243, 380, 384], "normalize_global_t": 244, "_golden_function_normalize_glob": 244, "normalize_hw_t": 245, "_golden_function_normalize_hw": 245, "ones_t": 246, "ones_like_t": 247, "trace_region_s": 248, "dispatch_core_config": [248, 397, 398], "dispatchcoreconfig": [248, 397, 398], "0x7fcfb9554e30": 248, "worker_l1_s": 248, "small": [248, 399], "default_l1_small_s": 248, "default_trace_region_s": 248, "allocat": 248, "dispatch_core_typ": [248, 397, 398], "dispatchcoretyp": [248, 397, 398], "0x7fbac5bfc1b0": 248, "outer_t": 249, "_golden_function_out": 249, "otim": 249, "pad_t": 250, "_preprocess_golden_function_input": 250, "locat": [250, 381, 382, 385, 399, 402], "mutual": 250, "output_tensor_shap": [250, 349, 380], "input_tensor_start": [250, 380], "union": 250, "use_multicor": [250, 348, 349, 366, 367], "pad_input": 250, "unpadded_shap": 251, "annot": [251, 377, 380], "fixeds": [251, 380], "permute_t": 252, "nullopt": [252, 354, 380, 400], "tthe": 252, "broken": 252, "garbag": 252, "polar_t": 253, "cartesian": 253, "theta": 253, "polar_bw_t": 254, "polygamma_t": 255, "_golden_function_polygamma": 255, "decim": [255, 293], "k": [255, 354, 358, 359, 381, 396], "polygamma_bw_t": 256, "polyval_t": 257, "_golden_function_polyv": 257, "coeffici": [257, 377], "coeff": 257, "sum_": [257, 381], "polynomi": 257, "pow_t": 258, "_golden_function_pow": 258, "expon": [258, 259, 295, 296, 354, 378, 386], "pow_bw_t": 259, "power": [259, 365, 378], "prelu_t": 260, "_golden_function_prelu": 260, "arrai": 260, "suitabl": [261, 262, 263, 264], "invoc": [261, 262, 263, 264], "exact": [261, 262, 263, 264, 381, 386, 397], "input_memory_config": [261, 262, 263, 264], "input_layout": [261, 262, 263, 264], "convtranspose2d": [262, 263], "conv_tranpose2d": 263, "weights_format": [263, 264], "iohw": 263, "has_bia": [263, 264], "term": [263, 264, 384], "oihw": 264, "prod_t": 265, "similar": [265, 385, 395, 397], "squeez": [265, 353, 395, 396, 397, 398, 400], "output_all_dim": 265, "prod_bw_t": 266, "particular": [266, 376, 383, 395, 402], "taken": 266, "all_dims_output": 266, "rad2deg_t": 267, "rad2deg_bw_t": 268, "_golden_function_rad2deg": 268, "rdiv_t": 269, "_golden_function_rdiv": 269, "numer": [269, 319, 381, 382, 386], "rounding_mod": 269, "rdiv_bw_t": 270, "real_t": 271, "real_bw_t": 272, "new_tensor": 273, "my_memory_config": 273, "reciprocal_t": 274, "inaccur": [274, 386], "characterist": 274, "fp": 274, "reciprocal_bw_t": 275, "_golden_function_reciproc": 275, "reduce_scatter_t": 276, "reduce0scatt": 276, "reglu_t": 279, "_golden_function_reglu": 279, "relu_t": 280, "relu6_t": 281, "relu6_bw_t": 282, "_golden_function_relu6": 282, "relu_bw_t": 283, "_golden_function_relu": 283, "relu_max_t": 284, "_golden_function_relu_max": 284, "upper_limit": 284, "cap": 284, "relu_min_t": 285, "_golden_function_relu_min": 285, "lower_limit": 285, "remainder_t": 286, "_golden_function_remaind": 286, "modulu": 286, "remainder_bw_t": 287, "repeat_t": 288, "repetit": [288, 290, 381], "repetition_vector": 288, "smallvector": 288, "repeat_bw_t": 289, "repeat_interleave_t": 290, "he": 290, "expand": [290, 386], "torch_input_tensor": [290, 400, 402], "torch_result": 290, "reshape_t": 291, "cost": [291, 381], "view": [291, 402], "condit": [291, 377], "met": 291, "new_shap": 291, "kwtype": 291, "rms_norm_t": 292, "round_t": 293, "_golden_function_round": [293, 294], "round_bw_t": 294, "rpow_t": 295, "_golden_function_rpow": 295, "upto": 295, "28": [295, 399, 401], "posit": [295, 359, 378, 381], "rpow_bw_t": 296, "rsqrt_t": 297, "rsqrt_bw_t": 298, "_golden_function_rsqrt": 298, "rsub_t": 299, "_golden_function_rsub": 299, "rsub_bw_t": 300, "subract": 300, "revers": 300, "scatter_t": 301, "_golden_function_scatt": 301, "selu_t": 302, "_golden_function_selu": [302, 303], "0507": 302, "67326": 302, "selu_bw_t": 303, "modifi": [304, 319, 378, 395], "short": [304, 399, 402], "sigmoid_t": 305, "vector_mod": 305, "better": [305, 342, 381], "rc": [305, 383], "sigmoid_accurate_t": 306, "sigmoid_bw_t": 307, "_golden_function_sigmoid": 307, "sign_t": 308, "sign_bw_t": 309, "signbit_t": 310, "silu_t": 311, "silu_bw_t": 312, "_golden_function_silu": 312, "sin_t": 313, "sin_bw_t": 314, "sinh_t": 315, "sinh_bw_t": 316, "slice_t": 317, "slice_start": 317, "input_tensor_shap": [317, 380], "slice_end": 317, "slice_step": 317, "unmodifi": 317, "undefin": [317, 381], "softmax_t": 318, "0310059": 318, "softplus_t": 319, "By": [319, 384, 396], "steep": 319, "higher": [319, 383, 385, 386], "steeper": 319, "approach": [319, 376, 384, 402], "hard": [319, 395], "stabil": [319, 381, 384], "veri": [319, 385, 395, 397], "softplus_bw_t": 320, "softshrink_t": 321, "_golden_function_softshrink": 321, "softshrink_bw_t": 322, "softsign_t": 323, "softsign_bw_t": 324, "sqrt_t": 325, "sqrt_bw_t": 326, "_golden_function_sqrt": 326, "square_t": 327, "square_bw_t": 328, "_golden_function_squar": 328, "squared_difference_t": 329, "_golden_function_squared_differ": 329, "squared_difference_bw_t": 330, "std_t": 331, "sub_bw_t": 332, "subalpha_t": 333, "_golden_function_subalpha": 333, "subalpha_bw_t": 334, "subtract_t": 335, "sum_t": 336, "swiglu_t": 337, "_golden_function_swiglu": 337, "swish_t": 338, "queueid": [339, 380, 381], "sub_device_id": 339, "subdeviceid": 339, "synchron": [339, 402], "wait": [339, 381, 385], "complet": [339, 381], "associ": 339, "ran": [339, 385, 397], "chip": 339, "set_sub_device_stall_group": 339, "queu": 339, "tan_t": 340, "tan_bw_t": 341, "_golden_function_tan": 341, "tanh_t": 342, "accuraci": [342, 377], "tanh_bw_t": 343, "_golden_function_tanh": 343, "tanhshrink_t": 344, "_golden_function_tanhshrink": [344, 345], "tanhshrink_bw_t": 345, "threshold_t": 346, "_golden_function_threshold": [346, 347], "threshold_bw_t": 347, "tilize_t": 348, "_nop_golden_funct": 348, "tilize_with_val_padding_t": 349, "800781": 350, "455078": 350, "585938": 350, "to_layout_t": 351, "organ": [351, 377, 386], "becom": 351, "thread": [351, 380, 399, 402], "42188": 351, "398438": 351, "to_memory_config_t": 352, "torch_rank": [353, 402], "Will": 353, "reach": 353, "mesh_compos": 353, "meshtotensor": 353, "torch_tensor": [353, 395], "3008": 353, "8438": 353, "3242": 353, "9023": 353, "5820": 353, "5312": 353, "topk_t": 354, "_create_golden_function_topk": 354, "largest": [354, 381, 386], "smallest": [354, 381], "sure": [354, 383, 387, 402], "bfloat8": 354, "caus": [354, 386], "down": [354, 381], "attention_softmax_t": 355, "head_siz": [355, 356, 357, 360, 397], "attention_mask": [355, 356, 397], "mask": [355, 356, 359], "softmaxprogramconfig": [355, 356], "softmaxdefaultprogramconfig": [355, 356], "causal_mask": [355, 356], "causal": [355, 356, 358], "attention_softmax__t": 356, "concatenate_heads_t": 357, "num_head": [357, 360, 397], "sequence_s": [357, 360, 376, 397, 398, 399], "scaled_dot_product_attention_t": 358, "mimick": 358, "flashattent": 358, "accept": [358, 359, 377, 384], "sdpaprogramconfig": [358, 359], "q": [358, 359], "parallel": [358, 359, 381, 385, 399], "nqh": 358, "input_tensor_q": [358, 359], "dh": [358, 359], "input_tensor_k": [358, 359], "nkv": [358, 359], "input_tensor_v": [358, 359], "attn_mask": [358, 359], "impli": 358, "is_caus": [358, 359], "scaled_dot_product_attention_decode_t": 359, "decod": [359, 401], "mqa": 359, "sdpamulticoreprogramconfig": 359, "nh": 359, "cur_po": 359, "cur_pos_tensor": 359, "pnh": 359, "skip": [359, 395, 396, 397, 398, 399, 400, 401], "split_query_key_value_and_split_heads_t": 360, "hidden_s": [360, 376, 397, 398], "Then": [360, 376, 383], "score": [360, 399], "kv_input_tensor": 360, "q1": 360, "k1": 360, "v1": [360, 383], "qn": 360, "kn": 360, "vn": 360, "cat": [360, 397, 401], "num_kv_head": 360, "contigu": [360, 381, 395], "transpose_kei": 360, "num": 360, "tril_t": 361, "diagon": [361, 362], "triu_t": 362, "trunc_t": 363, "_golden_function_trunc": [363, 364], "trunc_bw_t": 364, "unary_chain_t": 365, "ops_chain": 365, "unarywithparam": 365, "chain": 365, "unaryoptyp": [365, 381], "untilize_t": 366, "use_pack_until": [366, 367], "sub_core_grid": 366, "corerangeset": 366, "untilize_with_unpadding_t": 367, "output_tensor_end": [367, 380], "upsample_t": 368, "nearest": [368, 381], "scale_factor": [368, 381], "array2d": 368, "var_t": 369, "where_t": 370, "_golden_function_wher": 370, "where_bw_t": 371, "xlogy_t": 372, "_golden_function_xlogi": 372, "cdot": 372, "xlogy_bw_t": 373, "zeros_t": 374, "zeros_like_t": 375, "basi": 376, "rewritten": 376, "bert": [376, 398, 399], "modeling_bert": [376, 398], "bertintermedi": 376, "super": [376, 400], "dens": 376, "intermediate_s": 376, "forward": [376, 381, 400], "hidden_st": [376, 397, 398], "tdd": 376, "torch_bert": 376, "utility_funct": 376, "torch_random": 376, "utils_for_test": 376, "assert_with_pcc": 376, "mark": [376, 377], "parametr": 376, "phiyodr": [376, 398], "finetun": [376, 398], "squad2": [376, 398], "384": [376, 397, 398], "test_bert_intermedi": 376, "manual_se": [376, 395, 396, 397, 400, 401, 402], "bertconfig": [376, 398], "from_pretrain": [376, 398, 401], "eval": [376, 398, 399, 400, 401], "torch_hidden_st": [376, 397], "torch_output": [376, 397], "bert_intermedi": 376, "9999": [376, 395, 396, 397, 398, 400, 402], "dictionari": 376, "turn": 376, "ttnn_bert": [376, 398], "999": 376, "someth": 376, "ttnn_optimized_bert": [376, 398], "isinst": 376, "preprocess_linear_weight": [376, 397], "preprocess_linear_bia": [376, 397], "ff1_weight": 376, "ff1_bia": 376, "best": [376, 395], "integr": [376, 377, 378, 381], "incredibli": 377, "excit": 377, "exploratori": 377, "freedom": 377, "showcas": 377, "few": [377, 386, 395], "question": 377, "answer": 377, "see": [377, 381, 382, 384, 395, 399, 401], "highlight": [377, 386], "successfulli": [377, 387, 399, 401], "migrat": [377, 395, 396, 397, 398, 400, 402], "good": 377, "documen": 377, "describ": [377, 381], "credit": 377, "author": 377, "might": [377, 381, 396], "encount": 377, "demonstr": [377, 385], "adequ": 377, "achiev": [377, 384], "pearson": 377, "ci": 377, "pipelin": [377, 385], "unit": [377, 381], "metric": 377, "meet": 377, "continu": [377, 383, 384], "upon": 377, "everi": [377, 380, 385, 397, 402], "commit": [377, 399], "ongo": 377, "complianc": 377, "catch": 377, "regress": 377, "earli": 377, "collect": [377, 385, 387, 399], "varieti": [377, 381], "measur": 377, "special": [377, 381, 386], "run_device_perf_model": 377, "models_device_performance_bare_met": 377, "schedul": 377, "clear": [377, 384, 395, 396, 397, 398, 399, 400], "incorpor": 377, "autom": 377, "extern": [377, 384, 386], "servic": 377, "impact": 377, "run_perf_models_oth": 377, "run_perf_models_llm_javelin": 377, "run_perf_models_cnn_javelin": 377, "models_performance_bare_met": 377, "run_demos_single_card_n150_test": 377, "run_demos_single_card_n300_test": 377, "run_t3000_demo_test": 377, "test_ttnn_functional_resnet50": 377, "resnet50testinfra": 377, "setup": [377, 382, 383, 399, 401], "handl": [378, 395], "machin": [378, 380, 381, 385, 387, 401], "send": [378, 380, 381], "__name__": 378, "__main__": [378, 399], "pci": [378, 395, 396, 397, 398, 399, 400], "slot": 378, "tt_devic": [378, 380, 381], "py_tensor": [378, 380], "tt_tensor": [378, 380, 381], "tolist": [378, 380], "tt_relu_out": 378, "closedevic": 378, "power_fp": 378, "suppli": [378, 380, 381], "lastli": 378, "fallback_op": [378, 381], "py_tensor_exp": 378, "py_relu_out": 378, "py_pow_out": 378, "tt_pow_out": 378, "behav": [378, 381], "regular": 378, "hood": 378, "tt_silu_out": 378, "tt_exp_out": 378, "leav": 378, "anyth": 378, "manipul": 380, "sent": 380, "receiv": [380, 385], "platform": [380, 399], "ttdnn": 380, "z": [380, 381, 385, 401], "construct": [380, 402], "nor": 380, "subsect": 380, "insid": [380, 402], "63": [380, 399], "65": [380, 399, 401], "66": 380, "127": [380, 399], "3968": 380, "3969": 380, "3970": 380, "4031": 380, "4032": 380, "4033": 380, "4034": 380, "4095": 380, "4097": 380, "4098": 380, "4159": 380, "4160": 380, "4161": 380, "6462": 380, "4223": 380, "8064": 380, "8065": 380, "8066": 380, "8127": 380, "8128": 380, "8129": 380, "8130": 380, "8191": 380, "95": 380, "1984": 380, "1985": 380, "2015": [380, 399], "33": [380, 383, 396, 399], "96": [380, 399, 402], "97": [380, 399], "2016": 380, "2017": [380, 399, 401], "2047": 380, "2080": 380, "2081": 380, "2111": 380, "2144": 380, "2145": 380, "2175": 380, "4064": 380, "4065": 380, "fourth": [380, 381], "6111": 380, "6176": 380, "ownedstorag": [380, 381], "borrowedstorag": 380, "devicestorag": [380, 381], "pointer": 380, "That": [380, 386, 396], "numpi": [380, 386, 399, 401], "reason": 380, "data_typ": [380, 397], "No": [380, 381], "bank": 380, "arg1": 380, "arg2": 380, "arg3": 380, "arg4": 380, "arg5": 380, "arg6": 380, "divisbl": [380, 381], "arg7": 380, "single_bank": 380, "ptr": 380, "np": 380, "hostbuff": 380, "everywher": 380, "inp": 380, "tt_tensor_pad": 380, "npad": 380, "bottom": [380, 387], "storagetyp": 380, "memory_layout": [380, 381], "shard_spec": 380, "cq": 380, "uint8_t": 380, "ti": 380, "output_tensor_start": 380, "tt_tensor_unpad": 380, "nunpad": 380, "apart": 380, "restrict": 380, "eight": 380, "shardspec": 380, "dram_channel": 380, "rememb": 380, "py_output": 380, "unifi": 381, "tt_eager": [381, 399], "caller": 381, "launch": [381, 387], "plug": 381, "declar": 381, "newoper": 381, "programwithcallback": 381, "create_program": 381, "some_memb": 381, "optional_input_tensor": 381, "validate_with_output_tensor": 381, "programwithoptionaloutputtensor": 381, "box": [381, 383], "preferred_nam": 381, "parallelization_strategi": 381, "get_parallelization_strategi": 381, "parallelizationstrategyenum": 381, "enqueu": 381, "finish": [381, 385, 399], "asynchron": 381, "reload": 381, "program_cach": 381, "disable_and_clear": 381, "entri": 381, "num_entri": 381, "cachabl": 381, "override_runtime_args_callback": 381, "input_buff": 381, "output_buff": 381, "src_dram_buff": 381, "dst_dram_buff": 381, "tt_metal_logger_typ": [381, 402], "tt_metal_logger_level": [381, 402], "1280": 381, "layoutconversiononhost": 381, "miss": [381, 402], "eltwiseunari": 381, "op_typ": 381, "_tt": 381, "mul": [381, 397], "ellipsi": 381, "output_on_devic": 381, "third": 381, "fewer": 381, "four": 381, "paper": 381, "separ": 381, "normalized_shap": 381, "layer": [381, 385, 400], "m": [381, 383, 396], "reflect": [381, 384], "replic": 381, "align_corn": 381, "recompute_scale_factor": 381, "antialia": 381, "bilinear": 381, "bicub": 381, "trilinear": 381, "area": 381, "center": 381, "corner": 381, "pixel": 381, "anti": 381, "alias": 381, "output_s": 381, "total": [381, 385], "sigma": 381, "logist": 381, "x_": 381, "x_i": 381, "sum_j": 381, "x_j": 381, "lie": 381, "padding_mod": 381, "simplest": 381, "c_": 381, "h_": 381, "w_": 381, "precis": [381, 386], "n_i": 381, "_j": 381, "star": 381, "learnabl": 381, "num_batches_track": 381, "num_featur": 381, "affin": 381, "track_running_stat": 381, "4d": 381, "deep": 381, "covari": 381, "track": 381, "num_channel": 381, "lernabl": 381, "elementwise_affin": 381, "return_indic": 381, "channels_last": [381, 385], "reshape_2d": 381, "c_j": 381, "max_": 381, "ldot": 381, "implicit": 381, "infin": [381, 386], "mod": 381, "dividend": 381, "bitwis": 381, "NOT": [381, 395, 396, 397, 398, 400], "immedi": 381, "arithmet": 381, "operand": 381, "promot": 381, "behavior": [381, 386], "retain": 381, "argmin": 381, "fusion": 381, "togeth": 381, "fused_op": 381, "in_featur": 381, "out_featur": 381, "num_dim": 381, "moment": 381, "add_and_norm": 381, "flexibl": 381, "earlier": 381, "while": [381, 384], "ml": 382, "workload": 382, "choic": [382, 383, 395], "learn": [382, 386, 396, 399], "dive": 382, "deeper": 382, "jupyt": [382, 387, 399], "notebook": [382, 387, 399, 401], "softwar": [383, 387, 395, 396, 397, 398, 400], "packag": [383, 398, 399, 401], "asset": 383, "tag": 383, "compat": [383, 388, 391, 392], "galaxi": 383, "4u": 383, "ubuntu": [383, 395, 396, 397, 398, 399, 400, 401], "04": [383, 401], "fw_pack": 383, "v2": 383, "6u": 383, "17": [383, 397, 398, 399, 400, 401], "v80": 383, "v3": 383, "t3000": 383, "blackhol": 383, "18": [383, 395, 396, 397, 398, 399, 401], "wget": 383, "raw": [383, 401], "githubusercont": 383, "install_depend": 383, "sfpi": 383, "chmod": 383, "sudo": [383, 385], "dkm": 383, "debian": 383, "apt": 383, "fedora": 383, "dnf": 383, "enterpris": 383, "linux": [383, 399], "epel": 383, "cd": [383, 385, 399], "modprob": 383, "visit": 383, "pip": [383, 399, 401], "fw_tag": 383, "fwbundl": 383, "t300": 383, "fw": [383, 385, 395, 396, 397, 398, 400], "tar": 383, "correctli": 383, "telemetri": 383, "caution": 383, "Be": [383, 387], "loudbox": 383, "quietbox": 383, "closer": 383, "quickest": 383, "ai": [383, 395, 396, 397, 398, 399, 400], "quick": 383, "who": [383, 386], "recurs": 383, "build_met": [383, 385, 399], "pandoc": [383, 399], "libtbb": 383, "dev": [383, 399], "libcapston": 383, "pkg": 383, "doxygen": 383, "registri": 383, "pull": [383, 384], "ghcr": 383, "io": [383, 385], "amd64": 383, "rm": 383, "hugepag": [383, 395, 396, 397, 398, 399, 400], "1g": 383, "bash": [383, 399], "navig": 383, "architectur": [383, 388, 391, 392, 402], "wheel_fil": 383, "whl": [383, 399, 401], "governor": 383, "txt": [383, 399], "cpufrequtil": 383, "cpupow": 383, "frequenc": 383, "arch_nam": [383, 397, 398], "tt_metal_hom": [383, 385, 399], "python3": [383, 398, 399, 401], "run_op_on_devic": 383, "intend": 384, "reliabl": 384, "simultan": 384, "fine": 384, "tune": 384, "themselv": [384, 386], "goal": 384, "ask": 384, "driven": 384, "popular": 384, "kent": 384, "beck": 384, "benefit": 384, "submit": 384, "label": [384, 386], "fulli": [384, 385], "branch": 384, "brief": 384, "4730": 384, "rst": 384, "referenc": [384, 399], "sweep": 384, "codeown": 384, "pr": 384, "comment": 384, "test_perf_resnet": 385, "test_perf_bare_met": 385, "0185": 385, "consol": 385, "shorter": 385, "cli": 385, "explain": 385, "reset": 385, "tt_smi": 385, "tensix_reset": 385, "tensix": 385, "skew": 385, "timer": 385, "reboot": 385, "wh": 385, "analyz": 385, "1000": [385, 400, 401], "fixtur": 385, "ttl": 385, "dumpdeviceprofil": 385, "drop": 385, "around": 385, "120": [385, 399], "eighth": 385, "warn": [385, 395, 396, 397, 398, 399, 400], "mention": 385, "risc": 385, "those": 385, "analysi": 385, "affect": 385, "flow": 385, "python_fallback": [385, 399], "tt_dnn_cpu": 385, "tt_dnn_devic": [385, 399], "global": [385, 399], "fidel": [385, 399], "field": 385, "lofi": [385, 400], "hifi2": 385, "hifi3": 385, "clock": 385, "stamp": 385, "durat": [385, 397, 399, 402], "nanosecond": 385, "end_t": 385, "start_t": 385, "cycl": 385, "earliest": 385, "core_frequ": 385, "marker": 385, "brisc": 385, "ncrisc": 385, "trisc0": 385, "trisc1": 385, "trisc2": 385, "cb": 385, "front": [385, 401], "spent": 385, "cb_wait_front": 385, "reserv": 385, "cb_reserve_back": 385, "datamov": 385, "input_0_memori": 385, "dev_0_dram": 385, "dec_0_l1": 385, "tgz": 385, "filenam": [385, 401], "item": [385, 399], "aggreg": 385, "timestamp": 385, "4x4": 386, "32x32": 386, "still": 386, "transit": 386, "2x2": 386, "illustr": 386, "byte": 386, "sizeof": 386, "introduc": 386, "observ": 386, "magnitud": [386, 397], "flush": 386, "instabl": 386, "extrem": 386, "domin": 386, "lose": 386, "7014118346046923e": 386, "dataset": [386, 399], "frequent": 386, "occurr": 386, "uniform": 386, "deal": 386, "critic": 386, "homogen": 386, "unsuit": 386, "inher": 386, "owned_host_storag": 386, "borrowed_host_storag": 386, "device_storag": 386, "abstract": 386, "awai": 386, "compress": 386, "upper": 386, "remain": 386, "128x128": 386, "subset": 386, "know": 386, "understand": 386, "unshard": 386, "coordin": 386, "physic": 386, "certain": 387, "ramp": 387, "skillset": 387, "tree": 387, "lab": 387, "port": 387, "8888": 387, "hint": 387, "cell": 387, "central": 395, "sens": 395, "sram": 395, "concept": 395, "2024": [395, 396, 397, 398, 399, 400, 401, 402], "07": [395, 396, 397, 398, 400], "48": [395, 398, 399], "818": 395, "136": [395, 396, 397, 398], "cache_path": [395, 396, 397, 398, 400], "posixpath": [395, 396, 397, 398], "home": [395, 396, 397, 398, 399, 400, 401], "comparison_mode_pcc": [395, 396, 397, 398, 400, 402], "enable_comparison_mod": [395, 396, 397, 398, 400, 402], "enable_detailed_buffer_report": [395, 396, 397, 398, 400, 402], "enable_detailed_tensor_report": [395, 396, 397, 398, 400, 402], "enable_fast_runtime_mod": [395, 396, 397, 398, 400, 402], "enable_graph_report": [395, 396, 397, 398, 400, 402], "enable_log": [395, 396, 397, 398, 400, 402], "enable_model_cach": [395, 396, 397, 398, 400], "model_cache_path": [395, 396, 397, 398, 400], "report_nam": [395, 396, 397, 398, 400, 402], "root_report_path": [395, 396, 397, 398, 400], "throw_exception_on_fallback": [395, 396, 397, 398, 400], "tmp_dir": [395, 396, 397, 398, 400, 401], "905": 395, "operation_decor": [395, 396, 397, 398, 400], "758": [395, 396, 397, 398], "906": 395, "907": [395, 397], "908": [395, 397], "909": [395, 397], "910": [395, 397], "911": [395, 397], "914": [395, 397], "915": [395, 397], "pearson_correlation_coeffici": [395, 396, 397, 398, 400], "919": [395, 397], "920": 395, "921": [395, 397], "unsqueeze_to_4d": [395, 396, 397, 398, 400], "922": [395, 397], "923": [395, 397], "924": [395, 397], "925": [395, 397], "926": [395, 397], "allocate_tensor_on_devic": [395, 396, 397, 398, 400], "copy_host_to_device_tensor": [395, 396, 397, 398, 400], "927": [395, 397], "928": [395, 397], "929": [395, 397], "930": [395, 397], "931": [395, 397], "934": [395, 397], "935": 395, "936": [395, 397, 398], "937": 395, "938": [395, 397], "941": [395, 397], "942": [395, 397], "943": [395, 397], "948": [395, 397], "949": [395, 397, 398], "950": [395, 397], "951": [395, 397, 398], "952": [395, 397], "953": [395, 397], "954": [395, 397], "955": [395, 397], "958": [395, 397], "959": 395, "960": [395, 397, 398], "f": [395, 396, 397, 399, 401, 402], "As": [395, 396], "1234": 395, "again": 395, "action": 395, "98300": 395, "11301": 395, "37592": 395, "64318": 395, "53437": 395, "59434": 395, "69190": 395, "04268": 395, "33346": 395, "20231": 395, "15127": 395, "58303": 395, "pick": 395, "80078": 395, "69531": 395, "71484": 395, "33398": 395, "60156": 395, "36523": 395, "73047": 395, "90625": 395, "59766": 395, "83203": 395, "61719": 395, "53516": 395, "effici": [395, 396], "transfer": 395, "intuit": 395, "nshape": 395, "nlayout": 395, "21680": 395, "24316": 395, "19336": 395, "40625": 395, "81641": 395, "50781": 395, "09961": 395, "54688": 395, "70703": 395, "93359": 395, "06787": 395, "75781": 395, "insert": 395, "cale": 395, "49": [395, 399, 401], "027": [395, 396], "silicondriv": [395, 396, 397, 398, 399, 400], "detect": [395, 396, 397, 398, 399, 400], "040": 395, "init_detect_tt_device_numanod": [395, 396, 397, 398, 399, 400], "could": [395, 396, 397, 398, 399, 400], "numanodeset": [395, 396, 397, 398, 399, 400], "pci_bus_id": [395, 396, 397, 398, 399, 400], "0000": [395, 396, 397, 398, 399, 400], "00": [395, 396, 397, 398, 399, 400, 401], "041": 395, "bind_area_memory_nodeset": [395, 396, 397, 398, 399, 400], "unabl": [395, 396, 397, 398, 399, 400], "numanod": [395, 396, 397, 398, 399, 400], "membind": [395, 396, 397, 398, 399, 400], "ttsilicondevic": [395, 396, 397, 398, 399, 400], "init_hugepag": [395, 396, 397, 398, 399, 400], "bind_area_to_memory_nodeset": [395, 396, 397, 398, 399, 400], "fail": [395, 396, 397, 398, 399, 400], "ch": [395, 396, 397, 398, 399, 400], "effect": [395, 396, 397, 398, 399, 400], "decreas": [395, 396, 397, 398, 399, 400], "893": [395, 396, 397, 398, 399, 400], "082": 395, "ethernet": [395, 396, 397, 398, 400], "clk": [395, 396, 397, 398, 399, 400], "800": [395, 396, 397, 398], "mhz": [395, 396, 397, 398, 399, 400], "torch_input_tensor_a": [395, 402], "torch_input_tensor_b": [395, 402], "therefor": [395, 397], "stai": 395, "unless": [395, 402], "explicit": 395, "figur": 395, "hang": 395, "properli": 395, "41": [396, 399], "903": 396, "989": [396, 397], "990": [396, 397], "991": 396, "992": 396, "993": 396, "994": 396, "995": 396, "996": 396, "001": 396, "002": 396, "003": 396, "004": 396, "005": 396, "006": 396, "007": 396, "008": 396, "009": 396, "010": 396, "011": 396, "012": 396, "013": 396, "015": 396, "016": 396, "017": 396, "018": 396, "020": 396, "021": 396, "022": 396, "028": 396, "029": 396, "030": 396, "031": 396, "032": 396, "033": 396, "035": 396, "036": 396, "037": 396, "053": 396, "066": 396, "067": 396, "094": 396, "repeatedli": 396, "enable_program_cach": [396, 397, 402], "torch_a": 396, "torch_b": 396, "longer": 396, "signific": 396, "aslo": 396, "why": 396, "conver": 396, "todo": 396, "75000": 396, "25000": 396, "50000": 396, "62500": 396, "effeci": 396, "enjoi": 396, "massiv": 396, "eth": [397, 398], "54": [397, 399], "821": 397, "912": 397, "939": 397, "976": 397, "55": [397, 399], "014": 397, "fashion": 397, "multi_head_attent": 397, "query_weight": 397, "query_bia": 397, "key_weight": 397, "key_bia": 397, "value_weight": 397, "value_bia": 397, "output_weight": 397, "output_bia": 397, "fallback_reshap": 397, "get_fallback_funct": [397, 402], "attention_scor": 397, "attention_prob": 397, "context_lay": 397, "self_output": 397, "torch_attention_mask": [397, 398], "torch_query_weight": 397, "torch_query_bia": 397, "torch_key_weight": 397, "torch_key_bia": 397, "torch_value_weight": 397, "torch_value_bia": 397, "torch_output_weight": 397, "torch_output_bia": 397, "00607705116272": 397, "250946044921875": 397, "ahead": 397, "optimized_multi_head_attent": 397, "fused_qkv_weight": 397, "fused_qkv_bia": 397, "self_output_weight": 397, "self_output_bia": 397, "fused_qkv_output": 397, "context_layer_after_concatenate_head": 397, "qkv": 397, "torch_qkv_weight": 397, "torch_qkv_bia": 397, "qkv_weight": 397, "qkv_bia": 397, "optimized_output": 397, "474989175796509": 397, "020017147064208984": 397, "torch_optimized_output": 397, "allclos": 397, "19": [397, 399, 401], "ttnn_config_overrid": [398, 400, 402], "47": [398, 399], "183": 398, "133": [398, 399], "overrid": [398, 399, 402], "184": 398, "354": 398, "355": 398, "356": 398, "357": 398, "358": 398, "359": 398, "360": [398, 401], "362": 398, "366": 398, "367": 398, "368": 398, "369": 398, "370": 398, "371": 398, "372": 398, "373": [398, 399], "374": 398, "378": 398, "379": 398, "380": [398, 399], "381": 398, "383": 398, "385": 398, "390": 398, "391": 398, "392": 398, "393": 398, "394": 398, "395": 398, "396": 398, "397": 398, "398": 398, "399": 398, "set_verbosity_error": 398, "100": [398, 399], "412": 398, "442": 398, "447": 398, "googl": [398, 401], "bert_uncased_l": 398, "4_h": 398, "256_a": 398, "bertselfoutput": 398, "site": [398, 399, 401], "huggingface_hub": [398, 399], "file_download": 398, "1132": 398, "futurewarn": 398, "resume_download": 398, "resum": 398, "force_download": 398, "874": 398, "num_hidden_lay": 398, "bertforquestionansw": 398, "input_id": 398, "vocab_s": 398, "torch_token_type_id": 398, "torch_position_id": 398, "ttnn_bert_input": 398, "preprocess_input": 398, "bert_for_question_answ": 398, "50": 398, "339": 398, "manage_config": [398, 400, 402], "144": 398, "340": 398, "341": 398, "555": 398, "_paramet": [398, 400], "env": [398, 399, 400, 401], "34": [398, 399, 400], "343": 398, "634": 398, "636": 398, "147": [398, 399], "restor": [398, 400], "02": [398, 399], "947": 398, "unset": 399, "silent": 399, "nuke": 399, "jobserv": 399, "unavail": 399, "j1": 399, "parent": 399, "rule": 399, "artifact": 399, "conf": 399, "backend": 399, "pypi": [399, 401], "org": [399, 401], "satisfi": [399, 401], "setuptool": 399, "44": 399, "py3": 399, "kb": 399, "edit": 399, "obtain": 399, "statu": 399, "metadata": [399, 401], "click": 399, "loguru": 399, "58": 399, "ipywidget": 399, "139": 399, "90": [399, 400], "db": 399, "290ab3a34f2ef0b5a0f89235dc2d40fea83e77de84ed2dc05c": 399, "pyyaml": [399, 401], "cp38": 399, "linux_x86_64": 399, "jupyterlab": 399, "mb": 399, "pyelftool": 399, "py2": 399, "174": 399, "4f": 399, "ed": 399, "863cf4386fe6db3c09333712009ec1c5146a36f3904b469d13": 399, "curtsi": 399, "91": 399, "b7": 399, "0c117d73912c6c2beb1eb0d7d6884f4e79e6e5b5e91eeb34f5": 399, "torchtrail": 399, "manylinux_2_12_x86_64": 399, "manylinux2010_x86_64": 399, "matplotlib": 399, "toolz": 399, "pillow": [399, 401], "manylinux_2_17_x86_64": 399, "manylinux2014_x86_64": 399, "panda": 399, "2bcpu": 399, "199": 399, "dash": 399, "rich": 399, "238": 399, "seaborn": 399, "293": 399, "plotli": 399, "traitlet": 399, "85": 399, "widgetsnbextens": 399, "ipython": [399, 400, 401], "798": 399, "widget": 399, "jupyterlab_widget": 399, "215": 399, "comm": 399, "async": 399, "lru": 399, "async_lru": 399, "tomli": 399, "python_vers": 399, "server": 399, "jupyter_serv": 399, "jinja2": [399, 401], "ipykernel": 399, "116": 399, "shim": 399, "notebook_shim": 399, "jupyterlab_serv": 399, "lsp": 399, "jupyter_lsp": 399, "68": 399, "23": 399, "53": 399, "importlib": [399, 401], "importlib_resourc": 399, "importlib_metadata": 399, "jupyter_cor": 399, "tornado": 399, "abi3": 399, "manylinux_2_5_x86_64": 399, "manylinux1_x86_64": 399, "435": 399, "bless": 399, "cwcwidth": 399, "92": 399, "pyrsist": 399, "121": 399, "graphviz": 399, "networkx": [399, 401], "pypars": 399, "103": 399, "kiwisolv": 399, "contourpi": 399, "301": 399, "fonttool": 399, "dateutil": 399, "python_dateutil": 399, "247": [399, 400], "cycler": 399, "pytz": 399, "2020": 399, "505": 399, "extens": [399, 401], "typing_extens": 399, "html": 399, "compon": 399, "dash_html_compon": 399, "dash_tabl": 399, "flask": 399, "101": 399, "dash_core_compon": 399, "pygment": 399, "markdown": 399, "markdown_it_pi": 399, "84": 399, "tenac": 399, "24": [399, 401], "pickleshar": 399, "prompt": 399, "toolkit": 399, "37": 399, "prompt_toolkit": 399, "43": 399, "386": 399, "stack": 399, "stack_data": 399, "backcal": 399, "jedi": 399, "pexpect": 399, "sys_platform": 399, "win32": 399, "inlin": 399, "matplotlib_inlin": 399, "send2trash": 399, "anyio": 399, "termin": 399, "jupyter_server_termin": 399, "client": 399, "jupyter_cli": 399, "105": 399, "nbformat": 399, "77": 399, "nbconvert": 399, "257": [399, 400], "event": 399, "jupyter_ev": 399, "websocket": 399, "websocket_cli": 399, "pyzmq": 399, "prometheu": 399, "prometheus_cli": 399, "argon2": 399, "cffi": 399, "argon2_cffi": 399, "terminado": 399, "markupsaf": [399, 401], "26": 399, "nest": 399, "asyncio": 399, "nest_asyncio": 399, "psutil": 399, "cp36": 399, "288": 399, "debugpi": 399, "babel": 399, "62": 399, "jsonschema": 399, "json5": 399, "zipp": [399, 401], "platformdir": 399, "six": 399, "wcwidth": 399, "itsdanger": 399, "blinker": 399, "werkzeug": 399, "226": 399, "mdurl": 399, "pure": 399, "pure_ev": 399, "asttoken": 399, "27": [399, 401], "parso": 399, "ptyprocess": 399, "exceptiongroup": 399, "idna": [399, 401], "61": 399, "sniffio": 399, "fastjsonschema": 399, "defusedxml": 399, "beautifulsoup4": 399, "jupyterlab_pyg": 399, "pandocfilt": 399, "mistun": 399, "tinycss2": 399, "bleach": 399, "162": 399, "nbclient": 399, "rfc3986": 399, "rfc3986_valid": 399, "json": [399, 402], "logger": 399, "python_json_logg": 399, "rfc3339": 399, "rfc3339_valid": 399, "argon2_cffi_bind": 399, "86": 399, "urllib3": [399, 401], "charset": [399, 401], "charset_norm": 399, "141": 399, "certifi": [399, 401], "163": 399, "03": [399, 400], "jsonschema_specif": 399, "pkgutil": 399, "resolv": 399, "pkgutil_resolve_nam": 399, "attr": 399, "rpd": 399, "rpds_py": 399, "soupsiev": 399, "36": 399, "webencod": 399, "444": 399, "pycpars": 399, "118": 399, "pre_commit": 399, "202": 399, "black": 399, "twine": 399, "yamllint": 399, "docutil": 399, "570": 399, "sphinx": 399, "rtd": 399, "theme": 399, "sphinx_rtd_them": 399, "sphinxcontrib": 399, "email": 399, "sphinxcontrib_email": 399, "lxml": 399, "manylinux_2_24_x86_64": 399, "breath": 399, "35": 399, "nbsphinx": 399, "jqueri": 399, "sphinxcontrib_jqueri": 399, "3a": 399, "a8": 399, "3237a93e3a6261bd24edabf3277ca59f64c1710b3d8c7c72a0": 399, "317": 399, "timeout": 399, "pytest_timeout": 399, "6c": 399, "5706d21e6b4dff52e7af12bff9ca126a3f15beb4dff386aa29": 399, "jsbeautifi": 399, "462": 399, "xlsxwriter": 399, "152": 399, "tiktoken": 399, "tqdm": [399, 401], "sentencepiec": 399, "numba": 399, "56": [399, 400], "librosa": 399, "252": [399, 400], "timm": [399, 401], "549": 399, "opencv": 399, "headless": 399, "74": 399, "opencv_python_headless": 399, "cp37": 399, "diffus": [399, 401], "604": 399, "219": 399, "ftfy": 399, "gitpython": 399, "188": 399, "einop": 399, "multiprocess": 399, "70": 399, "py38": 399, "132": 399, "81": 399, "bert_scor": 399, "fsspec": [399, 401], "173": 399, "nodeenv": 399, "cfgv": 399, "98": 399, "virtualenv": 399, "pathspec": 399, "mypi": 399, "mypy_extens": 399, "pyproject_hook": 399, "render": 399, "readme_render": 399, "pkginfo": 399, "toolbelt": 399, "requests_toolbelt": 399, "keyr": 399, "images": 399, "serializinghtml": 399, "sphinxcontrib_serializinghtml": 399, "94": 399, "jsmath": 399, "sphinxcontrib_jsmath": 399, "snowballstemm": 399, "93": [399, 400], "htmlhelp": 399, "sphinxcontrib_htmlhelp": 399, "99": 399, "alabast": 399, "applehelp": 399, "sphinxcontrib_applehelp": 399, "devhelp": 399, "sphinxcontrib_devhelp": 399, "qthelp": 399, "sphinxcontrib_qthelp": 399, "ply": 399, "plumbum": 399, "iniconfig": 399, "pluggi": 399, "0rc8": 399, "editorconfig": 399, "pyarrow": 399, "xxhash": 399, "194": 399, "huggingfac": [399, 401], "hub": [399, 401], "330": 399, "aiohttp": 399, "dill": 399, "110": 399, "regex": [399, 401], "2019": [399, 401], "777": 399, "filelock": [399, 401], "llvmlite": 399, "0dev0": 399, "soxr": 399, "soundfil": 399, "pooch": 399, "lazi": 399, "loader": 399, "lazy_load": 399, "scipi": 399, "joblib": 399, "302": 399, "audioread": 399, "scikit": 399, "scikit_learn": 399, "msgpack": 399, "534": 399, "gitdb": 399, "distlib": 399, "468": 399, "nh3": 399, "secretstorag": 399, "jeepnei": 399, "jaraco": 399, "frozenlist": 399, "240": [399, 400], "async_timeout": 399, "aiosign": 399, "yarl": 399, "308": 399, "multidict": 399, "129": 399, "threadpoolctl": 399, "smmap": 399, "cryptographi": 399, "itertool": 399, "more_itertool": 399, "57": 399, "pyproject": 399, "uninstal": 399, "msg": 399, "t5": 399, "integration_test": 399, "test_perform": 399, "test_t5_for_conditional_gener": 399, "functional_t5": 399, "ttnn_functional_t5": 399, "09": [399, 400], "ops_devic": 399, "session": 399, "cachedir": 399, "pytest_cach": 399, "rootdir": 399, "configfil": 399, "ini": 399, "plugin": 399, "600": 399, "func_onli": 399, "670": 399, "681": 399, "08": 399, "684": [399, 400], "1202": 399, "llruntim": 399, "watcher": 399, "watch": 399, "109": 399, "465": 399, "ttnn_t5": 399, "6ba823894": 399, "149": 399, "484": 399, "487": 399, "216": 399, "489": 399, "721": 399, "359902381896973": 399, "07123565673828": 399, "722": 399, "102": 399, "44269247283137575": 399, "detach": 399, "summari": 399, "627": 399, "638": 399, "639": 399, "458": 399, "224": 399, "460": 399, "292": 399, "164": 399, "22393798828125": 399, "165": 399, "322504758834839": 399, "407821983919596": 399, "pd": 399, "glob": 399, "getenv": 399, "get_latest_report": 399, "base_path": 399, "latest_dir": 399, "listdir": 399, "isdir": 399, "getmtim": 399, "valueerror": [399, 400], "latest_profile_report": 399, "df": 399, "read_csv": 399, "2024_02_09_01_38_37": 399, "ops_perf_results_resnet_2024_02_09_01_38_37": 399, "output_0_w": 399, "output_0_z": 399, "output_0_i": 399, "output_0_x": 399, "output_0_layout": 399, "output_0_data": 399, "output_0_memori": 399, "depth": 399, "compileprogram": 399, "load_tensor_ttnn": 399, "137428381893955": 399, "137428382188762": 399, "294807": 399, "137428382500949": 399, "137428399402163": 399, "16901214": 399, "137428399802068": 399, "137428399873758": 399, "71690": 399, "137428400102635": 399, "137428400351033": 399, "248398": 399, "137428400548071": 399, "137428400792528": 399, "244457": 399, "4391": 399, "reshape_ttnn": 399, "4392": 399, "137450414555424": 399, "137450414599894": 399, "44470": 399, "4393": 399, "137450414740752": 399, "137450414782422": 399, "41670": 399, "4394": 399, "bcast_batch": 399, "tt_me": 399, "108": 399, "matmulparallelizationstrategi": 399, "multi_cor": 399, "137450414881851": 399, "137450414983440": 399, "101589": 399, "32128": 399, "dev_0_dram_interleav": 399, "4395": 399, "137450415113099": 399, "137450415158748": 399, "45649": 399, "from_device_ttnn": 399, "4396": 399, "137450415235897": 399, "137450453493048": 399, "38257151": 399, "fold_batch_norm2d_into_conv2d": 400, "168": 400, "82": 400, "768": 400, "242": 400, "246": 400, "248": 400, "249": 400, "250": [400, 401], "251": 400, "253": 400, "254": 400, "255": 400, "258": 400, "262": 400, "avg_pool2d": 400, "266": 400, "268": 400, "269": 400, "device_param": 400, "24576": 400, "310": 400, "324": 400, "325": 400, "363": 400, "conv3x3": 400, "in_plan": 400, "out_plan": 400, "3x3": 400, "torchbasicblock": 400, "expans": 400, "inplan": 400, "base_width": 400, "norm_lay": 400, "basicblock": 400, "notimplementederror": 400, "conv1": 400, "bn1": 400, "conv2": 400, "bn2": 400, "torch_model": 400, "state_dict": [400, 401], "create_custom_preprocessor": 400, "conv_weight_1": 400, "conv_bias_1": 400, "conv_weight_2": 400, "conv_bias_2": 400, "682": 400, "683": 400, "499": 400, "_initialize_model_and_preprocess_paramet": 400, "449": 400, "717": 400, "718": 400, "model_resnet_block_graph": 400, "conv_param": 400, "act_block_h": 400, "__call__": 400, "fp32_dest_acc_en": 400, "packer_l1_accum_en": 400, "_out_height": 400, "_out_width": 400, "return_output_s": 400, "return_prepared_device_weight": 400, "ttnnbasicblock": 400, "get_memory_config": 400, "ttnn_model": 400, "12638": 400, "walk": 401, "colab": 401, "research": 401, "run_dit": 401, "ipynb": 401, "tab": 401, "ov": 401, "assumpt": 401, "chdir": 401, "content": 401, "upgrad": 401, "save_imag": 401, "create_diffus": 401, "autoencoderkl": 401, "find_model": 401, "collis": 401, "pil": 401, "set_grad_en": 401, "cuda": 401, "is_avail": 401, "gpu": 401, "322": 401, "fatal": 401, "destin": 401, "date": 401, "safetensor": 401, "sympi": 401, "mpmath": 401, "image_s": 401, "512": 401, "vae_model": 401, "stabilityai": 401, "sd": 401, "vae": 401, "ft": 401, "ema": 401, "mse": 401, "latent_s": 401, "input_s": 401, "pt": 401, "load_state_dict": 401, "num_sampling_step": 401, "slider": 401, "cfg_scale": 401, "class_label": 401, "207": 401, "387": 401, "974": 401, "88": 401, "979": 401, "417": 401, "279": 401, "samples_per_row": 401, "nois": 401, "len": 401, "classifi": 401, "free": 401, "guidanc": 401, "y_null": 401, "model_kwarg": 401, "p_sample_loop": 401, "forward_with_cfg": 401, "clip_denois": 401, "null": 401, "18215": 401, "dit_model_graph": 401, "png": 401, "nrow": 401, "value_rang": 401, "06": 401, "987": 401, "210": 401, "show_svg": 401, "snippet": 402, "matmul_output_tensor": 402, "torch_matmul_output_tensor": 402, "unlik": 402, "start_tim": 402, "end_tim": 402, "stdout": 402, "6391518115997314": 402, "0007393360137939453": 402, "9998": 402, "exp_trac": 402, "substitut": 402, "disk": 402, "implementaiton": 402, "addition": 402, "ttnn_config_path": 402, "app": 402, "pre_hook_to_print_args_and_kwarg": 402, "post_hook_to_print_output": 402, "query_registered_oper": 402, "begin_graph_captur": 402, "runmod": 402, "no_dispatch": 402, "captured_graph": 402, "end_graph_captur": 402, "pretty_print": 402}, "objects": {"tt_lib.fallback_ops": [[381, 0, 1, "", "AdaptiveAvgPool2d"], [381, 0, 1, "", "BatchNorm2d"], [381, 0, 1, "", "Conv2d"], [381, 0, 1, "", "GroupNorm"], [381, 0, 1, "", "LayerNorm"], [381, 0, 1, "", "MaxPool2d"], [381, 0, 1, "", "binary_bitwise_and"], [381, 0, 1, "", "binary_bitwise_left_shift"], [381, 0, 1, "", "binary_bitwise_or"], [381, 0, 1, "", "binary_bitwise_right_shift"], [381, 0, 1, "", "binary_bitwise_xor"], [381, 0, 1, "", "binary_fmod"], [381, 0, 1, "", "bitwise_not"], [381, 0, 1, "", "ceil"], [381, 1, 1, "", "chunk"], [381, 1, 1, "", "concat"], [381, 1, 1, "", "conv2d"], [381, 0, 1, "", "floor"], [381, 1, 1, "", "full"], [381, 1, 1, "", "group_norm"], [381, 1, 1, "", "interpolate"], [381, 1, 1, "", "layer_norm"], [381, 1, 1, "", "pad"], [381, 1, 1, "", "repeat"], [381, 1, 1, "", "repeat_interleave"], [381, 1, 1, "", "reshape"], [381, 1, 1, "", "silu"], [381, 1, 1, "", "softmax"], [381, 1, 1, "", "tensor_slice"], [381, 0, 1, "", "torch_argmax"], [381, 0, 1, "", "torch_argmin"], [381, 0, 1, "", "trunc"], [381, 0, 1, "", "unary_bitwise_and"], [381, 0, 1, "", "unary_bitwise_left_shift"], [381, 0, 1, "", "unary_bitwise_or"], [381, 0, 1, "", "unary_bitwise_right_shift"], [381, 0, 1, "", "unary_bitwise_xor"], [381, 0, 1, "", "unary_fmod"]], "tt_lib.fused_ops.add_and_norm": [[381, 1, 1, "", "AddAndNorm"]], "tt_lib.fused_ops.layernorm": [[381, 1, 1, "", "Layernorm"]], "tt_lib.fused_ops.linear": [[381, 1, 1, "", "Linear"]], "ttnn": [[381, 0, 1, "", "BcastOpDim"], [381, 0, 1, "", "BcastOpMath"], [8, 0, 1, "", "Conv2dConfig"], [9, 0, 1, "", "Conv2dSliceConfig"], [10, 1, 1, "", "GetDefaultDevice"], [380, 0, 1, "", "MemoryConfig"], [11, 1, 1, "", "SetDefaultDevice"], [386, 0, 1, "", "Shape"], [380, 0, 1, "", "Tensor"], [12, 5, 1, "", "abs"], [13, 5, 1, "", "abs_bw"], [14, 5, 1, "", "acos"], [15, 5, 1, "", "acos_bw"], [16, 5, 1, "", "acosh"], [17, 5, 1, "", "acosh_bw"], [18, 5, 1, "", "add"], [19, 5, 1, "", "add_bw"], [20, 5, 1, "", "addalpha"], [21, 5, 1, "", "addalpha_bw"], [22, 5, 1, "", "addcdiv"], [23, 5, 1, "", "addcdiv_bw"], [24, 5, 1, "", "addcmul"], [25, 5, 1, "", "addcmul_bw"], [26, 5, 1, "", "all_gather"], [27, 5, 1, "", "alt_complex_rotate90"], [28, 5, 1, "", "angle"], [29, 5, 1, "", "angle_bw"], [30, 5, 1, "", "arange"], [31, 5, 1, "", "argmax"], [32, 5, 1, "", "as_tensor"], [33, 5, 1, "", "asin"], [34, 5, 1, "", "asin_bw"], [35, 5, 1, "", "asinh"], [36, 5, 1, "", "asinh_bw"], [37, 5, 1, "", "assign_bw"], [38, 5, 1, "", "atan"], [39, 5, 1, "", "atan2"], [40, 5, 1, "", "atan2_bw"], [41, 5, 1, "", "atan_bw"], [42, 5, 1, "", "atanh"], [43, 5, 1, "", "atanh_bw"], [44, 5, 1, "", "batch_norm"], [45, 5, 1, "", "bias_gelu_bw"], [46, 5, 1, "", "bitwise_and"], [47, 5, 1, "", "bitwise_left_shift"], [48, 5, 1, "", "bitwise_not"], [49, 5, 1, "", "bitwise_or"], [50, 5, 1, "", "bitwise_right_shift"], [51, 5, 1, "", "bitwise_xor"], [52, 5, 1, "", "cbrt"], [53, 5, 1, "", "ceil"], [54, 5, 1, "", "ceil_bw"], [55, 5, 1, "", "celu"], [56, 5, 1, "", "celu_bw"], [57, 5, 1, "", "clamp"], [58, 5, 1, "", "clamp_bw"], [59, 5, 1, "", "clip"], [60, 5, 1, "", "clip_bw"], [61, 5, 1, "", "clone"], [62, 1, 1, "", "close_device"], [63, 5, 1, "", "concat"], [64, 5, 1, "", "concat_bw"], [65, 5, 1, "", "conj"], [66, 5, 1, "", "conj_bw"], [67, 5, 1, "", "conv1d"], [68, 5, 1, "", "conv2d"], [69, 5, 1, "", "conv_transpose2d"], [70, 5, 1, "", "cos"], [71, 5, 1, "", "cos_bw"], [72, 5, 1, "", "cosh"], [73, 5, 1, "", "cosh_bw"], [74, 1, 1, "", "create_sharded_memory_config"], [75, 5, 1, "", "deallocate"], [76, 5, 1, "", "deg2rad"], [77, 5, 1, "", "deg2rad_bw"], [78, 5, 1, "", "digamma"], [79, 5, 1, "", "digamma_bw"], [80, 5, 1, "", "div"], [81, 5, 1, "", "div_bw"], [82, 5, 1, "", "div_no_nan"], [83, 5, 1, "", "div_no_nan_bw"], [84, 5, 1, "", "downsample"], [85, 5, 1, "", "dump_tensor"], [86, 5, 1, "", "elu"], [87, 5, 1, "", "elu_bw"], [88, 5, 1, "", "embedding"], [89, 5, 1, "", "embedding_bw"], [90, 5, 1, "", "empty"], [91, 5, 1, "", "empty_like"], [92, 5, 1, "", "eq"], [93, 5, 1, "", "eq_"], [94, 5, 1, "", "eqz"], [95, 5, 1, "", "erf"], [96, 5, 1, "", "erf_bw"], [97, 5, 1, "", "erfc"], [98, 5, 1, "", "erfc_bw"], [99, 5, 1, "", "erfinv"], [100, 5, 1, "", "erfinv_bw"], [101, 5, 1, "", "exp"], [102, 5, 1, "", "exp2"], [103, 5, 1, "", "exp2_bw"], [104, 5, 1, "", "exp_bw"], [113, 5, 1, "", "expm1"], [114, 5, 1, "", "expm1_bw"], [115, 5, 1, "", "fill"], [116, 5, 1, "", "fill_bw"], [117, 5, 1, "", "fill_ones_rm"], [118, 5, 1, "", "fill_rm"], [119, 5, 1, "", "fill_zero_bw"], [120, 5, 1, "", "floor"], [121, 5, 1, "", "floor_bw"], [122, 5, 1, "", "floor_div"], [123, 5, 1, "", "fmod"], [124, 5, 1, "", "fmod_bw"], [125, 1, 1, "", "format_input_tensor"], [126, 1, 1, "", "format_output_tensor"], [127, 5, 1, "", "frac"], [128, 5, 1, "", "frac_bw"], [129, 5, 1, "", "from_device"], [130, 5, 1, "", "from_torch"], [131, 5, 1, "", "full"], [132, 5, 1, "", "full_like"], [133, 5, 1, "", "gcd"], [134, 5, 1, "", "ge"], [135, 5, 1, "", "ge_"], [136, 5, 1, "", "geglu"], [137, 5, 1, "", "gelu"], [138, 5, 1, "", "gelu_bw"], [139, 5, 1, "", "gez"], [140, 5, 1, "", "global_avg_pool2d"], [141, 5, 1, "", "glu"], [142, 5, 1, "", "group_norm"], [143, 5, 1, "", "gt"], [144, 5, 1, "", "gt_"], [145, 5, 1, "", "gtz"], [146, 5, 1, "", "hardshrink"], [147, 5, 1, "", "hardshrink_bw"], [148, 5, 1, "", "hardsigmoid"], [149, 5, 1, "", "hardsigmoid_bw"], [150, 5, 1, "", "hardswish"], [151, 5, 1, "", "hardswish_bw"], [152, 5, 1, "", "hardtanh"], [153, 5, 1, "", "hardtanh_bw"], [154, 5, 1, "", "heaviside"], [155, 5, 1, "", "hypot"], [156, 5, 1, "", "hypot_bw"], [157, 5, 1, "", "i0"], [158, 5, 1, "", "i0_bw"], [159, 5, 1, "", "identity"], [160, 5, 1, "", "imag"], [161, 5, 1, "", "imag_bw"], [162, 5, 1, "", "indexed_fill"], [163, 5, 1, "", "is_imag"], [164, 5, 1, "", "is_real"], [165, 5, 1, "", "isclose"], [166, 5, 1, "", "isfinite"], [167, 5, 1, "", "isinf"], [168, 5, 1, "", "isnan"], [169, 5, 1, "", "isneginf"], [170, 5, 1, "", "isposinf"], [173, 5, 1, "", "l1_loss"], [174, 5, 1, "", "layer_norm"], [175, 5, 1, "", "lcm"], [176, 5, 1, "", "ldexp"], [177, 5, 1, "", "ldexp_bw"], [178, 5, 1, "", "le"], [179, 5, 1, "", "le_"], [180, 5, 1, "", "leaky_relu"], [181, 5, 1, "", "leaky_relu_bw"], [182, 5, 1, "", "lerp"], [183, 5, 1, "", "lerp_bw"], [184, 5, 1, "", "lez"], [185, 5, 1, "", "lgamma"], [186, 5, 1, "", "lgamma_bw"], [187, 5, 1, "", "linear"], [188, 5, 1, "", "load_tensor"], [189, 5, 1, "", "log"], [190, 5, 1, "", "log10"], [191, 5, 1, "", "log10_bw"], [192, 5, 1, "", "log1p"], [193, 5, 1, "", "log1p_bw"], [194, 5, 1, "", "log2"], [195, 5, 1, "", "log2_bw"], [196, 5, 1, "", "log_bw"], [197, 5, 1, "", "log_sigmoid"], [198, 5, 1, "", "log_sigmoid_bw"], [199, 5, 1, "", "logaddexp"], [200, 5, 1, "", "logaddexp2"], [201, 5, 1, "", "logaddexp2_bw"], [202, 5, 1, "", "logaddexp_bw"], [203, 5, 1, "", "logical_and"], [204, 5, 1, "", "logical_and_"], [205, 5, 1, "", "logical_not"], [206, 5, 1, "", "logical_not_"], [207, 5, 1, "", "logical_or"], [208, 5, 1, "", "logical_or_"], [209, 5, 1, "", "logical_xor"], [210, 5, 1, "", "logical_xor_"], [211, 5, 1, "", "logit"], [212, 5, 1, "", "logit_bw"], [213, 5, 1, "", "logiteps_bw"], [214, 5, 1, "", "lt"], [215, 5, 1, "", "lt_"], [216, 5, 1, "", "ltz"], [217, 5, 1, "", "mac"], [218, 1, 1, "", "manage_device"], [219, 5, 1, "", "matmul"], [220, 5, 1, "", "max"], [221, 5, 1, "", "max_bw"], [222, 5, 1, "", "max_pool2d"], [223, 5, 1, "", "maximum"], [224, 5, 1, "", "mean"], [225, 5, 1, "", "min"], [226, 5, 1, "", "min_bw"], [227, 5, 1, "", "minimum"], [228, 5, 1, "", "mish"], [231, 5, 1, "", "moreh_sum"], [232, 5, 1, "", "mse_loss"], [233, 5, 1, "", "mul_bw"], [234, 5, 1, "", "multigammaln"], [235, 5, 1, "", "multigammaln_bw"], [236, 5, 1, "", "multiply"], [237, 5, 1, "", "ne"], [238, 5, 1, "", "ne_"], [239, 5, 1, "", "neg"], [240, 5, 1, "", "neg_bw"], [241, 5, 1, "", "nextafter"], [242, 5, 1, "", "nez"], [243, 5, 1, "", "nonzero"], [244, 5, 1, "", "normalize_global"], [245, 5, 1, "", "normalize_hw"], [246, 5, 1, "", "ones"], [247, 5, 1, "", "ones_like"], [248, 1, 1, "", "open_device"], [249, 5, 1, "", "outer"], [250, 5, 1, "", "pad"], [251, 1, 1, "", "pad_to_tile_shape"], [252, 5, 1, "", "permute"], [253, 5, 1, "", "polar"], [254, 5, 1, "", "polar_bw"], [255, 5, 1, "", "polygamma"], [256, 5, 1, "", "polygamma_bw"], [257, 5, 1, "", "polyval"], [258, 5, 1, "", "pow"], [259, 5, 1, "", "pow_bw"], [260, 5, 1, "", "prelu"], [261, 1, 1, "", "prepare_conv_bias"], [262, 1, 1, "", "prepare_conv_transpose2d_bias"], [263, 1, 1, "", "prepare_conv_transpose2d_weights"], [264, 1, 1, "", "prepare_conv_weights"], [265, 5, 1, "", "prod"], [266, 5, 1, "", "prod_bw"], [267, 5, 1, "", "rad2deg"], [268, 5, 1, "", "rad2deg_bw"], [269, 5, 1, "", "rdiv"], [270, 5, 1, "", "rdiv_bw"], [271, 5, 1, "", "real"], [272, 5, 1, "", "real_bw"], [273, 5, 1, "", "reallocate"], [274, 5, 1, "", "reciprocal"], [275, 5, 1, "", "reciprocal_bw"], [276, 5, 1, "", "reduce_scatter"], [277, 1, 1, "", "register_post_operation_hook"], [278, 1, 1, "", "register_pre_operation_hook"], [279, 5, 1, "", "reglu"], [280, 5, 1, "", "relu"], [281, 5, 1, "", "relu6"], [282, 5, 1, "", "relu6_bw"], [283, 5, 1, "", "relu_bw"], [284, 5, 1, "", "relu_max"], [285, 5, 1, "", "relu_min"], [286, 5, 1, "", "remainder"], [287, 5, 1, "", "remainder_bw"], [288, 5, 1, "", "repeat"], [289, 5, 1, "", "repeat_bw"], [290, 5, 1, "", "repeat_interleave"], [291, 5, 1, "", "reshape"], [292, 5, 1, "", "rms_norm"], [293, 5, 1, "", "round"], [294, 5, 1, "", "round_bw"], [295, 5, 1, "", "rpow"], [296, 5, 1, "", "rpow_bw"], [297, 5, 1, "", "rsqrt"], [298, 5, 1, "", "rsqrt_bw"], [299, 5, 1, "", "rsub"], [300, 5, 1, "", "rsub_bw"], [301, 5, 1, "", "scatter"], [302, 5, 1, "", "selu"], [303, 5, 1, "", "selu_bw"], [304, 1, 1, "", "set_printoptions"], [305, 5, 1, "", "sigmoid"], [306, 5, 1, "", "sigmoid_accurate"], [307, 5, 1, "", "sigmoid_bw"], [308, 5, 1, "", "sign"], [309, 5, 1, "", "sign_bw"], [310, 5, 1, "", "signbit"], [311, 5, 1, "", "silu"], [312, 5, 1, "", "silu_bw"], [313, 5, 1, "", "sin"], [314, 5, 1, "", "sin_bw"], [315, 5, 1, "", "sinh"], [316, 5, 1, "", "sinh_bw"], [317, 5, 1, "", "slice"], [318, 5, 1, "", "softmax"], [319, 5, 1, "", "softplus"], [320, 5, 1, "", "softplus_bw"], [321, 5, 1, "", "softshrink"], [322, 5, 1, "", "softshrink_bw"], [323, 5, 1, "", "softsign"], [324, 5, 1, "", "softsign_bw"], [325, 5, 1, "", "sqrt"], [326, 5, 1, "", "sqrt_bw"], [327, 5, 1, "", "square"], [328, 5, 1, "", "square_bw"], [329, 5, 1, "", "squared_difference"], [330, 5, 1, "", "squared_difference_bw"], [331, 5, 1, "", "std"], [332, 5, 1, "", "sub_bw"], [333, 5, 1, "", "subalpha"], [334, 5, 1, "", "subalpha_bw"], [335, 5, 1, "", "subtract"], [336, 5, 1, "", "sum"], [337, 5, 1, "", "swiglu"], [338, 5, 1, "", "swish"], [339, 1, 1, "", "synchronize_device"], [340, 5, 1, "", "tan"], [341, 5, 1, "", "tan_bw"], [342, 5, 1, "", "tanh"], [343, 5, 1, "", "tanh_bw"], [344, 5, 1, "", "tanhshrink"], [345, 5, 1, "", "tanhshrink_bw"], [346, 5, 1, "", "threshold"], [347, 5, 1, "", "threshold_bw"], [348, 5, 1, "", "tilize"], [349, 5, 1, "", "tilize_with_val_padding"], [350, 5, 1, "", "to_device"], [351, 5, 1, "", "to_layout"], [352, 5, 1, "", "to_memory_config"], [353, 5, 1, "", "to_torch"], [354, 5, 1, "", "topk"], [361, 5, 1, "", "tril"], [362, 5, 1, "", "triu"], [363, 5, 1, "", "trunc"], [364, 5, 1, "", "trunc_bw"], [365, 5, 1, "", "unary_chain"], [366, 5, 1, "", "untilize"], [367, 5, 1, "", "untilize_with_unpadding"], [368, 5, 1, "", "upsample"], [369, 5, 1, "", "var"], [370, 5, 1, "", "where"], [371, 5, 1, "", "where_bw"], [372, 5, 1, "", "xlogy"], [373, 5, 1, "", "xlogy_bw"], [374, 5, 1, "", "zeros"], [375, 5, 1, "", "zeros_like"]], "ttnn.Conv2dConfig": [[8, 2, 1, "", "act_block_h_override"], [8, 2, 1, "", "act_block_w_div"], [8, 2, 1, "", "activation"], [8, 2, 1, "", "always_preprocess_weights"], [8, 2, 1, "", "core_grid"], [8, 2, 1, "", "deallocate_activation"], [8, 2, 1, "", "dtype"], [8, 2, 1, "", "enable_act_double_buffer"], [8, 2, 1, "", "enable_split_reader"], [8, 2, 1, "", "enable_subblock_padding"], [8, 2, 1, "", "enable_weights_double_buffer"], [8, 2, 1, "", "in_place"], [8, 2, 1, "", "output_layout"], [8, 2, 1, "", "override_sharding_config"], [8, 2, 1, "", "preprocess_weights_on_device"], [8, 2, 1, "", "reallocate_halo_output"], [8, 2, 1, "", "reshard_if_not_optimal"], [8, 2, 1, "", "shard_layout"], [8, 2, 1, "", "transpose_shards"], [8, 2, 1, "", "weights_dtype"]], "ttnn.Conv2dSliceConfig": [[9, 0, 1, "", "SliceTypeEnum"], [9, 2, 1, "", "num_slices"], [9, 2, 1, "", "slice_type"]], "ttnn.Conv2dSliceConfig.SliceTypeEnum": [[9, 3, 1, "", "SliceHeight"], [9, 3, 1, "", "SliceWidth"], [9, 2, 1, "", "name"], [9, 2, 1, "", "value"]], "ttnn.MemoryConfig": [[380, 4, 1, "", "__init__"]], "ttnn.Shape": [[386, 2, 1, "", "rank"], [386, 4, 1, "", "to_rank"]], "ttnn.Tensor": [[380, 4, 1, "", "__init__"], [380, 4, 1, "", "buffer"], [381, 1, 1, "", "cpu"], [380, 4, 1, "", "device"], [380, 4, 1, "", "get_dtype"], [380, 4, 1, "", "get_layout"], [380, 4, 1, "", "pad"], [380, 4, 1, "", "pad_to_tile"], [380, 4, 1, "", "storage_type"], [380, 4, 1, "", "to"], [380, 4, 1, "", "unpad"], [380, 4, 1, "", "unpad_from_tile"]], "ttnn.experimental": [[105, 5, 1, "", "all_reduce"], [106, 5, 1, "", "conv3d"], [107, 5, 1, "", "cumprod"], [108, 5, 1, "", "cumsum"], [109, 5, 1, "", "dropout"], [110, 5, 1, "", "gelu_bw"], [111, 5, 1, "", "rotary_embedding"], [112, 5, 1, "", "sort"]], "ttnn.kv_cache": [[171, 5, 1, "", "fill_cache_for_user_"], [172, 5, 1, "", "update_cache_for_token_"]], "ttnn.model_preprocessing": [[229, 1, 1, "", "preprocess_model"], [230, 1, 1, "", "preprocess_model_parameters"]], "ttnn.operations.moreh": [[381, 1, 1, "", "group_norm"], [381, 1, 1, "", "group_norm_backward"], [381, 1, 1, "", "logsoftmax"], [381, 1, 1, "", "logsoftmax_backward"], [381, 1, 1, "", "mean"], [381, 1, 1, "", "mean_backward"], [381, 1, 1, "", "norm"], [381, 1, 1, "", "norm_backward"], [381, 1, 1, "", "softmax"], [381, 1, 1, "", "softmax_backward"], [381, 1, 1, "", "softmin"], [381, 1, 1, "", "softmin_backward"]], "ttnn.transformer": [[355, 5, 1, "", "attention_softmax"], [356, 5, 1, "", "attention_softmax_"], [357, 5, 1, "", "concatenate_heads"], [358, 5, 1, "", "scaled_dot_product_attention"], [359, 5, 1, "", "scaled_dot_product_attention_decode"], [360, 5, 1, "", "split_query_key_value_and_split_heads"]]}, "objtypes": {"0": "py:class", "1": "py:function", "2": "py:property", "3": "py:attribute", "4": "py:method", "5": "py:data"}, "objnames": {"0": ["py", "class", "Python class"], "1": ["py", "function", "Python function"], "2": ["py", "property", "Python property"], "3": ["py", "attribute", "Python attribute"], "4": ["py", "method", "Python method"], "5": ["py", "data", "Python data"]}, "titleterms": {"welcom": 0, "tt": [0, 5, 6, 376, 378, 380, 381, 382, 383, 385, 402], "nn": [0, 5, 6, 376, 382, 383, 385, 402], "document": 0, "ttnn": [0, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 388, 391, 394, 395, 397, 398, 399, 400], "model": [0, 4, 7, 376, 382, 383, 397, 398, 401], "resourc": 0, "indic": 0, "tabl": 0, "contribut": [1, 383], "develop": 1, "support": [2, 402], "report": [2, 7, 385], "bug": 2, "featur": 2, "propos": 2, "request": 2, "troubleshoot": 2, "debug": [2, 402], "tip": 2, "commun": 2, "perform": [3, 396], "prerequisit": [3, 4, 383], "run": [3, 4, 378, 397, 400, 402], "perf": [3, 385], "file": 3, "all": [3, 383, 402], "get": [4, 382, 400], "start": [4, 382], "an": [4, 402], "exampl": [4, 6, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 62, 63, 64, 65, 66, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 107, 109, 110, 113, 114, 115, 116, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 173, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 253, 254, 255, 256, 257, 258, 259, 260, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 291, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 340, 341, 342, 343, 344, 345, 346, 347, 351, 352, 353, 361, 362, 363, 364, 365, 369, 370, 371, 372, 373, 374, 375, 376, 378, 380, 383, 402], "next": 4, "step": [4, 6, 376, 383], "what": [5, 6], "i": [5, 6], "ad": 6, "new": [6, 381, 384], "oper": [6, 7, 376, 381, 385, 389, 393, 395, 398, 399, 402], "faq": 6, "ar": [6, 383], "need": 6, "add": [6, 18, 393, 395], "c": [6, 402], "python": [6, 402], "devic": [6, 7, 378, 381, 383, 395, 396, 397, 402], "implement": [6, 397, 400], "1": [6, 376, 382, 383, 402], "2": [6, 376, 381, 382, 383, 401, 402], "bind": 6, "option": [6, 381, 383], "golden": 6, "function": [6, 384, 398, 402], "api": [7, 380, 381, 386], "memori": [7, 386], "config": [7, 386, 396], "core": 7, "tensor": [7, 378, 380, 381, 386, 393, 395, 396, 402], "creation": 7, "matrix": [7, 396], "multipl": [7, 386, 396], "pointwis": 7, "unari": 7, "binari": 7, "ternari": 7, "loss": 7, "reduct": 7, "data": [7, 386, 395], "movement": 7, "normal": 7, "moreh": 7, "transform": [7, 355, 356, 357, 358, 359, 360], "ccl": 7, "embed": [7, 88], "convolut": 7, "pool": 7, "vision": 7, "kv": 7, "cach": [7, 381, 396, 397, 402], "convers": 7, "hook": [7, 402], "conv2dconfig": 8, "conv2dsliceconfig": 9, "getdefaultdevic": 10, "setdefaultdevic": 11, "ab": 12, "abs_bw": 13, "aco": 14, "acos_bw": 15, "acosh": 16, "acosh_bw": 17, "add_bw": 19, "addalpha": 20, "addalpha_bw": 21, "addcdiv": 22, "addcdiv_bw": 23, "addcmul": 24, "addcmul_bw": 25, "all_gath": 26, "alt_complex_rotate90": 27, "angl": 28, "angle_bw": 29, "arang": 30, "argmax": 31, "as_tensor": 32, "asin": 33, "asin_bw": 34, "asinh": 35, "asinh_bw": 36, "assign_bw": 37, "atan": 38, "atan2": 39, "atan2_bw": 40, "atan_bw": 41, "atanh": 42, "atanh_bw": 43, "batch_norm": 44, "bias_gelu_bw": 45, "bitwise_and": 46, "bitwise_left_shift": 47, "bitwise_not": 48, "bitwise_or": 49, "bitwise_right_shift": 50, "bitwise_xor": 51, "cbrt": 52, "ceil": 53, "ceil_bw": 54, "celu": 55, "celu_bw": 56, "clamp": 57, "clamp_bw": 58, "clip": 59, "clip_bw": 60, "clone": [61, 383, 401], "close_devic": 62, "concat": 63, "concat_bw": 64, "conj": 65, "conj_bw": 66, "conv1d": 67, "conv2d": 68, "conv_transpose2d": 69, "co": 70, "cos_bw": 71, "cosh": 72, "cosh_bw": 73, "create_sharded_memory_config": 74, "dealloc": 75, "deg2rad": 76, "deg2rad_bw": 77, "digamma": 78, "digamma_bw": 79, "div": 80, "div_bw": 81, "div_no_nan": 82, "div_no_nan_bw": 83, "downsampl": 84, "dump_tensor": 85, "elu": 86, "elu_bw": 87, "embedding_bw": 89, "empti": 90, "empty_lik": 91, "eq": 92, "eq_": 93, "eqz": 94, "erf": 95, "erf_bw": 96, "erfc": 97, "erfc_bw": 98, "erfinv": 99, "erfinv_bw": 100, "exp": 101, "exp2": 102, "exp2_bw": 103, "exp_bw": 104, "experiment": [105, 106, 107, 108, 109, 110, 111, 112, 381], "all_reduc": 105, "conv3d": 106, "cumprod": 107, "cumsum": 108, "dropout": 109, "gelu_bw": [110, 138], "rotary_embed": 111, "sort": 112, "expm1": 113, "expm1_bw": 114, "fill": 115, "fill_bw": 116, "fill_ones_rm": 117, "fill_rm": 118, "fill_zero_bw": 119, "floor": 120, "floor_bw": 121, "floor_div": 122, "fmod": 123, "fmod_bw": 124, "format_input_tensor": 125, "format_output_tensor": 126, "frac": 127, "frac_bw": 128, "from_devic": 129, "from_torch": 130, "full": 131, "full_lik": 132, "gcd": 133, "ge": 134, "ge_": 135, "geglu": 136, "gelu": 137, "gez": 139, "global_avg_pool2d": 140, "glu": 141, "group_norm": 142, "gt": 143, "gt_": 144, "gtz": 145, "hardshrink": 146, "hardshrink_bw": 147, "hardsigmoid": 148, "hardsigmoid_bw": 149, "hardswish": 150, "hardswish_bw": 151, "hardtanh": 152, "hardtanh_bw": 153, "heavisid": 154, "hypot": 155, "hypot_bw": 156, "i0": 157, "i0_bw": 158, "ident": 159, "imag": [160, 383], "imag_bw": 161, "indexed_fil": 162, "is_imag": 163, "is_real": 164, "isclos": 165, "isfinit": 166, "isinf": 167, "isnan": 168, "isneginf": 169, "isposinf": 170, "kv_cach": [171, 172], "fill_cache_for_user_": 171, "update_cache_for_token_": 172, "l1_loss": 173, "layer_norm": 174, "lcm": 175, "ldexp": 176, "ldexp_bw": 177, "le": 178, "le_": 179, "leaky_relu": 180, "leaky_relu_bw": 181, "lerp": 182, "lerp_bw": 183, "lez": 184, "lgamma": 185, "lgamma_bw": 186, "linear": 187, "load_tensor": 188, "log": [189, 381, 402], "log10": 190, "log10_bw": 191, "log1p": 192, "log1p_bw": 193, "log2": 194, "log2_bw": 195, "log_bw": 196, "log_sigmoid": 197, "log_sigmoid_bw": 198, "logaddexp": 199, "logaddexp2": 200, "logaddexp2_bw": 201, "logaddexp_bw": 202, "logical_and": 203, "logical_and_": 204, "logical_not": 205, "logical_not_": 206, "logical_or": 207, "logical_or_": 208, "logical_xor": 209, "logical_xor_": 210, "logit": 211, "logit_bw": 212, "logiteps_bw": 213, "lt": 214, "lt_": 215, "ltz": 216, "mac": 217, "manage_devic": 218, "matmul": [219, 389], "max": 220, "max_bw": 221, "max_pool2d": 222, "maximum": 223, "mean": 224, "min": 225, "min_bw": 226, "minimum": 227, "mish": 228, "model_preprocess": [229, 230], "preprocess_model": 229, "preprocess_model_paramet": 230, "moreh_sum": 231, "mse_loss": 232, "mul_bw": 233, "multigammaln": 234, "multigammaln_bw": 235, "multipli": [236, 396], "ne": 237, "ne_": 238, "neg": 239, "neg_bw": 240, "nextaft": 241, "nez": 242, "nonzero": 243, "normalize_glob": 244, "normalize_hw": 245, "ones": 246, "ones_lik": 247, "open_devic": 248, "outer": 249, "pad": 250, "pad_to_tile_shap": 251, "permut": 252, "polar": 253, "polar_bw": 254, "polygamma": 255, "polygamma_bw": 256, "polyv": 257, "pow": 258, "pow_bw": 259, "prelu": 260, "prepare_conv_bia": 261, "prepare_conv_transpose2d_bia": 262, "prepare_conv_transpose2d_weight": 263, "prepare_conv_weight": 264, "prod": 265, "prod_bw": 266, "rad2deg": 267, "rad2deg_bw": 268, "rdiv": 269, "rdiv_bw": 270, "real": 271, "real_bw": 272, "realloc": 273, "reciproc": 274, "reciprocal_bw": 275, "reduce_scatt": 276, "register_post_operation_hook": 277, "register_pre_operation_hook": 278, "reglu": 279, "relu": 280, "relu6": 281, "relu6_bw": 282, "relu_bw": 283, "relu_max": 284, "relu_min": 285, "remaind": 286, "remainder_bw": 287, "repeat": 288, "repeat_bw": 289, "repeat_interleav": 290, "reshap": 291, "rms_norm": 292, "round": 293, "round_bw": 294, "rpow": 295, "rpow_bw": 296, "rsqrt": 297, "rsqrt_bw": 298, "rsub": 299, "rsub_bw": 300, "scatter": 301, "selu": 302, "selu_bw": 303, "set_printopt": 304, "sigmoid": 305, "sigmoid_accur": 306, "sigmoid_bw": 307, "sign": 308, "sign_bw": 309, "signbit": 310, "silu": 311, "silu_bw": 312, "sin": 313, "sin_bw": 314, "sinh": 315, "sinh_bw": 316, "slice": [317, 402], "softmax": 318, "softplu": 319, "softplus_bw": 320, "softshrink": 321, "softshrink_bw": 322, "softsign": 323, "softsign_bw": 324, "sqrt": 325, "sqrt_bw": 326, "squar": 327, "square_bw": 328, "squared_differ": 329, "squared_difference_bw": 330, "std": 331, "sub_bw": 332, "subalpha": 333, "subalpha_bw": 334, "subtract": 335, "sum": 336, "swiglu": 337, "swish": 338, "synchronize_devic": 339, "tan": 340, "tan_bw": 341, "tanh": 342, "tanh_bw": 343, "tanhshrink": 344, "tanhshrink_bw": 345, "threshold": 346, "threshold_bw": 347, "tiliz": 348, "tilize_with_val_pad": 349, "to_devic": 350, "to_layout": 351, "to_memory_config": 352, "to_torch": 353, "topk": 354, "attention_softmax": 355, "attention_softmax_": 356, "concatenate_head": 357, "scaled_dot_product_attent": 358, "scaled_dot_product_attention_decod": 359, "split_query_key_value_and_split_head": 360, "tril": 361, "triu": 362, "trunc": 363, "trunc_bw": 364, "unary_chain": 365, "until": 366, "untilize_with_unpad": 367, "upsampl": 368, "var": 369, "where": [370, 382], "where_bw": 371, "xlogi": 372, "xlogy_bw": 373, "zero": 374, "zeros_lik": 375, "convert": [376, 380, 395, 397, 402], "pytorch": [376, 378, 380, 401], "rewrit": 376, "switch": 376, "3": [376, 382, 383, 402], "optim": [376, 382, 397], "more": [376, 396], "build": [377, 382, 383, 401], "uplift": 377, "demo": [377, 382], "lib": [378, 381], "us": [378, 395, 396, 397, 398, 402], "one": 378, "op": 378, "from": [378, 381, 382, 383, 400, 401, 402], "acceler": 378, "odd": 378, "size": 378, "last": 378, "dim": 378, "depend": [379, 383], "overview": [380, 381], "storag": [380, 386, 395], "memoryconfig": 380, "between": 380, "infrastructur": 381, "member": 381, "input": 381, "output": [381, 395, 396, 397], "profil": [381, 385, 391, 399], "fast": 381, "dispatch": 381, "program": [381, 383, 396, 397, 402], "through": 381, "tt_lib": [381, 402], "primari": 381, "enum": 381, "fallback": 381, "fuse": 381, "mini": 381, "graph": [381, 388, 400, 401, 402], "librari": [381, 401], "complex": 381, "type": [381, 386, 395], "instal": [382, 383], "explor": 382, "our": [382, 383], "tutori": [382, 387], "multi": [382, 383, 390, 397], "head": [382, 390, 397], "attent": [382, 390, 397], "simpl": 382, "4": [382, 402], "To": [382, 383], "go": 382, "here": 382, "set": 383, "up": 383, "hardwar": 383, "driver": 383, "firmwar": 383, "system": 383, "level": 383, "kmd": 383, "updat": 383, "flash": 383, "manag": 383, "interfac": 383, "smi": 383, "card": 383, "configur": [383, 396, 397], "topologi": 383, "metalium": 383, "There": 383, "three": 383, "sourc": 383, "repositori": 383, "invok": 383, "script": 383, "docker": 383, "releas": 383, "wheel": 383, "download": [383, 401], "latest": 383, "For": 383, "user": 383, "onli": 383, "environ": 383, "you": 383, "verifi": 383, "your": 383, "try": 383, "execut": 383, "interest": 383, "onboard": 384, "header": 385, "profile_thi": 385, "descript": 385, "shape": 386, "layout": [386, 395, 396], "requir": 386, "width": 386, "limit": 386, "bfloat8_b": 386, "shard": 386, "torch": [388, 395, 396, 397, 398, 400, 402], "dit_xl_2": 388, "With": 388, "resnet": [392, 400], "basic": [392, 402], "block": [392, 400], "tracer": 394, "creat": [395, 400], "host": 395, "borrow": 395, "v": 395, "own": 395, "open": 395, "initi": [395, 396, 397], "b": [395, 396], "random": [395, 396], "valu": [395, 396], "inspect": [395, 396], "attribut": 395, "close": [395, 396, 397], "enabl": [396, 397, 402], "result": 396, "write": 397, "activ": 397, "weight": 397, "first": 397, "iter": 397, "subsequ": 397, "version": [397, 400], "pre": [397, 401, 402], "process": 397, "paramet": [397, 400], "check": 397, "match": 397, "origin": 397, "trace": [398, 400, 402], "modul": [398, 400], "written": 398, "torchvis": 400, "preprocess": 400, "displai": [400, 401], "pass": 400, "constructor": 400, "base": 401, "http": 401, "github": 401, "com": 401, "facebookresearch": 401, "dit": 401, "git": 401, "xl": 401, "sampl": 401, "train": 401, "__getitem__": 402, "5": 402, "intermedi": 402, "6": 402, "7": 402, "8": 402, "9": 402, "10": 402, "chang": 402, "string": 402, "represent": 402, "11": 402, "visual": 402, "web": 402, "browser": 402, "12": 402, "regist": 402, "post": 402, "13": 402, "queri": 402, "14": 402, "fall": 402, "back": 402, "15": 402, "captur": 402, "buffer": 402, "alloc": 402, "etc": 402}, "envversion": {"sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "nbsphinx": 4, "sphinx": 58}, "alltitles": {"Welcome to TT-NN documentation!": [[0, "welcome-to-tt-nn-documentation"]], "TTNN": [[0, null]], "Models": [[0, null]], "Resources": [[0, null]], "Indices and tables": [[0, "indices-and-tables"]], "Contributing as a developer": [[1, "contributing-as-a-developer"]], "Support": [[2, "support"]], "Reporting bugs, feature proposals, or support requests": [[2, "reporting-bugs-feature-proposals-or-support-requests"]], "Troubleshooting and debugging tips": [[2, "troubleshooting-and-debugging-tips"]], "Community": [[2, "community"]], "Performance": [[3, "performance"]], "Prerequisites": [[3, "prerequisites"], [4, "prerequisites"]], "Running a perf file": [[3, "running-a-perf-file"]], "Running all the perf files": [[3, "running-all-the-perf-files"]], "Getting Started": [[4, "getting-started"], [382, "getting-started"]], "Running an example model": [[4, "running-an-example-model"]], "Next steps": [[4, "next-steps"]], "What is TT-NN?": [[5, "what-is-tt-nn"]], "Adding New TT-NN Operation": [[6, "adding-new-tt-nn-operation"]], "FAQ": [[6, "faq"]], "What is a TT-NN operation?": [[6, "what-is-a-tt-nn-operation"]], "What steps are needed to add TT-NN operation in C++?": [[6, "what-steps-are-needed-to-add-tt-nn-operation-in-c"]], "What steps are needed to add TT-NN operation in Python?": [[6, "what-steps-are-needed-to-add-tt-nn-operation-in-python"]], "Example of Adding a new Device Operation": [[6, "example-of-adding-a-new-device-operation"]], "C++ Implementation": [[6, "c-implementation"]], "Step 1: Implement device operation": [[6, "step-1-implement-device-operation"]], "Step 2: Implement the operation in C++": [[6, "step-2-implement-the-operation-in-c"]], "Python Implementation": [[6, "python-implementation"]], "Step 1: Add Python binding": [[6, "step-1-add-python-binding"]], "Step 2: (Optional) Add golden function for the operation in Python": [[6, "step-2-optional-add-golden-function-for-the-operation-in-python"]], "APIs": [[7, "apis"], [386, "apis"]], "Device": [[7, "device"]], "Memory Config": [[7, "memory-config"], [386, "memory-config"]], "Operations": [[7, "operations"]], "Core": [[7, "core"]], "Tensor Creation": [[7, "tensor-creation"]], "Matrix Multiplication": [[7, "matrix-multiplication"], [396, "Matrix-Multiplication"]], "Pointwise Unary": [[7, "pointwise-unary"]], "Pointwise Binary": [[7, "pointwise-binary"]], "Pointwise Ternary": [[7, "pointwise-ternary"]], "Losses": [[7, "losses"]], "Reduction": [[7, "reduction"]], "Data Movement": [[7, "data-movement"]], "Normalization": [[7, "normalization"]], "Moreh Operations": [[7, "moreh-operations"]], "Transformer": [[7, "transformer"]], "CCL": [[7, "ccl"]], "Embedding": [[7, "embedding"]], "Convolution": [[7, "convolution"]], "Pooling": [[7, "pooling"]], "Vision": [[7, "vision"]], "KV Cache": [[7, "kv-cache"]], "Model Conversion": [[7, "model-conversion"]], "Reports": [[7, "reports"]], "Operation Hooks": [[7, "operation-hooks"]], "ttnn.Conv2dConfig": [[8, "ttnn-conv2dconfig"]], "ttnn.Conv2dSliceConfig": [[9, "ttnn-conv2dsliceconfig"]], "ttnn.GetDefaultDevice": [[10, "ttnn-getdefaultdevice"]], "Example": [[10, null], [11, null], [12, null], [13, null], [14, null], [15, null], [16, null], [17, null], [18, null], [19, null], [20, null], [21, null], [22, null], [23, null], [24, null], [25, null], [26, null], [27, null], [28, null], [29, null], [30, null], [33, null], [34, null], [35, null], [36, null], [37, null], [38, null], [39, null], [40, null], [41, null], [42, null], [43, null], [44, null], [45, null], [46, null], [47, null], [48, null], [49, null], [50, null], [51, null], [52, null], [53, null], [54, null], [55, null], [56, null], [57, null], [58, null], [59, null], [60, null], [62, null], [63, null], [64, null], [65, null], [66, null], [70, null], [71, null], [72, null], [73, null], [74, null], [75, null], [76, null], [77, null], [78, null], [79, null], [80, null], [81, null], [82, null], [83, null], [85, null], [86, null], [87, null], [88, null], [89, null], [90, null], [91, null], [92, null], [93, null], [94, null], [95, null], [96, null], [97, null], [98, null], [99, null], [100, null], [101, null], [102, null], [103, null], [104, null], [105, null], [107, null], [109, null], [110, null], [113, null], [114, null], [115, null], [116, null], [119, null], [120, null], [121, null], [122, null], [123, null], [124, null], [125, null], [126, null], [127, null], [128, null], [130, null], [131, null], [132, null], [133, null], [134, null], [135, null], [136, null], [137, null], [138, null], [139, null], [140, null], [141, null], [143, null], [144, null], [145, null], [146, null], [147, null], [148, null], [149, null], [150, null], [151, null], [152, null], [153, null], [154, null], [155, null], [156, null], [157, null], [158, null], [159, null], [160, null], [161, null], [162, null], [163, null], [164, null], [165, null], [166, null], [167, null], [168, null], [169, null], [170, null], [173, null], [175, null], [176, null], [177, null], [178, null], [179, null], [180, null], [181, null], [182, null], [183, null], [184, null], [185, null], [186, null], [187, null], [188, null], [189, null], [190, null], [191, null], [192, null], [193, null], [194, null], [195, null], [196, null], [197, null], [198, null], [199, null], [200, null], [201, null], [202, null], [203, null], [204, null], [205, null], [206, null], [207, null], [208, null], [209, null], [210, null], [211, null], [212, null], [213, null], [214, null], [215, null], [216, null], [217, null], [218, null], [219, null], [220, null], [221, null], [222, null], [223, null], [224, null], [225, null], [226, null], [227, null], [228, null], [232, null], [233, null], [234, null], [235, null], [236, null], [237, null], [238, null], [239, null], [240, null], [241, null], [242, null], [243, null], [244, null], [245, null], [246, null], [247, null], [248, null], [249, null], [250, null], [251, null], [253, null], [254, null], [255, null], [256, null], [257, null], [258, null], [259, null], [260, null], [266, null], [267, null], [268, null], [269, null], [270, null], [271, null], [272, null], [273, null], [274, null], [275, null], [276, null], [279, null], [280, null], [281, null], [282, null], [283, null], [284, null], [285, null], [286, null], [287, null], [288, null], [289, null], [291, null], [293, null], [294, null], [295, null], [296, null], [297, null], [298, null], [299, null], [300, null], [301, null], [302, null], [303, null], [305, null], [306, null], [307, null], [308, null], [309, null], [310, null], [311, null], [312, null], [313, null], [314, null], [315, null], [316, null], [317, null], [318, null], [319, null], [320, null], [321, null], [322, null], [323, null], [324, null], [325, null], [326, null], [327, null], [328, null], [329, null], [330, null], [331, null], [332, null], [333, null], [334, null], [335, null], [336, null], [337, null], [338, null], [340, null], [341, null], [342, null], [343, null], [344, null], [345, null], [346, null], [347, null], [351, null], [352, null], [353, null], [361, null], [362, null], [363, null], [364, null], [365, null], [369, null], [370, null], [371, null], [372, null], [373, null], [374, null], [375, null]], "ttnn.SetDefaultDevice": [[11, "ttnn-setdefaultdevice"]], "ttnn.abs": [[12, "ttnn-abs"]], "ttnn.abs_bw": [[13, "ttnn-abs-bw"]], "ttnn.acos": [[14, "ttnn-acos"]], "ttnn.acos_bw": [[15, "ttnn-acos-bw"]], "ttnn.acosh": [[16, "ttnn-acosh"]], "ttnn.acosh_bw": [[17, "ttnn-acosh-bw"]], "ttnn.add": [[18, "ttnn-add"]], "ttnn.add_bw": [[19, "ttnn-add-bw"]], "ttnn.addalpha": [[20, "ttnn-addalpha"]], "ttnn.addalpha_bw": [[21, "ttnn-addalpha-bw"]], "ttnn.addcdiv": [[22, "ttnn-addcdiv"]], "ttnn.addcdiv_bw": [[23, "ttnn-addcdiv-bw"]], "ttnn.addcmul": [[24, "ttnn-addcmul"]], "ttnn.addcmul_bw": [[25, "ttnn-addcmul-bw"]], "ttnn.all_gather": [[26, "ttnn-all-gather"]], "ttnn.alt_complex_rotate90": [[27, "ttnn-alt-complex-rotate90"]], "ttnn.angle": [[28, "ttnn-angle"]], "ttnn.angle_bw": [[29, "ttnn-angle-bw"]], "ttnn.arange": [[30, "ttnn-arange"]], "ttnn.argmax": [[31, "ttnn-argmax"]], "ttnn.as_tensor": [[32, "ttnn-as-tensor"]], "Examples": [[32, null], [304, null]], "ttnn.asin": [[33, "ttnn-asin"]], "ttnn.asin_bw": [[34, "ttnn-asin-bw"]], "ttnn.asinh": [[35, "ttnn-asinh"]], "ttnn.asinh_bw": [[36, "ttnn-asinh-bw"]], "ttnn.assign_bw": [[37, "ttnn-assign-bw"]], "ttnn.atan": [[38, "ttnn-atan"]], "ttnn.atan2": [[39, "ttnn-atan2"]], "ttnn.atan2_bw": [[40, "ttnn-atan2-bw"]], "ttnn.atan_bw": [[41, "ttnn-atan-bw"]], "ttnn.atanh": [[42, "ttnn-atanh"]], "ttnn.atanh_bw": [[43, "ttnn-atanh-bw"]], "ttnn.batch_norm": [[44, "ttnn-batch-norm"]], "ttnn.bias_gelu_bw": [[45, "ttnn-bias-gelu-bw"]], "ttnn.bitwise_and": [[46, "ttnn-bitwise-and"]], "ttnn.bitwise_left_shift": [[47, "ttnn-bitwise-left-shift"]], "ttnn.bitwise_not": [[48, "ttnn-bitwise-not"]], "ttnn.bitwise_or": [[49, "ttnn-bitwise-or"]], "ttnn.bitwise_right_shift": [[50, "ttnn-bitwise-right-shift"]], "ttnn.bitwise_xor": [[51, "ttnn-bitwise-xor"]], "ttnn.cbrt": [[52, "ttnn-cbrt"]], "ttnn.ceil": [[53, "ttnn-ceil"]], "ttnn.ceil_bw": [[54, "ttnn-ceil-bw"]], "ttnn.celu": [[55, "ttnn-celu"]], "ttnn.celu_bw": [[56, "ttnn-celu-bw"]], "ttnn.clamp": [[57, "ttnn-clamp"]], "ttnn.clamp_bw": [[58, "ttnn-clamp-bw"]], "ttnn.clip": [[59, "ttnn-clip"]], "ttnn.clip_bw": [[60, "ttnn-clip-bw"]], "ttnn.clone": [[61, "ttnn-clone"]], "ttnn.close_device": [[62, "ttnn-close-device"]], "ttnn.concat": [[63, "ttnn-concat"]], "ttnn.concat_bw": [[64, "ttnn-concat-bw"]], "ttnn.conj": [[65, "ttnn-conj"]], "ttnn.conj_bw": [[66, "ttnn-conj-bw"]], "ttnn.conv1d": [[67, "ttnn-conv1d"]], "ttnn.conv2d": [[68, "ttnn-conv2d"]], "ttnn.conv_transpose2d": [[69, "ttnn-conv-transpose2d"]], "ttnn.cos": [[70, "ttnn-cos"]], "ttnn.cos_bw": [[71, "ttnn-cos-bw"]], "ttnn.cosh": [[72, "ttnn-cosh"]], "ttnn.cosh_bw": [[73, "ttnn-cosh-bw"]], "ttnn.create_sharded_memory_config": [[74, "ttnn-create-sharded-memory-config"]], "ttnn.deallocate": [[75, "ttnn-deallocate"]], "ttnn.deg2rad": [[76, "ttnn-deg2rad"]], "ttnn.deg2rad_bw": [[77, "ttnn-deg2rad-bw"]], "ttnn.digamma": [[78, "ttnn-digamma"]], "ttnn.digamma_bw": [[79, "ttnn-digamma-bw"]], "ttnn.div": [[80, "ttnn-div"]], "ttnn.div_bw": [[81, "ttnn-div-bw"]], "ttnn.div_no_nan": [[82, "ttnn-div-no-nan"]], "ttnn.div_no_nan_bw": [[83, "ttnn-div-no-nan-bw"]], "ttnn.downsample": [[84, "ttnn-downsample"]], "ttnn.dump_tensor": [[85, "ttnn-dump-tensor"]], "ttnn.elu": [[86, "ttnn-elu"]], "ttnn.elu_bw": [[87, "ttnn-elu-bw"]], "ttnn.embedding": [[88, "ttnn-embedding"]], "ttnn.embedding_bw": [[89, "ttnn-embedding-bw"]], "ttnn.empty": [[90, "ttnn-empty"]], "ttnn.empty_like": [[91, "ttnn-empty-like"]], "ttnn.eq": [[92, "ttnn-eq"]], "ttnn.eq_": [[93, "ttnn-eq"]], "ttnn.eqz": [[94, "ttnn-eqz"]], "ttnn.erf": [[95, "ttnn-erf"]], "ttnn.erf_bw": [[96, "ttnn-erf-bw"]], "ttnn.erfc": [[97, "ttnn-erfc"]], "ttnn.erfc_bw": [[98, "ttnn-erfc-bw"]], "ttnn.erfinv": [[99, "ttnn-erfinv"]], "ttnn.erfinv_bw": [[100, "ttnn-erfinv-bw"]], "ttnn.exp": [[101, "ttnn-exp"]], "ttnn.exp2": [[102, "ttnn-exp2"]], "ttnn.exp2_bw": [[103, "ttnn-exp2-bw"]], "ttnn.exp_bw": [[104, "ttnn-exp-bw"]], "ttnn.experimental.all_reduce": [[105, "ttnn-experimental-all-reduce"]], "ttnn.experimental.conv3d": [[106, "ttnn-experimental-conv3d"]], "ttnn.experimental.cumprod": [[107, "ttnn-experimental-cumprod"]], "ttnn.experimental.cumsum": [[108, "ttnn-experimental-cumsum"]], "ttnn.experimental.dropout": [[109, "ttnn-experimental-dropout"]], "ttnn.experimental.gelu_bw": [[110, "ttnn-experimental-gelu-bw"]], "ttnn.experimental.rotary_embedding": [[111, "ttnn-experimental-rotary-embedding"]], "ttnn.experimental.sort": [[112, "ttnn-experimental-sort"]], "ttnn.expm1": [[113, "ttnn-expm1"]], "ttnn.expm1_bw": [[114, "ttnn-expm1-bw"]], "ttnn.fill": [[115, "ttnn-fill"]], "ttnn.fill_bw": [[116, "ttnn-fill-bw"]], "ttnn.fill_ones_rm": [[117, "ttnn-fill-ones-rm"]], "ttnn.fill_rm": [[118, "ttnn-fill-rm"]], "ttnn.fill_zero_bw": [[119, "ttnn-fill-zero-bw"]], "ttnn.floor": [[120, "ttnn-floor"]], "ttnn.floor_bw": [[121, "ttnn-floor-bw"]], "ttnn.floor_div": [[122, "ttnn-floor-div"]], "ttnn.fmod": [[123, "ttnn-fmod"]], "ttnn.fmod_bw": [[124, "ttnn-fmod-bw"]], "ttnn.format_input_tensor": [[125, "ttnn-format-input-tensor"]], "ttnn.format_output_tensor": [[126, "ttnn-format-output-tensor"]], "ttnn.frac": [[127, "ttnn-frac"]], "ttnn.frac_bw": [[128, "ttnn-frac-bw"]], "ttnn.from_device": [[129, "ttnn-from-device"]], "ttnn.from_torch": [[130, "ttnn-from-torch"]], "ttnn.full": [[131, "ttnn-full"]], "ttnn.full_like": [[132, "ttnn-full-like"]], "ttnn.gcd": [[133, "ttnn-gcd"]], "ttnn.ge": [[134, "ttnn-ge"]], "ttnn.ge_": [[135, "ttnn-ge"]], "ttnn.geglu": [[136, "ttnn-geglu"]], "ttnn.gelu": [[137, "ttnn-gelu"]], "ttnn.gelu_bw": [[138, "ttnn-gelu-bw"]], "ttnn.gez": [[139, "ttnn-gez"]], "ttnn.global_avg_pool2d": [[140, "ttnn-global-avg-pool2d"]], "ttnn.glu": [[141, "ttnn-glu"]], "ttnn.group_norm": [[142, "ttnn-group-norm"]], "ttnn.gt": [[143, "ttnn-gt"]], "ttnn.gt_": [[144, "ttnn-gt"]], "ttnn.gtz": [[145, "ttnn-gtz"]], "ttnn.hardshrink": [[146, "ttnn-hardshrink"]], "ttnn.hardshrink_bw": [[147, "ttnn-hardshrink-bw"]], "ttnn.hardsigmoid": [[148, "ttnn-hardsigmoid"]], "ttnn.hardsigmoid_bw": [[149, "ttnn-hardsigmoid-bw"]], "ttnn.hardswish": [[150, "ttnn-hardswish"]], "ttnn.hardswish_bw": [[151, "ttnn-hardswish-bw"]], "ttnn.hardtanh": [[152, "ttnn-hardtanh"]], "ttnn.hardtanh_bw": [[153, "ttnn-hardtanh-bw"]], "ttnn.heaviside": [[154, "ttnn-heaviside"]], "ttnn.hypot": [[155, "ttnn-hypot"]], "ttnn.hypot_bw": [[156, "ttnn-hypot-bw"]], "ttnn.i0": [[157, "ttnn-i0"]], "ttnn.i0_bw": [[158, "ttnn-i0-bw"]], "ttnn.identity": [[159, "ttnn-identity"]], "ttnn.imag": [[160, "ttnn-imag"]], "ttnn.imag_bw": [[161, "ttnn-imag-bw"]], "ttnn.indexed_fill": [[162, "ttnn-indexed-fill"]], "ttnn.is_imag": [[163, "ttnn-is-imag"]], "ttnn.is_real": [[164, "ttnn-is-real"]], "ttnn.isclose": [[165, "ttnn-isclose"]], "ttnn.isfinite": [[166, "ttnn-isfinite"]], "ttnn.isinf": [[167, "ttnn-isinf"]], "ttnn.isnan": [[168, "ttnn-isnan"]], "ttnn.isneginf": [[169, "ttnn-isneginf"]], "ttnn.isposinf": [[170, "ttnn-isposinf"]], "ttnn.kv_cache.fill_cache_for_user_": [[171, "ttnn-kv-cache-fill-cache-for-user"]], "ttnn.kv_cache.update_cache_for_token_": [[172, "ttnn-kv-cache-update-cache-for-token"]], "ttnn.l1_loss": [[173, "ttnn-l1-loss"]], "ttnn.layer_norm": [[174, "ttnn-layer-norm"]], "ttnn.lcm": [[175, "ttnn-lcm"]], "ttnn.ldexp": [[176, "ttnn-ldexp"]], "ttnn.ldexp_bw": [[177, "ttnn-ldexp-bw"]], "ttnn.le": [[178, "ttnn-le"]], "ttnn.le_": [[179, "ttnn-le"]], "ttnn.leaky_relu": [[180, "ttnn-leaky-relu"]], "ttnn.leaky_relu_bw": [[181, "ttnn-leaky-relu-bw"]], "ttnn.lerp": [[182, "ttnn-lerp"]], "ttnn.lerp_bw": [[183, "ttnn-lerp-bw"]], "ttnn.lez": [[184, "ttnn-lez"]], "ttnn.lgamma": [[185, "ttnn-lgamma"]], "ttnn.lgamma_bw": [[186, "ttnn-lgamma-bw"]], "ttnn.linear": [[187, "ttnn-linear"]], "ttnn.load_tensor": [[188, "ttnn-load-tensor"]], "ttnn.log": [[189, "ttnn-log"]], "ttnn.log10": [[190, "ttnn-log10"]], "ttnn.log10_bw": [[191, "ttnn-log10-bw"]], "ttnn.log1p": [[192, "ttnn-log1p"]], "ttnn.log1p_bw": [[193, "ttnn-log1p-bw"]], "ttnn.log2": [[194, "ttnn-log2"]], "ttnn.log2_bw": [[195, "ttnn-log2-bw"]], "ttnn.log_bw": [[196, "ttnn-log-bw"]], "ttnn.log_sigmoid": [[197, "ttnn-log-sigmoid"]], "ttnn.log_sigmoid_bw": [[198, "ttnn-log-sigmoid-bw"]], "ttnn.logaddexp": [[199, "ttnn-logaddexp"]], "ttnn.logaddexp2": [[200, "ttnn-logaddexp2"]], "ttnn.logaddexp2_bw": [[201, "ttnn-logaddexp2-bw"]], "ttnn.logaddexp_bw": [[202, "ttnn-logaddexp-bw"]], "ttnn.logical_and": [[203, "ttnn-logical-and"]], "ttnn.logical_and_": [[204, "ttnn-logical-and"]], "ttnn.logical_not": [[205, "ttnn-logical-not"]], "ttnn.logical_not_": [[206, "ttnn-logical-not"]], "ttnn.logical_or": [[207, "ttnn-logical-or"]], "ttnn.logical_or_": [[208, "ttnn-logical-or"]], "ttnn.logical_xor": [[209, "ttnn-logical-xor"]], "ttnn.logical_xor_": [[210, "ttnn-logical-xor"]], "ttnn.logit": [[211, "ttnn-logit"]], "ttnn.logit_bw": [[212, "ttnn-logit-bw"]], "ttnn.logiteps_bw": [[213, "ttnn-logiteps-bw"]], "ttnn.lt": [[214, "ttnn-lt"]], "ttnn.lt_": [[215, "ttnn-lt"]], "ttnn.ltz": [[216, "ttnn-ltz"]], "ttnn.mac": [[217, "ttnn-mac"]], "ttnn.manage_device": [[218, "ttnn-manage-device"]], "ttnn.matmul": [[219, "ttnn-matmul"]], "ttnn.max": [[220, "ttnn-max"]], "ttnn.max_bw": [[221, "ttnn-max-bw"]], "ttnn.max_pool2d": [[222, "ttnn-max-pool2d"]], "ttnn.maximum": [[223, "ttnn-maximum"]], "ttnn.mean": [[224, "ttnn-mean"]], "ttnn.min": [[225, "ttnn-min"]], "ttnn.min_bw": [[226, "ttnn-min-bw"]], "ttnn.minimum": [[227, "ttnn-minimum"]], "ttnn.mish": [[228, "ttnn-mish"]], "ttnn.model_preprocessing.preprocess_model": [[229, "ttnn-model-preprocessing-preprocess-model"]], "ttnn.model_preprocessing.preprocess_model_parameters": [[230, "ttnn-model-preprocessing-preprocess-model-parameters"]], "ttnn.moreh_sum": [[231, "ttnn-moreh-sum"]], "ttnn.mse_loss": [[232, "ttnn-mse-loss"]], "ttnn.mul_bw": [[233, "ttnn-mul-bw"]], "ttnn.multigammaln": [[234, "ttnn-multigammaln"]], "ttnn.multigammaln_bw": [[235, "ttnn-multigammaln-bw"]], "ttnn.multiply": [[236, "ttnn-multiply"]], "ttnn.ne": [[237, "ttnn-ne"]], "ttnn.ne_": [[238, "ttnn-ne"]], "ttnn.neg": [[239, "ttnn-neg"]], "ttnn.neg_bw": [[240, "ttnn-neg-bw"]], "ttnn.nextafter": [[241, "ttnn-nextafter"]], "ttnn.nez": [[242, "ttnn-nez"]], "ttnn.nonzero": [[243, "ttnn-nonzero"]], "ttnn.normalize_global": [[244, "ttnn-normalize-global"]], "ttnn.normalize_hw": [[245, "ttnn-normalize-hw"]], "ttnn.ones": [[246, "ttnn-ones"]], "ttnn.ones_like": [[247, "ttnn-ones-like"]], "ttnn.open_device": [[248, "ttnn-open-device"]], "ttnn.outer": [[249, "ttnn-outer"]], "ttnn.pad": [[250, "ttnn-pad"]], "ttnn.pad_to_tile_shape": [[251, "ttnn-pad-to-tile-shape"]], "ttnn.permute": [[252, "ttnn-permute"]], "ttnn.polar": [[253, "ttnn-polar"]], "ttnn.polar_bw": [[254, "ttnn-polar-bw"]], "ttnn.polygamma": [[255, "ttnn-polygamma"]], "ttnn.polygamma_bw": [[256, "ttnn-polygamma-bw"]], "ttnn.polyval": [[257, "ttnn-polyval"]], "ttnn.pow": [[258, "ttnn-pow"]], "ttnn.pow_bw": [[259, "ttnn-pow-bw"]], "ttnn.prelu": [[260, "ttnn-prelu"]], "ttnn.prepare_conv_bias": [[261, "ttnn-prepare-conv-bias"]], "ttnn.prepare_conv_transpose2d_bias": [[262, "ttnn-prepare-conv-transpose2d-bias"]], "ttnn.prepare_conv_transpose2d_weights": [[263, "ttnn-prepare-conv-transpose2d-weights"]], "ttnn.prepare_conv_weights": [[264, "ttnn-prepare-conv-weights"]], "ttnn.prod": [[265, "ttnn-prod"]], "ttnn.prod_bw": [[266, "ttnn-prod-bw"]], "ttnn.rad2deg": [[267, "ttnn-rad2deg"]], "ttnn.rad2deg_bw": [[268, "ttnn-rad2deg-bw"]], "ttnn.rdiv": [[269, "ttnn-rdiv"]], "ttnn.rdiv_bw": [[270, "ttnn-rdiv-bw"]], "ttnn.real": [[271, "ttnn-real"]], "ttnn.real_bw": [[272, "ttnn-real-bw"]], "ttnn.reallocate": [[273, "ttnn-reallocate"]], "ttnn.reciprocal": [[274, "ttnn-reciprocal"]], "ttnn.reciprocal_bw": [[275, "ttnn-reciprocal-bw"]], "ttnn.reduce_scatter": [[276, "ttnn-reduce-scatter"]], "ttnn.register_post_operation_hook": [[277, "ttnn-register-post-operation-hook"]], "ttnn.register_pre_operation_hook": [[278, "ttnn-register-pre-operation-hook"]], "ttnn.reglu": [[279, "ttnn-reglu"]], "ttnn.relu": [[280, "ttnn-relu"]], "ttnn.relu6": [[281, "ttnn-relu6"]], "ttnn.relu6_bw": [[282, "ttnn-relu6-bw"]], "ttnn.relu_bw": [[283, "ttnn-relu-bw"]], "ttnn.relu_max": [[284, "ttnn-relu-max"]], "ttnn.relu_min": [[285, "ttnn-relu-min"]], "ttnn.remainder": [[286, "ttnn-remainder"]], "ttnn.remainder_bw": [[287, "ttnn-remainder-bw"]], "ttnn.repeat": [[288, "ttnn-repeat"]], "ttnn.repeat_bw": [[289, "ttnn-repeat-bw"]], "ttnn.repeat_interleave": [[290, "ttnn-repeat-interleave"]], "ttnn.reshape": [[291, "ttnn-reshape"]], "ttnn.rms_norm": [[292, "ttnn-rms-norm"]], "ttnn.round": [[293, "ttnn-round"]], "ttnn.round_bw": [[294, "ttnn-round-bw"]], "ttnn.rpow": [[295, "ttnn-rpow"]], "ttnn.rpow_bw": [[296, "ttnn-rpow-bw"]], "ttnn.rsqrt": [[297, "ttnn-rsqrt"]], "ttnn.rsqrt_bw": [[298, "ttnn-rsqrt-bw"]], "ttnn.rsub": [[299, "ttnn-rsub"]], "ttnn.rsub_bw": [[300, "ttnn-rsub-bw"]], "ttnn.scatter": [[301, "ttnn-scatter"]], "ttnn.selu": [[302, "ttnn-selu"]], "ttnn.selu_bw": [[303, "ttnn-selu-bw"]], "ttnn.set_printoptions": [[304, "ttnn-set-printoptions"]], "ttnn.sigmoid": [[305, "ttnn-sigmoid"]], "ttnn.sigmoid_accurate": [[306, "ttnn-sigmoid-accurate"]], "ttnn.sigmoid_bw": [[307, "ttnn-sigmoid-bw"]], "ttnn.sign": [[308, "ttnn-sign"]], "ttnn.sign_bw": [[309, "ttnn-sign-bw"]], "ttnn.signbit": [[310, "ttnn-signbit"]], "ttnn.silu": [[311, "ttnn-silu"]], "ttnn.silu_bw": [[312, "ttnn-silu-bw"]], "ttnn.sin": [[313, "ttnn-sin"]], "ttnn.sin_bw": [[314, "ttnn-sin-bw"]], "ttnn.sinh": [[315, "ttnn-sinh"]], "ttnn.sinh_bw": [[316, "ttnn-sinh-bw"]], "ttnn.slice": [[317, "ttnn-slice"]], "ttnn.softmax": [[318, "ttnn-softmax"]], "ttnn.softplus": [[319, "ttnn-softplus"]], "ttnn.softplus_bw": [[320, "ttnn-softplus-bw"]], "ttnn.softshrink": [[321, "ttnn-softshrink"]], "ttnn.softshrink_bw": [[322, "ttnn-softshrink-bw"]], "ttnn.softsign": [[323, "ttnn-softsign"]], "ttnn.softsign_bw": [[324, "ttnn-softsign-bw"]], "ttnn.sqrt": [[325, "ttnn-sqrt"]], "ttnn.sqrt_bw": [[326, "ttnn-sqrt-bw"]], "ttnn.square": [[327, "ttnn-square"]], "ttnn.square_bw": [[328, "ttnn-square-bw"]], "ttnn.squared_difference": [[329, "ttnn-squared-difference"]], "ttnn.squared_difference_bw": [[330, "ttnn-squared-difference-bw"]], "ttnn.std": [[331, "ttnn-std"]], "ttnn.sub_bw": [[332, "ttnn-sub-bw"]], "ttnn.subalpha": [[333, "ttnn-subalpha"]], "ttnn.subalpha_bw": [[334, "ttnn-subalpha-bw"]], "ttnn.subtract": [[335, "ttnn-subtract"]], "ttnn.sum": [[336, "ttnn-sum"]], "ttnn.swiglu": [[337, "ttnn-swiglu"]], "ttnn.swish": [[338, "ttnn-swish"]], "ttnn.synchronize_device": [[339, "ttnn-synchronize-device"]], "ttnn.tan": [[340, "ttnn-tan"]], "ttnn.tan_bw": [[341, "ttnn-tan-bw"]], "ttnn.tanh": [[342, "ttnn-tanh"]], "ttnn.tanh_bw": [[343, "ttnn-tanh-bw"]], "ttnn.tanhshrink": [[344, "ttnn-tanhshrink"]], "ttnn.tanhshrink_bw": [[345, "ttnn-tanhshrink-bw"]], "ttnn.threshold": [[346, "ttnn-threshold"]], "ttnn.threshold_bw": [[347, "ttnn-threshold-bw"]], "ttnn.tilize": [[348, "ttnn-tilize"]], "ttnn.tilize_with_val_padding": [[349, "ttnn-tilize-with-val-padding"]], "ttnn.to_device": [[350, "ttnn-to-device"]], "ttnn.to_layout": [[351, "ttnn-to-layout"]], "ttnn.to_memory_config": [[352, "ttnn-to-memory-config"]], "ttnn.to_torch": [[353, "ttnn-to-torch"]], "ttnn.topk": [[354, "ttnn-topk"]], "ttnn.transformer.attention_softmax": [[355, "ttnn-transformer-attention-softmax"]], "ttnn.transformer.attention_softmax_": [[356, "ttnn-transformer-attention-softmax"]], "ttnn.transformer.concatenate_heads": [[357, "ttnn-transformer-concatenate-heads"]], "ttnn.transformer.scaled_dot_product_attention": [[358, "ttnn-transformer-scaled-dot-product-attention"]], "ttnn.transformer.scaled_dot_product_attention_decode": [[359, "ttnn-transformer-scaled-dot-product-attention-decode"]], "ttnn.transformer.split_query_key_value_and_split_heads": [[360, "ttnn-transformer-split-query-key-value-and-split-heads"]], "ttnn.tril": [[361, "ttnn-tril"]], "ttnn.triu": [[362, "ttnn-triu"]], "ttnn.trunc": [[363, "ttnn-trunc"]], "ttnn.trunc_bw": [[364, "ttnn-trunc-bw"]], "ttnn.unary_chain": [[365, "ttnn-unary-chain"]], "ttnn.untilize": [[366, "ttnn-untilize"]], "ttnn.untilize_with_unpadding": [[367, "ttnn-untilize-with-unpadding"]], "ttnn.upsample": [[368, "ttnn-upsample"]], "ttnn.var": [[369, "ttnn-var"]], "ttnn.where": [[370, "ttnn-where"]], "ttnn.where_bw": [[371, "ttnn-where-bw"]], "ttnn.xlogy": [[372, "ttnn-xlogy"]], "ttnn.xlogy_bw": [[373, "ttnn-xlogy-bw"]], "ttnn.zeros": [[374, "ttnn-zeros"]], "ttnn.zeros_like": [[375, "ttnn-zeros-like"]], "Converting PyTorch Model to TT-NN": [[376, "converting-pytorch-model-to-tt-nn"]], "Step 1 - Rewriting the Model": [[376, "step-1-rewriting-the-model"]], "Step 2 - Switching to ttnn Operations": [[376, "step-2-switching-to-ttnn-operations"]], "Step 3 - Optimizing the Model": [[376, "step-3-optimizing-the-model"]], "More examples": [[376, "more-examples"]], "Building and Uplifting Demos": [[377, "building-and-uplifting-demos"]], "Examples of Tensor and TT-LIB Use": [[378, "examples-of-tensor-and-tt-lib-use"]], "Run one OP from TT-LIB on TT Accelerator device": [[378, "run-one-op-from-tt-lib-on-tt-accelerator-device"]], "Run TT-LIB and PyTorch OPs": [[378, "run-tt-lib-and-pytorch-ops"]], "Tensors with odd size of last dim": [[378, "tensors-with-odd-size-of-last-dim"]], "Dependencies": [[379, "dependencies"]], "Tensor": [[380, "tensor"], [386, "tensor"]], "Overview": [[380, "overview"], [381, "overview"]], "Tensor Storage": [[380, "tensor-storage"]], "Tensor API": [[380, "tensor-api"]], "MemoryConfig": [[380, "memoryconfig"]], "Examples of converting between PyTorch Tensor and TT Tensor": [[380, "examples-of-converting-between-pytorch-tensor-and-tt-tensor"]], "Converting a PyTorch Tensor to a TT Tensor": [[380, "converting-a-pytorch-tensor-to-a-tt-tensor"]], "Converting a TT Tensor to a PyTorch Tensor": [[380, "converting-a-tt-tensor-to-a-pytorch-tensor"]], "TT-LIB": [[381, "tt-lib"]], "Operation Infrastructure": [[381, "operation-infrastructure"]], "New Device Operation": [[381, "new-device-operation"]], "New Device Operation with a member": [[381, "new-device-operation-with-a-member"]], "New Device Operation with Optional Input Tensors": [[381, "new-device-operation-with-optional-input-tensors"]], "New Device Operation with Optional Output Tensors": [[381, "new-device-operation-with-optional-output-tensors"]], "Profiler": [[381, "profiler"]], "Fast Dispatch": [[381, "fast-dispatch"]], "Program Caching": [[381, "program-caching"]], "Logs": [[381, "logs"]], "TT-LIB API through tt_lib": [[381, "tt-lib-api-through-tt-lib"]], "Primary Operations": [[381, "primary-operations"]], "Enums": [[381, "enums"]], "Fallback Operations": [[381, "fallback-operations"]], "Experimental Operations": [[381, "experimental-operations"]], "Fused Operations from tt_lib Mini-Graph Library": [[381, "fused-operations-from-tt-lib-mini-graph-library"]], "Complex Operations (Type 2)": [[381, "complex-operations-type-2"]], "1. Install and Build": [[382, "install-and-build"]], "2. Explore Our Model Demos": [[382, "explore-our-model-demos"]], "3. TT-NN Tutorial: Multi-Head Attention (Simple)": [[382, "tt-nn-tutorial-multi-head-attention-simple"]], "4. TT-NN Tutorial: Multi-Head Attention (Optimized)": [[382, "tt-nn-tutorial-multi-head-attention-optimized"]], "Where To Go From Here": [[382, "where-to-go-from-here"]], "Install": [[383, "install"]], "Prerequisites:": [[383, "prerequisites"]], "1: Set Up the Hardware": [[383, "set-up-the-hardware"]], "2: Install Driver & Firmware": [[383, "install-driver-firmware"]], "Install System-level Dependencies": [[383, "install-system-level-dependencies"]], "Install the Driver (TT-KMD)": [[383, "install-the-driver-tt-kmd"]], "Update Device TT-Firmware with TT-Flash": [[383, "update-device-tt-firmware-with-tt-flash"]], "Install System Management Interface (TT-SMI)": [[383, "install-system-management-interface-tt-smi"]], "(Optional) Multi-Card Configuration (TT-Topology)": [[383, "optional-multi-card-configuration-tt-topology"]], "TT-NN / TT-Metalium Installation": [[383, "tt-nn-tt-metalium-installation"]], "There are three options for installing TT-Metalium:": [[383, "there-are-three-options-for-installing-tt-metalium"]], "Option 1: From Source": [[383, "option-1-from-source"]], "Step 1. Clone the Repository:": [[383, "step-1-clone-the-repository"]], "Step 2. Invoke our Build Scripts:": [[383, "step-2-invoke-our-build-scripts"]], "Option 2: From Docker Release Image": [[383, "option-2-from-docker-release-image"]], "Option 3: From Wheel": [[383, "option-3-from-wheel"]], "Step 1. Download and Install the Latest Wheel:": [[383, "step-1-download-and-install-the-latest-wheel"]], "Step 2. (For models users only) Set Up Environment for Models:": [[383, "step-2-for-models-users-only-set-up-environment-for-models"]], "You are All Set!": [[383, "you-are-all-set"]], "To verify your installation, try executing a programming example:": [[383, "to-verify-your-installation-try-executing-a-programming-example"]], "Interested in Contributing?": [[383, "interested-in-contributing"]], "Onboarding New Functionality": [[384, "onboarding-new-functionality"]], "Profiling TT-NN Operations": [[385, "profiling-tt-nn-operations"]], "Perf Report Headers": [[385, "perf-report-headers"]], "profile_this description": [[385, "profile-this-description"]], "Shape": [[386, "shape"]], "Layout": [[386, "layout"], [395, "Layout"]], "Data Type": [[386, "data-type"], [395, "Data-Type"]], "Required Width Multiples for Data Types": [[386, "id4"]], "Limitation of BFLOAT8_B": [[386, "limitation-of-bfloat8-b"]], "Storage": [[386, "storage"]], "Tensor Sharding": [[386, "tensor-sharding"]], "Tutorials": [[387, "id1"]], "Graphing Torch DiT_XL_2 With TTNN": [[388, "graphing-torch-dit-xl-2-with-ttnn"]], "Matmul Operation": [[389, "matmul-operation"]], "Multi-Head Attention": [[390, "multi-head-attention"], [397, "Multi-Head-Attention"]], "ttnn Profiling": [[391, "ttnn-profiling"]], "Resnet Basic Block": [[392, "resnet-basic-block"]], "Tensor and Add Operation": [[393, "tensor-and-add-operation"], [395, "Tensor-and-Add-Operation"]], "ttnn Tracer": [[394, "ttnn-tracer"]], "Creating a tensor": [[395, "Creating-a-tensor"]], "Host Storage: Borrowed vs Owned": [[395, "Host-Storage:-Borrowed-vs-Owned"]], "Device storage": [[395, "Device-storage"]], "Open the device": [[395, "Open-the-device"]], "Initialize tensors a and b with random values using torch": [[395, "Initialize-tensors-a-and-b-with-random-values-using-torch"], [396, "Initialize-tensors-a-and-b-with-random-values-using-torch"]], "Add tensor a and b": [[395, "Add-tensor-a-and-b"]], "Inspect the output tensor of the add in ttnn": [[395, "Inspect-the-output-tensor-of-the-add-in-ttnn"]], "Convert to torch and inspect the attributes of the torch tensor": [[395, "Convert-to-torch-and-inspect-the-attributes-of-the-torch-tensor"]], "Close the device": [[395, "Close-the-device"], [396, "Close-the-device"], [397, "Close-the-device"]], "Enable program cache": [[396, "Enable-program-cache"], [397, "Enable-program-cache"]], "Configuration": [[396, "Configuration"], [397, "Configuration"]], "Matrix multiply tensor a and b": [[396, "Matrix-multiply-tensor-a-and-b"]], "Inspect the layout of matrix multiplication output": [[396, "Inspect-the-layout-of-matrix-multiplication-output"]], "Inspect the result of the matrix multiplication": [[396, "Inspect-the-result-of-the-matrix-multiplication"]], "Matrix multiply tensor a and b by using more performant config": [[396, "Matrix-multiply-tensor-a-and-b-by-using-more-performant-config"]], "Write Multi-Head Attention using ttnn": [[397, "Write-Multi-Head-Attention-using-ttnn"]], "Initialize activations and weights using torch": [[397, "Initialize-activations-and-weights-using-torch"]], "Convert activations and weights to ttnn": [[397, "Convert-activations-and-weights-to-ttnn"]], "Run the first iteration of Multi-Head Attention": [[397, "Run-the-first-iteration-of-Multi-Head-Attention"]], "Run a subsequent iteration of Multi-Head Attention": [[397, "Run-a-subsequent-iteration-of-Multi-Head-Attention"]], "Write optimized version of Multi-Head Attention": [[397, "Write-optimized-version-of-Multi-Head-Attention"]], "Pre-process the parameters of the optimized model": [[397, "Pre-process-the-parameters-of-the-optimized-model"]], "Run the first iteration of the optimized Multi-Head Attention": [[397, "Run-the-first-iteration-of-the-optimized-Multi-Head-Attention"]], "Run a subsequent iteration of the optimized Multi-Head Attention": [[397, "Run-a-subsequent-iteration-of-the-optimized-Multi-Head-Attention"]], "Check that the output of the optimized version matches the output of the original implementation": [[397, "Check-that-the-output-of-the-optimized-version-matches-the-output-of-the-original-implementation"]], "Tracing ttnn operations and torch modules/functions": [[398, "Tracing-ttnn-operations-and-torch-modules/functions"]], "Trace torch functions": [[398, "Trace-torch-functions"]], "Trace torch functions and ttnn operations": [[398, "Trace-torch-functions-and-ttnn-operations"]], "Trace torch functions, torch modules and ttnn operations": [[398, "Trace-torch-functions,-torch-modules-and-ttnn-operations"]], "Trace models written using ttnn": [[398, "Trace-models-written-using-ttnn"]], "Profiling ttnn operations": [[399, "Profiling-ttnn-operations"]], "Resnet Block": [[400, "Resnet-Block"]], "Torch Module (from torchvision)": [[400, "Torch-Module-(from-torchvision)"]], "Create torch module and preprocess it to get ttnn parameters": [[400, "Create-torch-module-and-preprocess-it-to-get-ttnn-parameters"]], "Display the parameters of the module": [[400, "Display-the-parameters-of-the-module"]], "Display the traced torch graph": [[400, "Display-the-traced-torch-graph"]], "Implement ttnn version of the module. Pass in the parameters into the constructor.": [[400, "Implement-ttnn-version-of-the-module.-Pass-in-the-parameters-into-the-constructor."]], "Run ttnn module and display the traced graph": [[400, "Run-ttnn-module-and-display-the-traced-graph"]], "Build a graph of a pytorch based model": [[401, "Build-a-graph-of-a-pytorch-based-model"]], "Clone the library from https://github.com/facebookresearch/DiT.git": [[401, "Clone-the-library-from-https://github.com/facebookresearch/DiT.git"]], "Download DiT-XL/2 Models": [[401, "Download-DiT-XL/2-Models"]], "Sample from Pre-trained DiT Models and build the graph": [[401, "Sample-from-Pre-trained-DiT-Models-and-build-the-graph"]], "Display the graph": [[401, "Display-the-graph"]], "Using TT-NN": [[402, "using-tt-nn"]], "Basic Examples": [[402, "basic-examples"]], "1. Converting from and to torch tensor": [[402, "converting-from-and-to-torch-tensor"]], "2. Running an operation on the device": [[402, "running-an-operation-on-the-device"]], "3. Using __getitem__ to slice the tensor": [[402, "using-getitem-to-slice-the-tensor"]], "4. Enabling program cache": [[402, "enabling-program-cache"]], "5. Debugging intermediate tensors": [[402, "debugging-intermediate-tensors"]], "6. Tracing the graph of operations": [[402, "tracing-the-graph-of-operations"]], "7. Using tt_lib operation in TT-NN": [[402, "using-tt-lib-operation-in-tt-nn"]], "8. Enabling Logging": [[402, "enabling-logging"]], "9. Supported Python Operators": [[402, "supported-python-operators"]], "10. Changing the string representation of the tensor": [[402, "changing-the-string-representation-of-the-tensor"]], "11. Visualize using Web Browser": [[402, "visualize-using-web-browser"]], "12. Register pre- and/or post-operation hooks": [[402, "register-pre-and-or-post-operation-hooks"]], "13. Query all operations": [[402, "query-all-operations"]], "14. Falling back to torch": [[402, "falling-back-to-torch"]], "15. Capturing graph of C++ functions, buffer allocations, etc": [[402, "capturing-graph-of-c-functions-buffer-allocations-etc"]]}, "indexentries": {"conv2dconfig (class in ttnn)": [[8, "ttnn.Conv2dConfig"]], "act_block_h_override (ttnn.conv2dconfig property)": [[8, "ttnn.Conv2dConfig.act_block_h_override"]], "act_block_w_div (ttnn.conv2dconfig property)": [[8, "ttnn.Conv2dConfig.act_block_w_div"]], "activation (ttnn.conv2dconfig property)": [[8, "ttnn.Conv2dConfig.activation"]], "always_preprocess_weights (ttnn.conv2dconfig property)": [[8, "ttnn.Conv2dConfig.always_preprocess_weights"]], "core_grid (ttnn.conv2dconfig property)": [[8, "ttnn.Conv2dConfig.core_grid"]], "deallocate_activation (ttnn.conv2dconfig property)": [[8, "ttnn.Conv2dConfig.deallocate_activation"]], "dtype (ttnn.conv2dconfig property)": [[8, "ttnn.Conv2dConfig.dtype"]], "enable_act_double_buffer (ttnn.conv2dconfig property)": [[8, "ttnn.Conv2dConfig.enable_act_double_buffer"]], "enable_split_reader (ttnn.conv2dconfig property)": [[8, "ttnn.Conv2dConfig.enable_split_reader"]], "enable_subblock_padding (ttnn.conv2dconfig property)": [[8, "ttnn.Conv2dConfig.enable_subblock_padding"]], "enable_weights_double_buffer (ttnn.conv2dconfig property)": [[8, "ttnn.Conv2dConfig.enable_weights_double_buffer"]], "in_place (ttnn.conv2dconfig property)": [[8, "ttnn.Conv2dConfig.in_place"]], "output_layout (ttnn.conv2dconfig property)": [[8, "ttnn.Conv2dConfig.output_layout"]], "override_sharding_config (ttnn.conv2dconfig property)": [[8, "ttnn.Conv2dConfig.override_sharding_config"]], "preprocess_weights_on_device (ttnn.conv2dconfig property)": [[8, "ttnn.Conv2dConfig.preprocess_weights_on_device"]], "reallocate_halo_output (ttnn.conv2dconfig property)": [[8, "ttnn.Conv2dConfig.reallocate_halo_output"]], "reshard_if_not_optimal (ttnn.conv2dconfig property)": [[8, "ttnn.Conv2dConfig.reshard_if_not_optimal"]], "shard_layout (ttnn.conv2dconfig property)": [[8, "ttnn.Conv2dConfig.shard_layout"]], "transpose_shards (ttnn.conv2dconfig property)": [[8, "ttnn.Conv2dConfig.transpose_shards"]], "weights_dtype (ttnn.conv2dconfig property)": [[8, "ttnn.Conv2dConfig.weights_dtype"]], "conv2dsliceconfig (class in ttnn)": [[9, "ttnn.Conv2dSliceConfig"]], "conv2dsliceconfig.slicetypeenum (class in ttnn)": [[9, "ttnn.Conv2dSliceConfig.SliceTypeEnum"]], "sliceheight (ttnn.conv2dsliceconfig.slicetypeenum attribute)": [[9, "ttnn.Conv2dSliceConfig.SliceTypeEnum.SliceHeight"]], "slicewidth (ttnn.conv2dsliceconfig.slicetypeenum attribute)": [[9, "ttnn.Conv2dSliceConfig.SliceTypeEnum.SliceWidth"]], "name (ttnn.conv2dsliceconfig.slicetypeenum property)": [[9, "ttnn.Conv2dSliceConfig.SliceTypeEnum.name"]], "num_slices (ttnn.conv2dsliceconfig property)": [[9, "ttnn.Conv2dSliceConfig.num_slices"]], "slice_type (ttnn.conv2dsliceconfig property)": [[9, "ttnn.Conv2dSliceConfig.slice_type"]], "value (ttnn.conv2dsliceconfig.slicetypeenum property)": [[9, "ttnn.Conv2dSliceConfig.SliceTypeEnum.value"]], "getdefaultdevice() (in module ttnn)": [[10, "ttnn.GetDefaultDevice"]], "setdefaultdevice() (in module ttnn)": [[11, "ttnn.SetDefaultDevice"]], "abs (in module ttnn)": [[12, "ttnn.abs"]], "abs_bw (in module ttnn)": [[13, "ttnn.abs_bw"]], "acos (in module ttnn)": [[14, "ttnn.acos"]], "acos_bw (in module ttnn)": [[15, "ttnn.acos_bw"]], "acosh (in module ttnn)": [[16, "ttnn.acosh"]], "acosh_bw (in module ttnn)": [[17, "ttnn.acosh_bw"]], "add (in module ttnn)": [[18, "ttnn.add"]], "add_bw (in module ttnn)": [[19, "ttnn.add_bw"]], "addalpha (in module ttnn)": [[20, "ttnn.addalpha"]], "addalpha_bw (in module ttnn)": [[21, "ttnn.addalpha_bw"]], "addcdiv (in module ttnn)": [[22, "ttnn.addcdiv"]], "addcdiv_bw (in module ttnn)": [[23, "ttnn.addcdiv_bw"]], "addcmul (in module ttnn)": [[24, "ttnn.addcmul"]], "addcmul_bw (in module ttnn)": [[25, "ttnn.addcmul_bw"]], "all_gather (in module ttnn)": [[26, "ttnn.all_gather"]], "alt_complex_rotate90 (in module ttnn)": [[27, "ttnn.alt_complex_rotate90"]], "angle (in module ttnn)": [[28, "ttnn.angle"]], "angle_bw (in module ttnn)": [[29, "ttnn.angle_bw"]], "arange (in module ttnn)": [[30, "ttnn.arange"]], "argmax (in module ttnn)": [[31, "ttnn.argmax"]], "as_tensor (in module ttnn)": [[32, "ttnn.as_tensor"]], "asin (in module ttnn)": [[33, "ttnn.asin"]], "asin_bw (in module ttnn)": [[34, "ttnn.asin_bw"]], "asinh (in module ttnn)": [[35, "ttnn.asinh"]], "asinh_bw (in module ttnn)": [[36, "ttnn.asinh_bw"]], "assign_bw (in module ttnn)": [[37, "ttnn.assign_bw"]], "atan (in module ttnn)": [[38, "ttnn.atan"]], "atan2 (in module ttnn)": [[39, "ttnn.atan2"]], "atan2_bw (in module ttnn)": [[40, "ttnn.atan2_bw"]], "atan_bw (in module ttnn)": [[41, "ttnn.atan_bw"]], "atanh (in module ttnn)": [[42, "ttnn.atanh"]], "atanh_bw (in module ttnn)": [[43, "ttnn.atanh_bw"]], "batch_norm (in module ttnn)": [[44, "ttnn.batch_norm"]], "bias_gelu_bw (in module ttnn)": [[45, "ttnn.bias_gelu_bw"]], "bitwise_and (in module ttnn)": [[46, "ttnn.bitwise_and"]], "bitwise_left_shift (in module ttnn)": [[47, "ttnn.bitwise_left_shift"]], "bitwise_not (in module ttnn)": [[48, "ttnn.bitwise_not"]], "bitwise_or (in module ttnn)": [[49, "ttnn.bitwise_or"]], "bitwise_right_shift (in module ttnn)": [[50, "ttnn.bitwise_right_shift"]], "bitwise_xor (in module ttnn)": [[51, "ttnn.bitwise_xor"]], "cbrt (in module ttnn)": [[52, "ttnn.cbrt"]], "ceil (in module ttnn)": [[53, "ttnn.ceil"]], "ceil_bw (in module ttnn)": [[54, "ttnn.ceil_bw"]], "celu (in module ttnn)": [[55, "ttnn.celu"]], "celu_bw (in module ttnn)": [[56, "ttnn.celu_bw"]], "clamp (in module ttnn)": [[57, "ttnn.clamp"]], "clamp_bw (in module ttnn)": [[58, "ttnn.clamp_bw"]], "clip (in module ttnn)": [[59, "ttnn.clip"]], "clip_bw (in module ttnn)": [[60, "ttnn.clip_bw"]], "clone (in module ttnn)": [[61, "ttnn.clone"]], "close_device() (in module ttnn)": [[62, "ttnn.close_device"]], "concat (in module ttnn)": [[63, "ttnn.concat"]], "concat_bw (in module ttnn)": [[64, "ttnn.concat_bw"]], "conj (in module ttnn)": [[65, "ttnn.conj"]], "conj_bw (in module ttnn)": [[66, "ttnn.conj_bw"]], "conv1d (in module ttnn)": [[67, "ttnn.conv1d"]], "conv2d (in module ttnn)": [[68, "ttnn.conv2d"]], "conv_transpose2d (in module ttnn)": [[69, "ttnn.conv_transpose2d"]], "cos (in module ttnn)": [[70, "ttnn.cos"]], "cos_bw (in module ttnn)": [[71, "ttnn.cos_bw"]], "cosh (in module ttnn)": [[72, "ttnn.cosh"]], "cosh_bw (in module ttnn)": [[73, "ttnn.cosh_bw"]], "create_sharded_memory_config() (in module ttnn)": [[74, "ttnn.create_sharded_memory_config"]], "deallocate (in module ttnn)": [[75, "ttnn.deallocate"]], "deg2rad (in module ttnn)": [[76, "ttnn.deg2rad"]], "deg2rad_bw (in module ttnn)": [[77, "ttnn.deg2rad_bw"]], "digamma (in module ttnn)": [[78, "ttnn.digamma"]], "digamma_bw (in module ttnn)": [[79, "ttnn.digamma_bw"]], "div (in module ttnn)": [[80, "ttnn.div"]], "div_bw (in module ttnn)": [[81, "ttnn.div_bw"]], "div_no_nan (in module ttnn)": [[82, "ttnn.div_no_nan"]], "div_no_nan_bw (in module ttnn)": [[83, "ttnn.div_no_nan_bw"]], "downsample (in module ttnn)": [[84, "ttnn.downsample"]], "dump_tensor (in module ttnn)": [[85, "ttnn.dump_tensor"]], "elu (in module ttnn)": [[86, "ttnn.elu"]], "elu_bw (in module ttnn)": [[87, "ttnn.elu_bw"]], "embedding (in module ttnn)": [[88, "ttnn.embedding"]], "embedding_bw (in module ttnn)": [[89, "ttnn.embedding_bw"]], "empty (in module ttnn)": [[90, "ttnn.empty"]], "empty_like (in module ttnn)": [[91, "ttnn.empty_like"]], "eq (in module ttnn)": [[92, "ttnn.eq"]], "eq_ (in module ttnn)": [[93, "ttnn.eq_"]], "eqz (in module ttnn)": [[94, "ttnn.eqz"]], "erf (in module ttnn)": [[95, "ttnn.erf"]], "erf_bw (in module ttnn)": [[96, "ttnn.erf_bw"]], "erfc (in module ttnn)": [[97, "ttnn.erfc"]], "erfc_bw (in module ttnn)": [[98, "ttnn.erfc_bw"]], "erfinv (in module ttnn)": [[99, "ttnn.erfinv"]], "erfinv_bw (in module ttnn)": [[100, "ttnn.erfinv_bw"]], "exp (in module ttnn)": [[101, "ttnn.exp"]], "exp2 (in module ttnn)": [[102, "ttnn.exp2"]], "exp2_bw (in module ttnn)": [[103, "ttnn.exp2_bw"]], "exp_bw (in module ttnn)": [[104, "ttnn.exp_bw"]], "all_reduce (in module ttnn.experimental)": [[105, "ttnn.experimental.all_reduce"]], "conv3d (in module ttnn.experimental)": [[106, "ttnn.experimental.conv3d"]], "cumprod (in module ttnn.experimental)": [[107, "ttnn.experimental.cumprod"]], "cumsum (in module ttnn.experimental)": [[108, "ttnn.experimental.cumsum"]], "dropout (in module ttnn.experimental)": [[109, "ttnn.experimental.dropout"]], "gelu_bw (in module ttnn.experimental)": [[110, "ttnn.experimental.gelu_bw"]], "rotary_embedding (in module ttnn.experimental)": [[111, "ttnn.experimental.rotary_embedding"]], "sort (in module ttnn.experimental)": [[112, "ttnn.experimental.sort"]], "expm1 (in module ttnn)": [[113, "ttnn.expm1"]], "expm1_bw (in module ttnn)": [[114, "ttnn.expm1_bw"]], "fill (in module ttnn)": [[115, "ttnn.fill"]], "fill_bw (in module ttnn)": [[116, "ttnn.fill_bw"]], "fill_ones_rm (in module ttnn)": [[117, "ttnn.fill_ones_rm"]], "fill_rm (in module ttnn)": [[118, "ttnn.fill_rm"]], "fill_zero_bw (in module ttnn)": [[119, "ttnn.fill_zero_bw"]], "floor (in module ttnn)": [[120, "ttnn.floor"]], "floor_bw (in module ttnn)": [[121, "ttnn.floor_bw"]], "floor_div (in module ttnn)": [[122, "ttnn.floor_div"]], "fmod (in module ttnn)": [[123, "ttnn.fmod"]], "fmod_bw (in module ttnn)": [[124, "ttnn.fmod_bw"]], "format_input_tensor() (in module ttnn)": [[125, "ttnn.format_input_tensor"]], "format_output_tensor() (in module ttnn)": [[126, "ttnn.format_output_tensor"]], "frac (in module ttnn)": [[127, "ttnn.frac"]], "frac_bw (in module ttnn)": [[128, "ttnn.frac_bw"]], "from_device (in module ttnn)": [[129, "ttnn.from_device"]], "from_torch (in module ttnn)": [[130, "ttnn.from_torch"]], "full (in module ttnn)": [[131, "ttnn.full"]], "full_like (in module ttnn)": [[132, "ttnn.full_like"]], "gcd (in module ttnn)": [[133, "ttnn.gcd"]], "ge (in module ttnn)": [[134, "ttnn.ge"]], "ge_ (in module ttnn)": [[135, "ttnn.ge_"]], "geglu (in module ttnn)": [[136, "ttnn.geglu"]], "gelu (in module ttnn)": [[137, "ttnn.gelu"]], "gelu_bw (in module ttnn)": [[138, "ttnn.gelu_bw"]], "gez (in module ttnn)": [[139, "ttnn.gez"]], "global_avg_pool2d (in module ttnn)": [[140, "ttnn.global_avg_pool2d"]], "glu (in module ttnn)": [[141, "ttnn.glu"]], "group_norm (in module ttnn)": [[142, "ttnn.group_norm"]], "gt (in module ttnn)": [[143, "ttnn.gt"]], "gt_ (in module ttnn)": [[144, "ttnn.gt_"]], "gtz (in module ttnn)": [[145, "ttnn.gtz"]], "hardshrink (in module ttnn)": [[146, "ttnn.hardshrink"]], "hardshrink_bw (in module ttnn)": [[147, "ttnn.hardshrink_bw"]], "hardsigmoid (in module ttnn)": [[148, "ttnn.hardsigmoid"]], "hardsigmoid_bw (in module ttnn)": [[149, "ttnn.hardsigmoid_bw"]], "hardswish (in module ttnn)": [[150, "ttnn.hardswish"]], "hardswish_bw (in module ttnn)": [[151, "ttnn.hardswish_bw"]], "hardtanh (in module ttnn)": [[152, "ttnn.hardtanh"]], "hardtanh_bw (in module ttnn)": [[153, "ttnn.hardtanh_bw"]], "heaviside (in module ttnn)": [[154, "ttnn.heaviside"]], "hypot (in module ttnn)": [[155, "ttnn.hypot"]], "hypot_bw (in module ttnn)": [[156, "ttnn.hypot_bw"]], "i0 (in module ttnn)": [[157, "ttnn.i0"]], "i0_bw (in module ttnn)": [[158, "ttnn.i0_bw"]], "identity (in module ttnn)": [[159, "ttnn.identity"]], "imag (in module ttnn)": [[160, "ttnn.imag"]], "imag_bw (in module ttnn)": [[161, "ttnn.imag_bw"]], "indexed_fill (in module ttnn)": [[162, "ttnn.indexed_fill"]], "is_imag (in module ttnn)": [[163, "ttnn.is_imag"]], "is_real (in module ttnn)": [[164, "ttnn.is_real"]], "isclose (in module ttnn)": [[165, "ttnn.isclose"]], "isfinite (in module ttnn)": [[166, "ttnn.isfinite"]], "isinf (in module ttnn)": [[167, "ttnn.isinf"]], "isnan (in module ttnn)": [[168, "ttnn.isnan"]], "isneginf (in module ttnn)": [[169, "ttnn.isneginf"]], "isposinf (in module ttnn)": [[170, "ttnn.isposinf"]], "fill_cache_for_user_ (in module ttnn.kv_cache)": [[171, "ttnn.kv_cache.fill_cache_for_user_"]], "update_cache_for_token_ (in module ttnn.kv_cache)": [[172, "ttnn.kv_cache.update_cache_for_token_"]], "l1_loss (in module ttnn)": [[173, "ttnn.l1_loss"]], "layer_norm (in module ttnn)": [[174, "ttnn.layer_norm"]], "lcm (in module ttnn)": [[175, "ttnn.lcm"]], "ldexp (in module ttnn)": [[176, "ttnn.ldexp"]], "ldexp_bw (in module ttnn)": [[177, "ttnn.ldexp_bw"]], "le (in module ttnn)": [[178, "ttnn.le"]], "le_ (in module ttnn)": [[179, "ttnn.le_"]], "leaky_relu (in module ttnn)": [[180, "ttnn.leaky_relu"]], "leaky_relu_bw (in module ttnn)": [[181, "ttnn.leaky_relu_bw"]], "lerp (in module ttnn)": [[182, "ttnn.lerp"]], "lerp_bw (in module ttnn)": [[183, "ttnn.lerp_bw"]], "lez (in module ttnn)": [[184, "ttnn.lez"]], "lgamma (in module ttnn)": [[185, "ttnn.lgamma"]], "lgamma_bw (in module ttnn)": [[186, "ttnn.lgamma_bw"]], "linear (in module ttnn)": [[187, "ttnn.linear"]], "load_tensor (in module ttnn)": [[188, "ttnn.load_tensor"]], "log (in module ttnn)": [[189, "ttnn.log"]], "log10 (in module ttnn)": [[190, "ttnn.log10"]], "log10_bw (in module ttnn)": [[191, "ttnn.log10_bw"]], "log1p (in module ttnn)": [[192, "ttnn.log1p"]], "log1p_bw (in module ttnn)": [[193, "ttnn.log1p_bw"]], "log2 (in module ttnn)": [[194, "ttnn.log2"]], "log2_bw (in module ttnn)": [[195, "ttnn.log2_bw"]], "log_bw (in module ttnn)": [[196, "ttnn.log_bw"]], "log_sigmoid (in module ttnn)": [[197, "ttnn.log_sigmoid"]], "log_sigmoid_bw (in module ttnn)": [[198, "ttnn.log_sigmoid_bw"]], "logaddexp (in module ttnn)": [[199, "ttnn.logaddexp"]], "logaddexp2 (in module ttnn)": [[200, "ttnn.logaddexp2"]], "logaddexp2_bw (in module ttnn)": [[201, "ttnn.logaddexp2_bw"]], "logaddexp_bw (in module ttnn)": [[202, "ttnn.logaddexp_bw"]], "logical_and (in module ttnn)": [[203, "ttnn.logical_and"]], "logical_and_ (in module ttnn)": [[204, "ttnn.logical_and_"]], "logical_not (in module ttnn)": [[205, "ttnn.logical_not"]], "logical_not_ (in module ttnn)": [[206, "ttnn.logical_not_"]], "logical_or (in module ttnn)": [[207, "ttnn.logical_or"]], "logical_or_ (in module ttnn)": [[208, "ttnn.logical_or_"]], "logical_xor (in module ttnn)": [[209, "ttnn.logical_xor"]], "logical_xor_ (in module ttnn)": [[210, "ttnn.logical_xor_"]], "logit (in module ttnn)": [[211, "ttnn.logit"]], "logit_bw (in module ttnn)": [[212, "ttnn.logit_bw"]], "logiteps_bw (in module ttnn)": [[213, "ttnn.logiteps_bw"]], "lt (in module ttnn)": [[214, "ttnn.lt"]], "lt_ (in module ttnn)": [[215, "ttnn.lt_"]], "ltz (in module ttnn)": [[216, "ttnn.ltz"]], "mac (in module ttnn)": [[217, "ttnn.mac"]], "manage_device() (in module ttnn)": [[218, "ttnn.manage_device"]], "matmul (in module ttnn)": [[219, "ttnn.matmul"]], "max (in module ttnn)": [[220, "ttnn.max"]], "max_bw (in module ttnn)": [[221, "ttnn.max_bw"]], "max_pool2d (in module ttnn)": [[222, "ttnn.max_pool2d"]], "maximum (in module ttnn)": [[223, "ttnn.maximum"]], "mean (in module ttnn)": [[224, "ttnn.mean"]], "min (in module ttnn)": [[225, "ttnn.min"]], "min_bw (in module ttnn)": [[226, "ttnn.min_bw"]], "minimum (in module ttnn)": [[227, "ttnn.minimum"]], "mish (in module ttnn)": [[228, "ttnn.mish"]], "preprocess_model() (in module ttnn.model_preprocessing)": [[229, "ttnn.model_preprocessing.preprocess_model"]], "preprocess_model_parameters() (in module ttnn.model_preprocessing)": [[230, "ttnn.model_preprocessing.preprocess_model_parameters"]], "moreh_sum (in module ttnn)": [[231, "ttnn.moreh_sum"]], "mse_loss (in module ttnn)": [[232, "ttnn.mse_loss"]], "mul_bw (in module ttnn)": [[233, "ttnn.mul_bw"]], "multigammaln (in module ttnn)": [[234, "ttnn.multigammaln"]], "multigammaln_bw (in module ttnn)": [[235, "ttnn.multigammaln_bw"]], "multiply (in module ttnn)": [[236, "ttnn.multiply"]], "ne (in module ttnn)": [[237, "ttnn.ne"]], "ne_ (in module ttnn)": [[238, "ttnn.ne_"]], "neg (in module ttnn)": [[239, "ttnn.neg"]], "neg_bw (in module ttnn)": [[240, "ttnn.neg_bw"]], "nextafter (in module ttnn)": [[241, "ttnn.nextafter"]], "nez (in module ttnn)": [[242, "ttnn.nez"]], "nonzero (in module ttnn)": [[243, "ttnn.nonzero"]], "normalize_global (in module ttnn)": [[244, "ttnn.normalize_global"]], "normalize_hw (in module ttnn)": [[245, "ttnn.normalize_hw"]], "ones (in module ttnn)": [[246, "ttnn.ones"]], "ones_like (in module ttnn)": [[247, "ttnn.ones_like"]], "open_device() (in module ttnn)": [[248, "ttnn.open_device"]], "outer (in module ttnn)": [[249, "ttnn.outer"]], "pad (in module ttnn)": [[250, "ttnn.pad"]], "pad_to_tile_shape() (in module ttnn)": [[251, "ttnn.pad_to_tile_shape"]], "permute (in module ttnn)": [[252, "ttnn.permute"]], "polar (in module ttnn)": [[253, "ttnn.polar"]], "polar_bw (in module ttnn)": [[254, "ttnn.polar_bw"]], "polygamma (in module ttnn)": [[255, "ttnn.polygamma"]], "polygamma_bw (in module ttnn)": [[256, "ttnn.polygamma_bw"]], "polyval (in module ttnn)": [[257, "ttnn.polyval"]], "pow (in module ttnn)": [[258, "ttnn.pow"]], "pow_bw (in module ttnn)": [[259, "ttnn.pow_bw"]], "prelu (in module ttnn)": [[260, "ttnn.prelu"]], "prepare_conv_bias() (in module ttnn)": [[261, "ttnn.prepare_conv_bias"]], "prepare_conv_transpose2d_bias() (in module ttnn)": [[262, "ttnn.prepare_conv_transpose2d_bias"]], "prepare_conv_transpose2d_weights() (in module ttnn)": [[263, "ttnn.prepare_conv_transpose2d_weights"]], "prepare_conv_weights() (in module ttnn)": [[264, "ttnn.prepare_conv_weights"]], "prod (in module ttnn)": [[265, "ttnn.prod"]], "prod_bw (in module ttnn)": [[266, "ttnn.prod_bw"]], "rad2deg (in module ttnn)": [[267, "ttnn.rad2deg"]], "rad2deg_bw (in module ttnn)": [[268, "ttnn.rad2deg_bw"]], "rdiv (in module ttnn)": [[269, "ttnn.rdiv"]], "rdiv_bw (in module ttnn)": [[270, "ttnn.rdiv_bw"]], "real (in module ttnn)": [[271, "ttnn.real"]], "real_bw (in module ttnn)": [[272, "ttnn.real_bw"]], "reallocate (in module ttnn)": [[273, "ttnn.reallocate"]], "reciprocal (in module ttnn)": [[274, "ttnn.reciprocal"]], "reciprocal_bw (in module ttnn)": [[275, "ttnn.reciprocal_bw"]], "reduce_scatter (in module ttnn)": [[276, "ttnn.reduce_scatter"]], "register_post_operation_hook() (in module ttnn)": [[277, "ttnn.register_post_operation_hook"]], "register_pre_operation_hook() (in module ttnn)": [[278, "ttnn.register_pre_operation_hook"]], "reglu (in module ttnn)": [[279, "ttnn.reglu"]], "relu (in module ttnn)": [[280, "ttnn.relu"]], "relu6 (in module ttnn)": [[281, "ttnn.relu6"]], "relu6_bw (in module ttnn)": [[282, "ttnn.relu6_bw"]], "relu_bw (in module ttnn)": [[283, "ttnn.relu_bw"]], "relu_max (in module ttnn)": [[284, "ttnn.relu_max"]], "relu_min (in module ttnn)": [[285, "ttnn.relu_min"]], "remainder (in module ttnn)": [[286, "ttnn.remainder"]], "remainder_bw (in module ttnn)": [[287, "ttnn.remainder_bw"]], "repeat (in module ttnn)": [[288, "ttnn.repeat"]], "repeat_bw (in module ttnn)": [[289, "ttnn.repeat_bw"]], "repeat_interleave (in module ttnn)": [[290, "ttnn.repeat_interleave"]], "reshape (in module ttnn)": [[291, "ttnn.reshape"]], "rms_norm (in module ttnn)": [[292, "ttnn.rms_norm"]], "round (in module ttnn)": [[293, "ttnn.round"]], "round_bw (in module ttnn)": [[294, "ttnn.round_bw"]], "rpow (in module ttnn)": [[295, "ttnn.rpow"]], "rpow_bw (in module ttnn)": [[296, "ttnn.rpow_bw"]], "rsqrt (in module ttnn)": [[297, "ttnn.rsqrt"]], "rsqrt_bw (in module ttnn)": [[298, "ttnn.rsqrt_bw"]], "rsub (in module ttnn)": [[299, "ttnn.rsub"]], "rsub_bw (in module ttnn)": [[300, "ttnn.rsub_bw"]], "scatter (in module ttnn)": [[301, "ttnn.scatter"]], "selu (in module ttnn)": [[302, "ttnn.selu"]], "selu_bw (in module ttnn)": [[303, "ttnn.selu_bw"]], "set_printoptions() (in module ttnn)": [[304, "ttnn.set_printoptions"]], "sigmoid (in module ttnn)": [[305, "ttnn.sigmoid"]], "sigmoid_accurate (in module ttnn)": [[306, "ttnn.sigmoid_accurate"]], "sigmoid_bw (in module ttnn)": [[307, "ttnn.sigmoid_bw"]], "sign (in module ttnn)": [[308, "ttnn.sign"]], "sign_bw (in module ttnn)": [[309, "ttnn.sign_bw"]], "signbit (in module ttnn)": [[310, "ttnn.signbit"]], "silu (in module ttnn)": [[311, "ttnn.silu"]], "silu_bw (in module ttnn)": [[312, "ttnn.silu_bw"]], "sin (in module ttnn)": [[313, "ttnn.sin"]], "sin_bw (in module ttnn)": [[314, "ttnn.sin_bw"]], "sinh (in module ttnn)": [[315, "ttnn.sinh"]], "sinh_bw (in module ttnn)": [[316, "ttnn.sinh_bw"]], "slice (in module ttnn)": [[317, "ttnn.slice"]], "softmax (in module ttnn)": [[318, "ttnn.softmax"]], "softplus (in module ttnn)": [[319, "ttnn.softplus"]], "softplus_bw (in module ttnn)": [[320, "ttnn.softplus_bw"]], "softshrink (in module ttnn)": [[321, "ttnn.softshrink"]], "softshrink_bw (in module ttnn)": [[322, "ttnn.softshrink_bw"]], "softsign (in module ttnn)": [[323, "ttnn.softsign"]], "softsign_bw (in module ttnn)": [[324, "ttnn.softsign_bw"]], "sqrt (in module ttnn)": [[325, "ttnn.sqrt"]], "sqrt_bw (in module ttnn)": [[326, "ttnn.sqrt_bw"]], "square (in module ttnn)": [[327, "ttnn.square"]], "square_bw (in module ttnn)": [[328, "ttnn.square_bw"]], "squared_difference (in module ttnn)": [[329, "ttnn.squared_difference"]], "squared_difference_bw (in module ttnn)": [[330, "ttnn.squared_difference_bw"]], "std (in module ttnn)": [[331, "ttnn.std"]], "sub_bw (in module ttnn)": [[332, "ttnn.sub_bw"]], "subalpha (in module ttnn)": [[333, "ttnn.subalpha"]], "subalpha_bw (in module ttnn)": [[334, "ttnn.subalpha_bw"]], "subtract (in module ttnn)": [[335, "ttnn.subtract"]], "sum (in module ttnn)": [[336, "ttnn.sum"]], "swiglu (in module ttnn)": [[337, "ttnn.swiglu"]], "swish (in module ttnn)": [[338, "ttnn.swish"]], "synchronize_device() (in module ttnn)": [[339, "ttnn.synchronize_device"]], "tan (in module ttnn)": [[340, "ttnn.tan"]], "tan_bw (in module ttnn)": [[341, "ttnn.tan_bw"]], "tanh (in module ttnn)": [[342, "ttnn.tanh"]], "tanh_bw (in module ttnn)": [[343, "ttnn.tanh_bw"]], "tanhshrink (in module ttnn)": [[344, "ttnn.tanhshrink"]], "tanhshrink_bw (in module ttnn)": [[345, "ttnn.tanhshrink_bw"]], "threshold (in module ttnn)": [[346, "ttnn.threshold"]], "threshold_bw (in module ttnn)": [[347, "ttnn.threshold_bw"]], "tilize (in module ttnn)": [[348, "ttnn.tilize"]], "tilize_with_val_padding (in module ttnn)": [[349, "ttnn.tilize_with_val_padding"]], "to_device (in module ttnn)": [[350, "ttnn.to_device"]], "to_layout (in module ttnn)": [[351, "ttnn.to_layout"]], "to_memory_config (in module ttnn)": [[352, "ttnn.to_memory_config"]], "to_torch (in module ttnn)": [[353, "ttnn.to_torch"]], "topk (in module ttnn)": [[354, "ttnn.topk"]], "attention_softmax (in module ttnn.transformer)": [[355, "ttnn.transformer.attention_softmax"]], "attention_softmax_ (in module ttnn.transformer)": [[356, "ttnn.transformer.attention_softmax_"]], "concatenate_heads (in module ttnn.transformer)": [[357, "ttnn.transformer.concatenate_heads"]], "scaled_dot_product_attention (in module ttnn.transformer)": [[358, "ttnn.transformer.scaled_dot_product_attention"]], "scaled_dot_product_attention_decode (in module ttnn.transformer)": [[359, "ttnn.transformer.scaled_dot_product_attention_decode"]], "split_query_key_value_and_split_heads (in module ttnn.transformer)": [[360, "ttnn.transformer.split_query_key_value_and_split_heads"]], "tril (in module ttnn)": [[361, "ttnn.tril"]], "triu (in module ttnn)": [[362, "ttnn.triu"]], "trunc (in module ttnn)": [[363, "ttnn.trunc"]], "trunc_bw (in module ttnn)": [[364, "ttnn.trunc_bw"]], "unary_chain (in module ttnn)": [[365, "ttnn.unary_chain"]], "untilize (in module ttnn)": [[366, "ttnn.untilize"]], "untilize_with_unpadding (in module ttnn)": [[367, "ttnn.untilize_with_unpadding"]], "upsample (in module ttnn)": [[368, "ttnn.upsample"]], "var (in module ttnn)": [[369, "ttnn.var"]], "where (in module ttnn)": [[370, "ttnn.where"]], "where_bw (in module ttnn)": [[371, "ttnn.where_bw"]], "xlogy (in module ttnn)": [[372, "ttnn.xlogy"]], "xlogy_bw (in module ttnn)": [[373, "ttnn.xlogy_bw"]], "zeros (in module ttnn)": [[374, "ttnn.zeros"]], "zeros_like (in module ttnn)": [[375, "ttnn.zeros_like"]], "memoryconfig (class in ttnn)": [[380, "ttnn.MemoryConfig"]], "tensor (class in ttnn)": [[380, "ttnn.Tensor"]], "__init__() (ttnn.memoryconfig method)": [[380, "ttnn.MemoryConfig.__init__"]], "__init__() (ttnn.tensor method)": [[380, "ttnn.Tensor.__init__"]], "buffer() (ttnn.tensor method)": [[380, "ttnn.Tensor.buffer"]], "device() (ttnn.tensor method)": [[380, "ttnn.Tensor.device"]], "get_dtype() (ttnn.tensor method)": [[380, "ttnn.Tensor.get_dtype"]], "get_layout() (ttnn.tensor method)": [[380, "ttnn.Tensor.get_layout"]], "pad() (ttnn.tensor method)": [[380, "ttnn.Tensor.pad"]], "pad_to_tile() (ttnn.tensor method)": [[380, "ttnn.Tensor.pad_to_tile"]], "storage_type() (ttnn.tensor method)": [[380, "ttnn.Tensor.storage_type"]], "to() (ttnn.tensor method)": [[380, "ttnn.Tensor.to"]], "unpad() (ttnn.tensor method)": [[380, "ttnn.Tensor.unpad"]], "unpad_from_tile() (ttnn.tensor method)": [[380, "ttnn.Tensor.unpad_from_tile"]], "adaptiveavgpool2d (class in tt_lib.fallback_ops)": [[381, "tt_lib.fallback_ops.AdaptiveAvgPool2d"]], "addandnorm() (in module tt_lib.fused_ops.add_and_norm)": [[381, "tt_lib.fused_ops.add_and_norm.AddAndNorm"]], "batchnorm2d (class in tt_lib.fallback_ops)": [[381, "tt_lib.fallback_ops.BatchNorm2d"]], "bcastopdim (class in ttnn)": [[381, "ttnn.BcastOpDim"]], "bcastopmath (class in ttnn)": [[381, "ttnn.BcastOpMath"]], "conv2d (class in tt_lib.fallback_ops)": [[381, "tt_lib.fallback_ops.Conv2d"]], "groupnorm (class in tt_lib.fallback_ops)": [[381, "tt_lib.fallback_ops.GroupNorm"]], "layernorm (class in tt_lib.fallback_ops)": [[381, "tt_lib.fallback_ops.LayerNorm"]], "layernorm() (in module tt_lib.fused_ops.layernorm)": [[381, "tt_lib.fused_ops.layernorm.Layernorm"]], "linear() (in module tt_lib.fused_ops.linear)": [[381, "tt_lib.fused_ops.linear.Linear"]], "maxpool2d (class in tt_lib.fallback_ops)": [[381, "tt_lib.fallback_ops.MaxPool2d"]], "binary_bitwise_and (class in tt_lib.fallback_ops)": [[381, "tt_lib.fallback_ops.binary_bitwise_and"]], "binary_bitwise_left_shift (class in tt_lib.fallback_ops)": [[381, "tt_lib.fallback_ops.binary_bitwise_left_shift"]], "binary_bitwise_or (class in tt_lib.fallback_ops)": [[381, "tt_lib.fallback_ops.binary_bitwise_or"]], "binary_bitwise_right_shift (class in tt_lib.fallback_ops)": [[381, "tt_lib.fallback_ops.binary_bitwise_right_shift"]], "binary_bitwise_xor (class in tt_lib.fallback_ops)": [[381, "tt_lib.fallback_ops.binary_bitwise_xor"]], "binary_fmod (class in tt_lib.fallback_ops)": [[381, "tt_lib.fallback_ops.binary_fmod"]], "bitwise_not (class in tt_lib.fallback_ops)": [[381, "tt_lib.fallback_ops.bitwise_not"]], "ceil (class in tt_lib.fallback_ops)": [[381, "tt_lib.fallback_ops.ceil"]], "chunk() (in module tt_lib.fallback_ops)": [[381, "tt_lib.fallback_ops.chunk"]], "concat() (in module tt_lib.fallback_ops)": [[381, "tt_lib.fallback_ops.concat"]], "conv2d() (in module tt_lib.fallback_ops)": [[381, "tt_lib.fallback_ops.conv2d"]], "cpu() (in module ttnn.tensor)": [[381, "ttnn.Tensor.cpu"]], "floor (class in tt_lib.fallback_ops)": [[381, "tt_lib.fallback_ops.floor"]], "full() (in module tt_lib.fallback_ops)": [[381, "tt_lib.fallback_ops.full"]], "group_norm() (in module tt_lib.fallback_ops)": [[381, "tt_lib.fallback_ops.group_norm"]], "group_norm() (in module ttnn.operations.moreh)": [[381, "ttnn.operations.moreh.group_norm"]], "group_norm_backward() (in module ttnn.operations.moreh)": [[381, "ttnn.operations.moreh.group_norm_backward"]], "interpolate() (in module tt_lib.fallback_ops)": [[381, "tt_lib.fallback_ops.interpolate"]], "layer_norm() (in module tt_lib.fallback_ops)": [[381, "tt_lib.fallback_ops.layer_norm"]], "logsoftmax() (in module ttnn.operations.moreh)": [[381, "ttnn.operations.moreh.logsoftmax"]], "logsoftmax_backward() (in module ttnn.operations.moreh)": [[381, "ttnn.operations.moreh.logsoftmax_backward"]], "mean() (in module ttnn.operations.moreh)": [[381, "ttnn.operations.moreh.mean"]], "mean_backward() (in module ttnn.operations.moreh)": [[381, "ttnn.operations.moreh.mean_backward"]], "norm() (in module ttnn.operations.moreh)": [[381, "ttnn.operations.moreh.norm"]], "norm_backward() (in module ttnn.operations.moreh)": [[381, "ttnn.operations.moreh.norm_backward"]], "pad() (in module tt_lib.fallback_ops)": [[381, "tt_lib.fallback_ops.pad"]], "repeat() (in module tt_lib.fallback_ops)": [[381, "tt_lib.fallback_ops.repeat"]], "repeat_interleave() (in module tt_lib.fallback_ops)": [[381, "tt_lib.fallback_ops.repeat_interleave"]], "reshape() (in module tt_lib.fallback_ops)": [[381, "tt_lib.fallback_ops.reshape"]], "silu() (in module tt_lib.fallback_ops)": [[381, "tt_lib.fallback_ops.silu"]], "softmax() (in module tt_lib.fallback_ops)": [[381, "tt_lib.fallback_ops.softmax"]], "softmax() (in module ttnn.operations.moreh)": [[381, "ttnn.operations.moreh.softmax"]], "softmax_backward() (in module ttnn.operations.moreh)": [[381, "ttnn.operations.moreh.softmax_backward"]], "softmin() (in module ttnn.operations.moreh)": [[381, "ttnn.operations.moreh.softmin"]], "softmin_backward() (in module ttnn.operations.moreh)": [[381, "ttnn.operations.moreh.softmin_backward"]], "tensor_slice() (in module tt_lib.fallback_ops)": [[381, "tt_lib.fallback_ops.tensor_slice"]], "torch_argmax (class in tt_lib.fallback_ops)": [[381, "tt_lib.fallback_ops.torch_argmax"]], "torch_argmin (class in tt_lib.fallback_ops)": [[381, "tt_lib.fallback_ops.torch_argmin"]], "trunc (class in tt_lib.fallback_ops)": [[381, "tt_lib.fallback_ops.trunc"]], "unary_bitwise_and (class in tt_lib.fallback_ops)": [[381, "tt_lib.fallback_ops.unary_bitwise_and"]], "unary_bitwise_left_shift (class in tt_lib.fallback_ops)": [[381, "tt_lib.fallback_ops.unary_bitwise_left_shift"]], "unary_bitwise_or (class in tt_lib.fallback_ops)": [[381, "tt_lib.fallback_ops.unary_bitwise_or"]], "unary_bitwise_right_shift (class in tt_lib.fallback_ops)": [[381, "tt_lib.fallback_ops.unary_bitwise_right_shift"]], "unary_bitwise_xor (class in tt_lib.fallback_ops)": [[381, "tt_lib.fallback_ops.unary_bitwise_xor"]], "unary_fmod (class in tt_lib.fallback_ops)": [[381, "tt_lib.fallback_ops.unary_fmod"]], "shape (class in ttnn)": [[386, "ttnn.Shape"]], "rank (ttnn.shape property)": [[386, "ttnn.Shape.rank"]], "to_rank() (ttnn.shape method)": [[386, "ttnn.Shape.to_rank"]]}})