Search.setIndex({"docnames": ["index", "resources/contributing", "resources/support", "tt_metal_models/get_performance", "tt_metal_models/get_started", "ttnn/about", "ttnn/adding_new_ttnn_operation", "ttnn/api", "ttnn/api/ttnn.GetDefaultDevice", "ttnn/api/ttnn.SetDefaultDevice", "ttnn/api/ttnn.abs", "ttnn/api/ttnn.abs_bw", "ttnn/api/ttnn.acos", "ttnn/api/ttnn.acos_bw", "ttnn/api/ttnn.acosh", "ttnn/api/ttnn.acosh_bw", "ttnn/api/ttnn.add", "ttnn/api/ttnn.add_bw", "ttnn/api/ttnn.addalpha", "ttnn/api/ttnn.addalpha_bw", "ttnn/api/ttnn.addcdiv", "ttnn/api/ttnn.addcdiv_bw", "ttnn/api/ttnn.addcmul", "ttnn/api/ttnn.addcmul_bw", "ttnn/api/ttnn.all_gather", "ttnn/api/ttnn.angle", "ttnn/api/ttnn.angle_bw", "ttnn/api/ttnn.arange", "ttnn/api/ttnn.argmax", "ttnn/api/ttnn.as_tensor", "ttnn/api/ttnn.asin", "ttnn/api/ttnn.asin_bw", "ttnn/api/ttnn.asinh", "ttnn/api/ttnn.asinh_bw", "ttnn/api/ttnn.assign_bw", "ttnn/api/ttnn.atan", "ttnn/api/ttnn.atan2", "ttnn/api/ttnn.atan2_bw", "ttnn/api/ttnn.atan_bw", "ttnn/api/ttnn.atanh", "ttnn/api/ttnn.atanh_bw", "ttnn/api/ttnn.batch_norm", "ttnn/api/ttnn.bias_gelu_bw", "ttnn/api/ttnn.bitwise_and", "ttnn/api/ttnn.bitwise_left_shift", "ttnn/api/ttnn.bitwise_not", "ttnn/api/ttnn.bitwise_or", "ttnn/api/ttnn.bitwise_right_shift", "ttnn/api/ttnn.bitwise_xor", "ttnn/api/ttnn.cbrt", "ttnn/api/ttnn.ceil", "ttnn/api/ttnn.ceil_bw", "ttnn/api/ttnn.celu", "ttnn/api/ttnn.celu_bw", "ttnn/api/ttnn.clamp", "ttnn/api/ttnn.clamp_bw", "ttnn/api/ttnn.clip", "ttnn/api/ttnn.clip_bw", "ttnn/api/ttnn.clone", "ttnn/api/ttnn.close_device", "ttnn/api/ttnn.concat", "ttnn/api/ttnn.concat_bw", "ttnn/api/ttnn.conj", "ttnn/api/ttnn.conj_bw", "ttnn/api/ttnn.cos", "ttnn/api/ttnn.cos_bw", "ttnn/api/ttnn.cosh", "ttnn/api/ttnn.cosh_bw", "ttnn/api/ttnn.create_sharded_memory_config", "ttnn/api/ttnn.deallocate", "ttnn/api/ttnn.deg2rad", "ttnn/api/ttnn.deg2rad_bw", "ttnn/api/ttnn.digamma", "ttnn/api/ttnn.digamma_bw", "ttnn/api/ttnn.div", "ttnn/api/ttnn.div_bw", "ttnn/api/ttnn.div_no_nan", "ttnn/api/ttnn.div_no_nan_bw", "ttnn/api/ttnn.downsample", "ttnn/api/ttnn.dump_tensor", "ttnn/api/ttnn.elu", "ttnn/api/ttnn.elu_bw", "ttnn/api/ttnn.embedding", "ttnn/api/ttnn.embedding_bw", "ttnn/api/ttnn.empty", "ttnn/api/ttnn.empty_like", "ttnn/api/ttnn.eq", "ttnn/api/ttnn.eq_", "ttnn/api/ttnn.eqz", "ttnn/api/ttnn.erf", "ttnn/api/ttnn.erf_bw", "ttnn/api/ttnn.erfc", "ttnn/api/ttnn.erfc_bw", "ttnn/api/ttnn.erfinv", "ttnn/api/ttnn.erfinv_bw", "ttnn/api/ttnn.exp", "ttnn/api/ttnn.exp2", "ttnn/api/ttnn.exp2_bw", "ttnn/api/ttnn.exp_bw", "ttnn/api/ttnn.experimental.all_reduce", "ttnn/api/ttnn.experimental.cumprod", "ttnn/api/ttnn.experimental.dropout", "ttnn/api/ttnn.experimental.gelu_bw", "ttnn/api/ttnn.experimental.rotary_embedding", "ttnn/api/ttnn.experimental.sort", "ttnn/api/ttnn.expm1", "ttnn/api/ttnn.expm1_bw", "ttnn/api/ttnn.fill", "ttnn/api/ttnn.fill_bw", "ttnn/api/ttnn.fill_ones_rm", "ttnn/api/ttnn.fill_rm", "ttnn/api/ttnn.fill_zero_bw", "ttnn/api/ttnn.floor", "ttnn/api/ttnn.floor_bw", "ttnn/api/ttnn.floor_div", "ttnn/api/ttnn.fmod", "ttnn/api/ttnn.fmod_bw", "ttnn/api/ttnn.format_input_tensor", "ttnn/api/ttnn.format_output_tensor", "ttnn/api/ttnn.frac", "ttnn/api/ttnn.frac_bw", "ttnn/api/ttnn.from_device", "ttnn/api/ttnn.from_torch", "ttnn/api/ttnn.full", "ttnn/api/ttnn.full_like", "ttnn/api/ttnn.gcd", "ttnn/api/ttnn.ge", "ttnn/api/ttnn.ge_", "ttnn/api/ttnn.geglu", "ttnn/api/ttnn.gelu", "ttnn/api/ttnn.gelu_bw", "ttnn/api/ttnn.gez", "ttnn/api/ttnn.global_avg_pool2d", "ttnn/api/ttnn.glu", "ttnn/api/ttnn.group_norm", "ttnn/api/ttnn.gt", "ttnn/api/ttnn.gt_", "ttnn/api/ttnn.gtz", "ttnn/api/ttnn.hardshrink", "ttnn/api/ttnn.hardshrink_bw", "ttnn/api/ttnn.hardsigmoid", "ttnn/api/ttnn.hardsigmoid_bw", "ttnn/api/ttnn.hardswish", "ttnn/api/ttnn.hardswish_bw", "ttnn/api/ttnn.hardtanh", "ttnn/api/ttnn.hardtanh_bw", "ttnn/api/ttnn.heaviside", "ttnn/api/ttnn.hypot", "ttnn/api/ttnn.hypot_bw", "ttnn/api/ttnn.i0", "ttnn/api/ttnn.i0_bw", "ttnn/api/ttnn.identity", "ttnn/api/ttnn.imag", "ttnn/api/ttnn.imag_bw", "ttnn/api/ttnn.indexed_fill", "ttnn/api/ttnn.is_imag", "ttnn/api/ttnn.is_real", "ttnn/api/ttnn.isclose", "ttnn/api/ttnn.isfinite", "ttnn/api/ttnn.isinf", "ttnn/api/ttnn.isnan", "ttnn/api/ttnn.isneginf", "ttnn/api/ttnn.isposinf", "ttnn/api/ttnn.kv_cache.fill_cache_for_user_", "ttnn/api/ttnn.kv_cache.update_cache_for_token_", "ttnn/api/ttnn.l1_loss", "ttnn/api/ttnn.layer_norm", "ttnn/api/ttnn.lcm", "ttnn/api/ttnn.ldexp", "ttnn/api/ttnn.ldexp_bw", "ttnn/api/ttnn.le", "ttnn/api/ttnn.le_", "ttnn/api/ttnn.leaky_relu", "ttnn/api/ttnn.leaky_relu_bw", "ttnn/api/ttnn.lerp", "ttnn/api/ttnn.lerp_bw", "ttnn/api/ttnn.lez", "ttnn/api/ttnn.lgamma", "ttnn/api/ttnn.lgamma_bw", "ttnn/api/ttnn.linear", "ttnn/api/ttnn.load_tensor", "ttnn/api/ttnn.log", "ttnn/api/ttnn.log10", "ttnn/api/ttnn.log10_bw", "ttnn/api/ttnn.log1p", "ttnn/api/ttnn.log1p_bw", "ttnn/api/ttnn.log2", "ttnn/api/ttnn.log2_bw", "ttnn/api/ttnn.log_bw", "ttnn/api/ttnn.log_sigmoid", "ttnn/api/ttnn.log_sigmoid_bw", "ttnn/api/ttnn.logaddexp", "ttnn/api/ttnn.logaddexp2", "ttnn/api/ttnn.logaddexp2_bw", "ttnn/api/ttnn.logaddexp_bw", "ttnn/api/ttnn.logical_and", "ttnn/api/ttnn.logical_and_", "ttnn/api/ttnn.logical_not", "ttnn/api/ttnn.logical_not_", "ttnn/api/ttnn.logical_or", "ttnn/api/ttnn.logical_or_", "ttnn/api/ttnn.logical_xor", "ttnn/api/ttnn.logical_xor_", "ttnn/api/ttnn.logit", "ttnn/api/ttnn.logit_bw", "ttnn/api/ttnn.logiteps_bw", "ttnn/api/ttnn.lt", "ttnn/api/ttnn.lt_", "ttnn/api/ttnn.ltz", "ttnn/api/ttnn.mac", "ttnn/api/ttnn.manage_device", "ttnn/api/ttnn.matmul", "ttnn/api/ttnn.max", "ttnn/api/ttnn.max_bw", "ttnn/api/ttnn.max_pool2d", "ttnn/api/ttnn.maximum", "ttnn/api/ttnn.mean", "ttnn/api/ttnn.min", "ttnn/api/ttnn.min_bw", "ttnn/api/ttnn.minimum", "ttnn/api/ttnn.mish", "ttnn/api/ttnn.model_preprocessing.preprocess_model", "ttnn/api/ttnn.model_preprocessing.preprocess_model_parameters", "ttnn/api/ttnn.moreh_sum", "ttnn/api/ttnn.mse_loss", "ttnn/api/ttnn.mul_bw", "ttnn/api/ttnn.multigammaln", "ttnn/api/ttnn.multigammaln_bw", "ttnn/api/ttnn.multiply", "ttnn/api/ttnn.ne", "ttnn/api/ttnn.ne_", "ttnn/api/ttnn.neg", "ttnn/api/ttnn.neg_bw", "ttnn/api/ttnn.nextafter", "ttnn/api/ttnn.nez", "ttnn/api/ttnn.nonzero", "ttnn/api/ttnn.normalize_global", "ttnn/api/ttnn.normalize_hw", "ttnn/api/ttnn.ones", "ttnn/api/ttnn.ones_like", "ttnn/api/ttnn.open_device", "ttnn/api/ttnn.outer", "ttnn/api/ttnn.pad", "ttnn/api/ttnn.pad_to_tile_shape", "ttnn/api/ttnn.permute", "ttnn/api/ttnn.polar", "ttnn/api/ttnn.polar_bw", "ttnn/api/ttnn.polygamma", "ttnn/api/ttnn.polygamma_bw", "ttnn/api/ttnn.polyval", "ttnn/api/ttnn.pow", "ttnn/api/ttnn.pow_bw", "ttnn/api/ttnn.prelu", "ttnn/api/ttnn.prod", "ttnn/api/ttnn.prod_bw", "ttnn/api/ttnn.rad2deg", "ttnn/api/ttnn.rad2deg_bw", "ttnn/api/ttnn.rdiv", "ttnn/api/ttnn.rdiv_bw", "ttnn/api/ttnn.real", "ttnn/api/ttnn.real_bw", "ttnn/api/ttnn.reallocate", "ttnn/api/ttnn.reciprocal", "ttnn/api/ttnn.reciprocal_bw", "ttnn/api/ttnn.reduce_scatter", "ttnn/api/ttnn.register_post_operation_hook", "ttnn/api/ttnn.register_pre_operation_hook", "ttnn/api/ttnn.reglu", "ttnn/api/ttnn.relu", "ttnn/api/ttnn.relu6", "ttnn/api/ttnn.relu6_bw", "ttnn/api/ttnn.relu_bw", "ttnn/api/ttnn.relu_max", "ttnn/api/ttnn.relu_min", "ttnn/api/ttnn.remainder", "ttnn/api/ttnn.remainder_bw", "ttnn/api/ttnn.repeat", "ttnn/api/ttnn.repeat_bw", "ttnn/api/ttnn.repeat_interleave", "ttnn/api/ttnn.reshape", "ttnn/api/ttnn.rms_norm", "ttnn/api/ttnn.round", "ttnn/api/ttnn.round_bw", "ttnn/api/ttnn.rpow", "ttnn/api/ttnn.rpow_bw", "ttnn/api/ttnn.rsqrt", "ttnn/api/ttnn.rsqrt_bw", "ttnn/api/ttnn.rsub", "ttnn/api/ttnn.rsub_bw", "ttnn/api/ttnn.scatter", "ttnn/api/ttnn.selu", "ttnn/api/ttnn.selu_bw", "ttnn/api/ttnn.set_printoptions", "ttnn/api/ttnn.sigmoid", "ttnn/api/ttnn.sigmoid_accurate", "ttnn/api/ttnn.sigmoid_bw", "ttnn/api/ttnn.sign", "ttnn/api/ttnn.sign_bw", "ttnn/api/ttnn.signbit", "ttnn/api/ttnn.silu", "ttnn/api/ttnn.silu_bw", "ttnn/api/ttnn.sin", "ttnn/api/ttnn.sin_bw", "ttnn/api/ttnn.sinh", "ttnn/api/ttnn.sinh_bw", "ttnn/api/ttnn.slice", "ttnn/api/ttnn.softmax", "ttnn/api/ttnn.softplus", "ttnn/api/ttnn.softplus_bw", "ttnn/api/ttnn.softshrink", "ttnn/api/ttnn.softshrink_bw", "ttnn/api/ttnn.softsign", "ttnn/api/ttnn.softsign_bw", "ttnn/api/ttnn.sqrt", "ttnn/api/ttnn.sqrt_bw", "ttnn/api/ttnn.square", "ttnn/api/ttnn.square_bw", "ttnn/api/ttnn.squared_difference", "ttnn/api/ttnn.squared_difference_bw", "ttnn/api/ttnn.std", "ttnn/api/ttnn.sub_bw", "ttnn/api/ttnn.subalpha", "ttnn/api/ttnn.subalpha_bw", "ttnn/api/ttnn.subtract", "ttnn/api/ttnn.sum", "ttnn/api/ttnn.swiglu", "ttnn/api/ttnn.swish", "ttnn/api/ttnn.synchronize_device", "ttnn/api/ttnn.tan", "ttnn/api/ttnn.tan_bw", "ttnn/api/ttnn.tanh", "ttnn/api/ttnn.tanh_bw", "ttnn/api/ttnn.tanhshrink", "ttnn/api/ttnn.tanhshrink_bw", "ttnn/api/ttnn.threshold", "ttnn/api/ttnn.threshold_bw", "ttnn/api/ttnn.tilize", "ttnn/api/ttnn.tilize_with_val_padding", "ttnn/api/ttnn.to_device", "ttnn/api/ttnn.to_layout", "ttnn/api/ttnn.to_memory_config", "ttnn/api/ttnn.to_torch", "ttnn/api/ttnn.topk", "ttnn/api/ttnn.transformer.attention_softmax", "ttnn/api/ttnn.transformer.attention_softmax_", "ttnn/api/ttnn.transformer.concatenate_heads", "ttnn/api/ttnn.transformer.scaled_dot_product_attention", "ttnn/api/ttnn.transformer.scaled_dot_product_attention_decode", "ttnn/api/ttnn.transformer.split_query_key_value_and_split_heads", "ttnn/api/ttnn.tril", "ttnn/api/ttnn.triu", "ttnn/api/ttnn.trunc", "ttnn/api/ttnn.trunc_bw", "ttnn/api/ttnn.unary_chain", "ttnn/api/ttnn.untilize", "ttnn/api/ttnn.untilize_with_unpadding", "ttnn/api/ttnn.upsample", "ttnn/api/ttnn.var", "ttnn/api/ttnn.where", "ttnn/api/ttnn.where_bw", "ttnn/api/ttnn.xlogy", "ttnn/api/ttnn.xlogy_bw", "ttnn/api/ttnn.zeros", "ttnn/api/ttnn.zeros_like", "ttnn/converting_torch_model_to_ttnn", "ttnn/demos", "ttnn/dependencies/examples", "ttnn/dependencies/index", "ttnn/dependencies/tensor", "ttnn/dependencies/tt_lib", "ttnn/get_started", "ttnn/installing", "ttnn/onboarding", "ttnn/profiling_ttnn_operations", "ttnn/tensor", "ttnn/tutorials", "ttnn/tutorials/graphing_torch_dit", "ttnn/tutorials/matmul", "ttnn/tutorials/multihead-attention", "ttnn/tutorials/profiling", "ttnn/tutorials/resnet-basic-block", "ttnn/tutorials/tensor_and_add_operation", "ttnn/tutorials/ttnn-tracer", "ttnn/tutorials/ttnn_tutorials/001", "ttnn/tutorials/ttnn_tutorials/002", "ttnn/tutorials/ttnn_tutorials/003", "ttnn/tutorials/ttnn_tutorials/004", "ttnn/tutorials/ttnn_tutorials/005", "ttnn/tutorials/ttnn_tutorials/006", "ttnn/tutorials/ttnn_tutorials/007", "ttnn/usage"], "filenames": ["index.rst", "resources/contributing.rst", "resources/support.rst", "tt_metal_models/get_performance.rst", "tt_metal_models/get_started.rst", "ttnn/about.rst", "ttnn/adding_new_ttnn_operation.rst", "ttnn/api.rst", "ttnn/api/ttnn.GetDefaultDevice.rst", "ttnn/api/ttnn.SetDefaultDevice.rst", "ttnn/api/ttnn.abs.rst", "ttnn/api/ttnn.abs_bw.rst", "ttnn/api/ttnn.acos.rst", "ttnn/api/ttnn.acos_bw.rst", "ttnn/api/ttnn.acosh.rst", "ttnn/api/ttnn.acosh_bw.rst", "ttnn/api/ttnn.add.rst", "ttnn/api/ttnn.add_bw.rst", "ttnn/api/ttnn.addalpha.rst", "ttnn/api/ttnn.addalpha_bw.rst", "ttnn/api/ttnn.addcdiv.rst", "ttnn/api/ttnn.addcdiv_bw.rst", "ttnn/api/ttnn.addcmul.rst", "ttnn/api/ttnn.addcmul_bw.rst", "ttnn/api/ttnn.all_gather.rst", "ttnn/api/ttnn.angle.rst", "ttnn/api/ttnn.angle_bw.rst", "ttnn/api/ttnn.arange.rst", "ttnn/api/ttnn.argmax.rst", "ttnn/api/ttnn.as_tensor.rst", "ttnn/api/ttnn.asin.rst", "ttnn/api/ttnn.asin_bw.rst", "ttnn/api/ttnn.asinh.rst", "ttnn/api/ttnn.asinh_bw.rst", "ttnn/api/ttnn.assign_bw.rst", "ttnn/api/ttnn.atan.rst", "ttnn/api/ttnn.atan2.rst", "ttnn/api/ttnn.atan2_bw.rst", "ttnn/api/ttnn.atan_bw.rst", "ttnn/api/ttnn.atanh.rst", "ttnn/api/ttnn.atanh_bw.rst", "ttnn/api/ttnn.batch_norm.rst", "ttnn/api/ttnn.bias_gelu_bw.rst", "ttnn/api/ttnn.bitwise_and.rst", "ttnn/api/ttnn.bitwise_left_shift.rst", "ttnn/api/ttnn.bitwise_not.rst", "ttnn/api/ttnn.bitwise_or.rst", "ttnn/api/ttnn.bitwise_right_shift.rst", "ttnn/api/ttnn.bitwise_xor.rst", "ttnn/api/ttnn.cbrt.rst", "ttnn/api/ttnn.ceil.rst", "ttnn/api/ttnn.ceil_bw.rst", "ttnn/api/ttnn.celu.rst", "ttnn/api/ttnn.celu_bw.rst", "ttnn/api/ttnn.clamp.rst", "ttnn/api/ttnn.clamp_bw.rst", "ttnn/api/ttnn.clip.rst", "ttnn/api/ttnn.clip_bw.rst", "ttnn/api/ttnn.clone.rst", "ttnn/api/ttnn.close_device.rst", "ttnn/api/ttnn.concat.rst", "ttnn/api/ttnn.concat_bw.rst", "ttnn/api/ttnn.conj.rst", "ttnn/api/ttnn.conj_bw.rst", "ttnn/api/ttnn.cos.rst", "ttnn/api/ttnn.cos_bw.rst", "ttnn/api/ttnn.cosh.rst", "ttnn/api/ttnn.cosh_bw.rst", "ttnn/api/ttnn.create_sharded_memory_config.rst", "ttnn/api/ttnn.deallocate.rst", "ttnn/api/ttnn.deg2rad.rst", "ttnn/api/ttnn.deg2rad_bw.rst", "ttnn/api/ttnn.digamma.rst", "ttnn/api/ttnn.digamma_bw.rst", "ttnn/api/ttnn.div.rst", "ttnn/api/ttnn.div_bw.rst", "ttnn/api/ttnn.div_no_nan.rst", "ttnn/api/ttnn.div_no_nan_bw.rst", "ttnn/api/ttnn.downsample.rst", "ttnn/api/ttnn.dump_tensor.rst", "ttnn/api/ttnn.elu.rst", "ttnn/api/ttnn.elu_bw.rst", "ttnn/api/ttnn.embedding.rst", "ttnn/api/ttnn.embedding_bw.rst", "ttnn/api/ttnn.empty.rst", "ttnn/api/ttnn.empty_like.rst", "ttnn/api/ttnn.eq.rst", "ttnn/api/ttnn.eq_.rst", "ttnn/api/ttnn.eqz.rst", "ttnn/api/ttnn.erf.rst", "ttnn/api/ttnn.erf_bw.rst", "ttnn/api/ttnn.erfc.rst", "ttnn/api/ttnn.erfc_bw.rst", "ttnn/api/ttnn.erfinv.rst", "ttnn/api/ttnn.erfinv_bw.rst", "ttnn/api/ttnn.exp.rst", "ttnn/api/ttnn.exp2.rst", "ttnn/api/ttnn.exp2_bw.rst", "ttnn/api/ttnn.exp_bw.rst", "ttnn/api/ttnn.experimental.all_reduce.rst", "ttnn/api/ttnn.experimental.cumprod.rst", "ttnn/api/ttnn.experimental.dropout.rst", "ttnn/api/ttnn.experimental.gelu_bw.rst", "ttnn/api/ttnn.experimental.rotary_embedding.rst", "ttnn/api/ttnn.experimental.sort.rst", "ttnn/api/ttnn.expm1.rst", "ttnn/api/ttnn.expm1_bw.rst", "ttnn/api/ttnn.fill.rst", "ttnn/api/ttnn.fill_bw.rst", "ttnn/api/ttnn.fill_ones_rm.rst", "ttnn/api/ttnn.fill_rm.rst", "ttnn/api/ttnn.fill_zero_bw.rst", "ttnn/api/ttnn.floor.rst", "ttnn/api/ttnn.floor_bw.rst", "ttnn/api/ttnn.floor_div.rst", "ttnn/api/ttnn.fmod.rst", "ttnn/api/ttnn.fmod_bw.rst", "ttnn/api/ttnn.format_input_tensor.rst", "ttnn/api/ttnn.format_output_tensor.rst", "ttnn/api/ttnn.frac.rst", "ttnn/api/ttnn.frac_bw.rst", "ttnn/api/ttnn.from_device.rst", "ttnn/api/ttnn.from_torch.rst", "ttnn/api/ttnn.full.rst", "ttnn/api/ttnn.full_like.rst", "ttnn/api/ttnn.gcd.rst", "ttnn/api/ttnn.ge.rst", "ttnn/api/ttnn.ge_.rst", "ttnn/api/ttnn.geglu.rst", "ttnn/api/ttnn.gelu.rst", "ttnn/api/ttnn.gelu_bw.rst", "ttnn/api/ttnn.gez.rst", "ttnn/api/ttnn.global_avg_pool2d.rst", "ttnn/api/ttnn.glu.rst", "ttnn/api/ttnn.group_norm.rst", "ttnn/api/ttnn.gt.rst", "ttnn/api/ttnn.gt_.rst", "ttnn/api/ttnn.gtz.rst", "ttnn/api/ttnn.hardshrink.rst", "ttnn/api/ttnn.hardshrink_bw.rst", "ttnn/api/ttnn.hardsigmoid.rst", "ttnn/api/ttnn.hardsigmoid_bw.rst", "ttnn/api/ttnn.hardswish.rst", "ttnn/api/ttnn.hardswish_bw.rst", "ttnn/api/ttnn.hardtanh.rst", "ttnn/api/ttnn.hardtanh_bw.rst", "ttnn/api/ttnn.heaviside.rst", "ttnn/api/ttnn.hypot.rst", "ttnn/api/ttnn.hypot_bw.rst", "ttnn/api/ttnn.i0.rst", "ttnn/api/ttnn.i0_bw.rst", "ttnn/api/ttnn.identity.rst", "ttnn/api/ttnn.imag.rst", "ttnn/api/ttnn.imag_bw.rst", "ttnn/api/ttnn.indexed_fill.rst", "ttnn/api/ttnn.is_imag.rst", "ttnn/api/ttnn.is_real.rst", "ttnn/api/ttnn.isclose.rst", "ttnn/api/ttnn.isfinite.rst", "ttnn/api/ttnn.isinf.rst", "ttnn/api/ttnn.isnan.rst", "ttnn/api/ttnn.isneginf.rst", "ttnn/api/ttnn.isposinf.rst", "ttnn/api/ttnn.kv_cache.fill_cache_for_user_.rst", "ttnn/api/ttnn.kv_cache.update_cache_for_token_.rst", "ttnn/api/ttnn.l1_loss.rst", "ttnn/api/ttnn.layer_norm.rst", "ttnn/api/ttnn.lcm.rst", "ttnn/api/ttnn.ldexp.rst", "ttnn/api/ttnn.ldexp_bw.rst", "ttnn/api/ttnn.le.rst", "ttnn/api/ttnn.le_.rst", "ttnn/api/ttnn.leaky_relu.rst", "ttnn/api/ttnn.leaky_relu_bw.rst", "ttnn/api/ttnn.lerp.rst", "ttnn/api/ttnn.lerp_bw.rst", "ttnn/api/ttnn.lez.rst", "ttnn/api/ttnn.lgamma.rst", "ttnn/api/ttnn.lgamma_bw.rst", "ttnn/api/ttnn.linear.rst", "ttnn/api/ttnn.load_tensor.rst", "ttnn/api/ttnn.log.rst", "ttnn/api/ttnn.log10.rst", "ttnn/api/ttnn.log10_bw.rst", "ttnn/api/ttnn.log1p.rst", "ttnn/api/ttnn.log1p_bw.rst", "ttnn/api/ttnn.log2.rst", "ttnn/api/ttnn.log2_bw.rst", "ttnn/api/ttnn.log_bw.rst", "ttnn/api/ttnn.log_sigmoid.rst", "ttnn/api/ttnn.log_sigmoid_bw.rst", "ttnn/api/ttnn.logaddexp.rst", "ttnn/api/ttnn.logaddexp2.rst", "ttnn/api/ttnn.logaddexp2_bw.rst", "ttnn/api/ttnn.logaddexp_bw.rst", "ttnn/api/ttnn.logical_and.rst", "ttnn/api/ttnn.logical_and_.rst", "ttnn/api/ttnn.logical_not.rst", "ttnn/api/ttnn.logical_not_.rst", "ttnn/api/ttnn.logical_or.rst", "ttnn/api/ttnn.logical_or_.rst", "ttnn/api/ttnn.logical_xor.rst", "ttnn/api/ttnn.logical_xor_.rst", "ttnn/api/ttnn.logit.rst", "ttnn/api/ttnn.logit_bw.rst", "ttnn/api/ttnn.logiteps_bw.rst", "ttnn/api/ttnn.lt.rst", "ttnn/api/ttnn.lt_.rst", "ttnn/api/ttnn.ltz.rst", "ttnn/api/ttnn.mac.rst", "ttnn/api/ttnn.manage_device.rst", "ttnn/api/ttnn.matmul.rst", "ttnn/api/ttnn.max.rst", "ttnn/api/ttnn.max_bw.rst", "ttnn/api/ttnn.max_pool2d.rst", "ttnn/api/ttnn.maximum.rst", "ttnn/api/ttnn.mean.rst", "ttnn/api/ttnn.min.rst", "ttnn/api/ttnn.min_bw.rst", "ttnn/api/ttnn.minimum.rst", "ttnn/api/ttnn.mish.rst", "ttnn/api/ttnn.model_preprocessing.preprocess_model.rst", "ttnn/api/ttnn.model_preprocessing.preprocess_model_parameters.rst", "ttnn/api/ttnn.moreh_sum.rst", "ttnn/api/ttnn.mse_loss.rst", "ttnn/api/ttnn.mul_bw.rst", "ttnn/api/ttnn.multigammaln.rst", "ttnn/api/ttnn.multigammaln_bw.rst", "ttnn/api/ttnn.multiply.rst", "ttnn/api/ttnn.ne.rst", "ttnn/api/ttnn.ne_.rst", "ttnn/api/ttnn.neg.rst", "ttnn/api/ttnn.neg_bw.rst", "ttnn/api/ttnn.nextafter.rst", "ttnn/api/ttnn.nez.rst", "ttnn/api/ttnn.nonzero.rst", "ttnn/api/ttnn.normalize_global.rst", "ttnn/api/ttnn.normalize_hw.rst", "ttnn/api/ttnn.ones.rst", "ttnn/api/ttnn.ones_like.rst", "ttnn/api/ttnn.open_device.rst", "ttnn/api/ttnn.outer.rst", "ttnn/api/ttnn.pad.rst", "ttnn/api/ttnn.pad_to_tile_shape.rst", "ttnn/api/ttnn.permute.rst", "ttnn/api/ttnn.polar.rst", "ttnn/api/ttnn.polar_bw.rst", "ttnn/api/ttnn.polygamma.rst", "ttnn/api/ttnn.polygamma_bw.rst", "ttnn/api/ttnn.polyval.rst", "ttnn/api/ttnn.pow.rst", "ttnn/api/ttnn.pow_bw.rst", "ttnn/api/ttnn.prelu.rst", "ttnn/api/ttnn.prod.rst", "ttnn/api/ttnn.prod_bw.rst", "ttnn/api/ttnn.rad2deg.rst", "ttnn/api/ttnn.rad2deg_bw.rst", "ttnn/api/ttnn.rdiv.rst", "ttnn/api/ttnn.rdiv_bw.rst", "ttnn/api/ttnn.real.rst", "ttnn/api/ttnn.real_bw.rst", "ttnn/api/ttnn.reallocate.rst", "ttnn/api/ttnn.reciprocal.rst", "ttnn/api/ttnn.reciprocal_bw.rst", "ttnn/api/ttnn.reduce_scatter.rst", "ttnn/api/ttnn.register_post_operation_hook.rst", "ttnn/api/ttnn.register_pre_operation_hook.rst", "ttnn/api/ttnn.reglu.rst", "ttnn/api/ttnn.relu.rst", "ttnn/api/ttnn.relu6.rst", "ttnn/api/ttnn.relu6_bw.rst", "ttnn/api/ttnn.relu_bw.rst", "ttnn/api/ttnn.relu_max.rst", "ttnn/api/ttnn.relu_min.rst", "ttnn/api/ttnn.remainder.rst", "ttnn/api/ttnn.remainder_bw.rst", "ttnn/api/ttnn.repeat.rst", "ttnn/api/ttnn.repeat_bw.rst", "ttnn/api/ttnn.repeat_interleave.rst", "ttnn/api/ttnn.reshape.rst", "ttnn/api/ttnn.rms_norm.rst", "ttnn/api/ttnn.round.rst", "ttnn/api/ttnn.round_bw.rst", "ttnn/api/ttnn.rpow.rst", "ttnn/api/ttnn.rpow_bw.rst", "ttnn/api/ttnn.rsqrt.rst", "ttnn/api/ttnn.rsqrt_bw.rst", "ttnn/api/ttnn.rsub.rst", "ttnn/api/ttnn.rsub_bw.rst", "ttnn/api/ttnn.scatter.rst", "ttnn/api/ttnn.selu.rst", "ttnn/api/ttnn.selu_bw.rst", "ttnn/api/ttnn.set_printoptions.rst", "ttnn/api/ttnn.sigmoid.rst", "ttnn/api/ttnn.sigmoid_accurate.rst", "ttnn/api/ttnn.sigmoid_bw.rst", "ttnn/api/ttnn.sign.rst", "ttnn/api/ttnn.sign_bw.rst", "ttnn/api/ttnn.signbit.rst", "ttnn/api/ttnn.silu.rst", "ttnn/api/ttnn.silu_bw.rst", "ttnn/api/ttnn.sin.rst", "ttnn/api/ttnn.sin_bw.rst", "ttnn/api/ttnn.sinh.rst", "ttnn/api/ttnn.sinh_bw.rst", "ttnn/api/ttnn.slice.rst", "ttnn/api/ttnn.softmax.rst", "ttnn/api/ttnn.softplus.rst", "ttnn/api/ttnn.softplus_bw.rst", "ttnn/api/ttnn.softshrink.rst", "ttnn/api/ttnn.softshrink_bw.rst", "ttnn/api/ttnn.softsign.rst", "ttnn/api/ttnn.softsign_bw.rst", "ttnn/api/ttnn.sqrt.rst", "ttnn/api/ttnn.sqrt_bw.rst", "ttnn/api/ttnn.square.rst", "ttnn/api/ttnn.square_bw.rst", "ttnn/api/ttnn.squared_difference.rst", "ttnn/api/ttnn.squared_difference_bw.rst", "ttnn/api/ttnn.std.rst", "ttnn/api/ttnn.sub_bw.rst", "ttnn/api/ttnn.subalpha.rst", "ttnn/api/ttnn.subalpha_bw.rst", "ttnn/api/ttnn.subtract.rst", "ttnn/api/ttnn.sum.rst", "ttnn/api/ttnn.swiglu.rst", "ttnn/api/ttnn.swish.rst", "ttnn/api/ttnn.synchronize_device.rst", "ttnn/api/ttnn.tan.rst", "ttnn/api/ttnn.tan_bw.rst", "ttnn/api/ttnn.tanh.rst", "ttnn/api/ttnn.tanh_bw.rst", "ttnn/api/ttnn.tanhshrink.rst", "ttnn/api/ttnn.tanhshrink_bw.rst", "ttnn/api/ttnn.threshold.rst", "ttnn/api/ttnn.threshold_bw.rst", "ttnn/api/ttnn.tilize.rst", "ttnn/api/ttnn.tilize_with_val_padding.rst", "ttnn/api/ttnn.to_device.rst", "ttnn/api/ttnn.to_layout.rst", "ttnn/api/ttnn.to_memory_config.rst", "ttnn/api/ttnn.to_torch.rst", "ttnn/api/ttnn.topk.rst", "ttnn/api/ttnn.transformer.attention_softmax.rst", "ttnn/api/ttnn.transformer.attention_softmax_.rst", "ttnn/api/ttnn.transformer.concatenate_heads.rst", "ttnn/api/ttnn.transformer.scaled_dot_product_attention.rst", "ttnn/api/ttnn.transformer.scaled_dot_product_attention_decode.rst", "ttnn/api/ttnn.transformer.split_query_key_value_and_split_heads.rst", "ttnn/api/ttnn.tril.rst", "ttnn/api/ttnn.triu.rst", "ttnn/api/ttnn.trunc.rst", "ttnn/api/ttnn.trunc_bw.rst", "ttnn/api/ttnn.unary_chain.rst", "ttnn/api/ttnn.untilize.rst", "ttnn/api/ttnn.untilize_with_unpadding.rst", "ttnn/api/ttnn.upsample.rst", "ttnn/api/ttnn.var.rst", "ttnn/api/ttnn.where.rst", "ttnn/api/ttnn.where_bw.rst", "ttnn/api/ttnn.xlogy.rst", "ttnn/api/ttnn.xlogy_bw.rst", "ttnn/api/ttnn.zeros.rst", "ttnn/api/ttnn.zeros_like.rst", "ttnn/converting_torch_model_to_ttnn.rst", "ttnn/demos.rst", "ttnn/dependencies/examples.rst", "ttnn/dependencies/index.rst", "ttnn/dependencies/tensor.rst", "ttnn/dependencies/tt_lib.rst", "ttnn/get_started.rst", "ttnn/installing.md", "ttnn/onboarding.rst", "ttnn/profiling_ttnn_operations.rst", "ttnn/tensor.rst", "ttnn/tutorials.rst", "ttnn/tutorials/graphing_torch_dit.rst", "ttnn/tutorials/matmul.rst", "ttnn/tutorials/multihead-attention.rst", "ttnn/tutorials/profiling.rst", "ttnn/tutorials/resnet-basic-block.rst", "ttnn/tutorials/tensor_and_add_operation.rst", "ttnn/tutorials/ttnn-tracer.rst", "ttnn/tutorials/ttnn_tutorials/001.ipynb", "ttnn/tutorials/ttnn_tutorials/002.ipynb", "ttnn/tutorials/ttnn_tutorials/003.ipynb", "ttnn/tutorials/ttnn_tutorials/004.ipynb", "ttnn/tutorials/ttnn_tutorials/005.ipynb", "ttnn/tutorials/ttnn_tutorials/006.ipynb", "ttnn/tutorials/ttnn_tutorials/007.ipynb", "ttnn/usage.rst"], "titles": ["Welcome to TT-NN documentation!", "Contributing as a developer", "Support", "Performance", "Getting Started", "What is TT-NN?", "Adding New TT-NN Operation", "APIs", "ttnn.GetDefaultDevice", "ttnn.SetDefaultDevice", "ttnn.abs", "ttnn.abs_bw", "ttnn.acos", "ttnn.acos_bw", "ttnn.acosh", "ttnn.acosh_bw", "ttnn.add", "ttnn.add_bw", "ttnn.addalpha", "ttnn.addalpha_bw", "ttnn.addcdiv", "ttnn.addcdiv_bw", "ttnn.addcmul", "ttnn.addcmul_bw", "ttnn.all_gather", "ttnn.angle", "ttnn.angle_bw", "ttnn.arange", "ttnn.argmax", "ttnn.as_tensor", "ttnn.asin", "ttnn.asin_bw", "ttnn.asinh", "ttnn.asinh_bw", "ttnn.assign_bw", "ttnn.atan", "ttnn.atan2", "ttnn.atan2_bw", "ttnn.atan_bw", "ttnn.atanh", "ttnn.atanh_bw", "ttnn.batch_norm", "ttnn.bias_gelu_bw", "ttnn.bitwise_and", "ttnn.bitwise_left_shift", "ttnn.bitwise_not", "ttnn.bitwise_or", "ttnn.bitwise_right_shift", "ttnn.bitwise_xor", "ttnn.cbrt", "ttnn.ceil", "ttnn.ceil_bw", "ttnn.celu", "ttnn.celu_bw", "ttnn.clamp", "ttnn.clamp_bw", "ttnn.clip", "ttnn.clip_bw", "ttnn.clone", "ttnn.close_device", "ttnn.concat", "ttnn.concat_bw", "ttnn.conj", "ttnn.conj_bw", "ttnn.cos", "ttnn.cos_bw", "ttnn.cosh", "ttnn.cosh_bw", "ttnn.create_sharded_memory_config", "ttnn.deallocate", "ttnn.deg2rad", "ttnn.deg2rad_bw", "ttnn.digamma", "ttnn.digamma_bw", "ttnn.div", "ttnn.div_bw", "ttnn.div_no_nan", "ttnn.div_no_nan_bw", "ttnn.downsample", "ttnn.dump_tensor", "ttnn.elu", "ttnn.elu_bw", "ttnn.embedding", "ttnn.embedding_bw", "ttnn.empty", "ttnn.empty_like", "ttnn.eq", "ttnn.eq_", "ttnn.eqz", "ttnn.erf", "ttnn.erf_bw", "ttnn.erfc", "ttnn.erfc_bw", "ttnn.erfinv", "ttnn.erfinv_bw", "ttnn.exp", "ttnn.exp2", "ttnn.exp2_bw", "ttnn.exp_bw", "ttnn.experimental.all_reduce", "ttnn.experimental.cumprod", "ttnn.experimental.dropout", "ttnn.experimental.gelu_bw", "ttnn.experimental.rotary_embedding", "ttnn.experimental.sort", "ttnn.expm1", "ttnn.expm1_bw", "ttnn.fill", "ttnn.fill_bw", "ttnn.fill_ones_rm", "ttnn.fill_rm", "ttnn.fill_zero_bw", "ttnn.floor", "ttnn.floor_bw", "ttnn.floor_div", "ttnn.fmod", "ttnn.fmod_bw", "ttnn.format_input_tensor", "ttnn.format_output_tensor", "ttnn.frac", "ttnn.frac_bw", "ttnn.from_device", "ttnn.from_torch", "ttnn.full", "ttnn.full_like", "ttnn.gcd", "ttnn.ge", "ttnn.ge_", "ttnn.geglu", "ttnn.gelu", "ttnn.gelu_bw", "ttnn.gez", "ttnn.global_avg_pool2d", "ttnn.glu", "ttnn.group_norm", "ttnn.gt", "ttnn.gt_", "ttnn.gtz", "ttnn.hardshrink", "ttnn.hardshrink_bw", "ttnn.hardsigmoid", "ttnn.hardsigmoid_bw", "ttnn.hardswish", "ttnn.hardswish_bw", "ttnn.hardtanh", "ttnn.hardtanh_bw", "ttnn.heaviside", "ttnn.hypot", "ttnn.hypot_bw", "ttnn.i0", "ttnn.i0_bw", "ttnn.identity", "ttnn.imag", "ttnn.imag_bw", "ttnn.indexed_fill", "ttnn.is_imag", "ttnn.is_real", "ttnn.isclose", "ttnn.isfinite", "ttnn.isinf", "ttnn.isnan", "ttnn.isneginf", "ttnn.isposinf", "ttnn.kv_cache.fill_cache_for_user_", "ttnn.kv_cache.update_cache_for_token_", "ttnn.l1_loss", "ttnn.layer_norm", "ttnn.lcm", "ttnn.ldexp", "ttnn.ldexp_bw", "ttnn.le", "ttnn.le_", "ttnn.leaky_relu", "ttnn.leaky_relu_bw", "ttnn.lerp", "ttnn.lerp_bw", "ttnn.lez", "ttnn.lgamma", "ttnn.lgamma_bw", "ttnn.linear", "ttnn.load_tensor", "ttnn.log", "ttnn.log10", "ttnn.log10_bw", "ttnn.log1p", "ttnn.log1p_bw", "ttnn.log2", "ttnn.log2_bw", "ttnn.log_bw", "ttnn.log_sigmoid", "ttnn.log_sigmoid_bw", "ttnn.logaddexp", "ttnn.logaddexp2", "ttnn.logaddexp2_bw", "ttnn.logaddexp_bw", "ttnn.logical_and", "ttnn.logical_and_", "ttnn.logical_not", "ttnn.logical_not_", "ttnn.logical_or", "ttnn.logical_or_", "ttnn.logical_xor", "ttnn.logical_xor_", "ttnn.logit", "ttnn.logit_bw", "ttnn.logiteps_bw", "ttnn.lt", "ttnn.lt_", "ttnn.ltz", "ttnn.mac", "ttnn.manage_device", "ttnn.matmul", "ttnn.max", "ttnn.max_bw", "ttnn.max_pool2d", "ttnn.maximum", "ttnn.mean", "ttnn.min", "ttnn.min_bw", "ttnn.minimum", "ttnn.mish", "ttnn.model_preprocessing.preprocess_model", "ttnn.model_preprocessing.preprocess_model_parameters", "ttnn.moreh_sum", "ttnn.mse_loss", "ttnn.mul_bw", "ttnn.multigammaln", "ttnn.multigammaln_bw", "ttnn.multiply", "ttnn.ne", "ttnn.ne_", "ttnn.neg", "ttnn.neg_bw", "ttnn.nextafter", "ttnn.nez", "ttnn.nonzero", "ttnn.normalize_global", "ttnn.normalize_hw", "ttnn.ones", "ttnn.ones_like", "ttnn.open_device", "ttnn.outer", "ttnn.pad", "ttnn.pad_to_tile_shape", "ttnn.permute", "ttnn.polar", "ttnn.polar_bw", "ttnn.polygamma", "ttnn.polygamma_bw", "ttnn.polyval", "ttnn.pow", "ttnn.pow_bw", "ttnn.prelu", "ttnn.prod", "ttnn.prod_bw", "ttnn.rad2deg", "ttnn.rad2deg_bw", "ttnn.rdiv", "ttnn.rdiv_bw", "ttnn.real", "ttnn.real_bw", "ttnn.reallocate", "ttnn.reciprocal", "ttnn.reciprocal_bw", "ttnn.reduce_scatter", "ttnn.register_post_operation_hook", "ttnn.register_pre_operation_hook", "ttnn.reglu", "ttnn.relu", "ttnn.relu6", "ttnn.relu6_bw", "ttnn.relu_bw", "ttnn.relu_max", "ttnn.relu_min", "ttnn.remainder", "ttnn.remainder_bw", "ttnn.repeat", "ttnn.repeat_bw", "ttnn.repeat_interleave", "ttnn.reshape", "ttnn.rms_norm", "ttnn.round", "ttnn.round_bw", "ttnn.rpow", "ttnn.rpow_bw", "ttnn.rsqrt", "ttnn.rsqrt_bw", "ttnn.rsub", "ttnn.rsub_bw", "ttnn.scatter", "ttnn.selu", "ttnn.selu_bw", "ttnn.set_printoptions", "ttnn.sigmoid", "ttnn.sigmoid_accurate", "ttnn.sigmoid_bw", "ttnn.sign", "ttnn.sign_bw", "ttnn.signbit", "ttnn.silu", "ttnn.silu_bw", "ttnn.sin", "ttnn.sin_bw", "ttnn.sinh", "ttnn.sinh_bw", "ttnn.slice", "ttnn.softmax", "ttnn.softplus", "ttnn.softplus_bw", "ttnn.softshrink", "ttnn.softshrink_bw", "ttnn.softsign", "ttnn.softsign_bw", "ttnn.sqrt", "ttnn.sqrt_bw", "ttnn.square", "ttnn.square_bw", "ttnn.squared_difference", "ttnn.squared_difference_bw", "ttnn.std", "ttnn.sub_bw", "ttnn.subalpha", "ttnn.subalpha_bw", "ttnn.subtract", "ttnn.sum", "ttnn.swiglu", "ttnn.swish", "ttnn.synchronize_device", "ttnn.tan", "ttnn.tan_bw", "ttnn.tanh", "ttnn.tanh_bw", "ttnn.tanhshrink", "ttnn.tanhshrink_bw", "ttnn.threshold", "ttnn.threshold_bw", "ttnn.tilize", "ttnn.tilize_with_val_padding", "ttnn.to_device", "ttnn.to_layout", "ttnn.to_memory_config", "ttnn.to_torch", "ttnn.topk", "ttnn.transformer.attention_softmax", "ttnn.transformer.attention_softmax_", "ttnn.transformer.concatenate_heads", "ttnn.transformer.scaled_dot_product_attention", "ttnn.transformer.scaled_dot_product_attention_decode", "ttnn.transformer.split_query_key_value_and_split_heads", "ttnn.tril", "ttnn.triu", "ttnn.trunc", "ttnn.trunc_bw", "ttnn.unary_chain", "ttnn.untilize", "ttnn.untilize_with_unpadding", "ttnn.upsample", "ttnn.var", "ttnn.where", "ttnn.where_bw", "ttnn.xlogy", "ttnn.xlogy_bw", "ttnn.zeros", "ttnn.zeros_like", "Converting PyTorch Model to TT-NN", "Building and Uplifting Demos", "Examples of Tensor and TT-LIB Use", "Dependencies", "Tensor", "TT-LIB", "Getting Started", "Install", "Onboarding New Functionality", "Profiling TT-NN Operations", "Tensor", "Tutorials", "Graphing Torch DiT_XL_2 With TTNN", "Matmul Operation", "Multi-Head Attention", "ttnn Profiling", "Resnet Basic Block", "Tensor and Add Operation", "ttnn Tracer", "Tensor and Add Operation", "Matrix Multiplication", "Multi-Head Attention", "Tracing ttnn operations and torch modules/functions", "Profiling ttnn operations", "Resnet Block", "Build a graph of a pytorch based model", "Using TT-NN"], "terms": {"what": [0, 365, 373, 389], "i": [0, 3, 4, 8, 9, 14, 17, 19, 28, 32, 37, 39, 41, 42, 45, 59, 60, 61, 68, 74, 75, 78, 82, 83, 86, 99, 100, 103, 104, 107, 109, 110, 117, 118, 120, 121, 122, 126, 128, 132, 133, 135, 148, 150, 155, 156, 169, 170, 175, 179, 182, 186, 193, 194, 206, 210, 211, 213, 214, 218, 221, 222, 225, 227, 229, 240, 242, 243, 245, 247, 249, 250, 252, 257, 262, 264, 265, 266, 267, 272, 273, 278, 279, 283, 288, 305, 306, 318, 320, 322, 325, 327, 328, 329, 339, 341, 342, 343, 344, 346, 347, 348, 356, 361, 364, 365, 366, 368, 369, 370, 371, 372, 373, 374, 375, 376, 379, 380, 383, 384, 385, 386, 387, 388, 389, 390], "get": [0, 8, 345, 364, 366, 368, 371, 375, 380, 383, 384, 385, 386, 387], "start": [0, 27, 174, 242, 305, 364, 368, 369, 371, 373, 385, 387], "1": [0, 3, 10, 11, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 31, 33, 34, 36, 37, 38, 40, 41, 42, 43, 44, 45, 46, 47, 48, 51, 52, 53, 54, 55, 56, 57, 60, 61, 62, 63, 65, 67, 71, 73, 74, 75, 76, 77, 80, 81, 82, 83, 86, 87, 89, 90, 91, 92, 94, 95, 97, 98, 99, 100, 101, 102, 103, 104, 106, 107, 108, 109, 110, 111, 113, 114, 115, 116, 117, 118, 120, 122, 123, 125, 126, 127, 128, 129, 130, 132, 133, 135, 136, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 150, 151, 152, 153, 154, 155, 156, 157, 165, 167, 168, 169, 170, 171, 172, 173, 174, 175, 178, 183, 184, 185, 187, 188, 190, 191, 192, 193, 194, 195, 196, 199, 200, 201, 202, 203, 204, 205, 206, 207, 209, 211, 212, 213, 214, 215, 216, 217, 218, 219, 224, 225, 226, 227, 228, 229, 230, 232, 233, 235, 236, 237, 238, 239, 241, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 256, 257, 258, 259, 260, 262, 263, 264, 267, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 293, 294, 295, 297, 300, 302, 304, 305, 306, 307, 308, 309, 310, 312, 314, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 328, 329, 330, 331, 333, 334, 335, 339, 346, 347, 348, 351, 352, 357, 358, 359, 360, 361, 362, 363, 366, 368, 369, 372, 374, 383, 384, 385, 386, 387, 388, 389], "instal": [0, 3, 365, 373, 375, 387, 389], "build": [0, 366, 375, 376, 379, 387], "2": [0, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 61, 63, 64, 65, 66, 67, 70, 71, 72, 73, 74, 75, 76, 77, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 100, 101, 102, 104, 105, 106, 107, 108, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 122, 123, 124, 125, 126, 127, 129, 130, 131, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 153, 154, 157, 158, 159, 160, 161, 162, 165, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 238, 239, 242, 243, 244, 246, 247, 248, 249, 250, 251, 252, 253, 255, 256, 257, 258, 260, 262, 263, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 281, 282, 283, 284, 285, 286, 287, 288, 290, 291, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 326, 328, 329, 330, 331, 332, 333, 334, 335, 341, 346, 348, 349, 350, 351, 352, 353, 357, 358, 359, 360, 361, 362, 363, 367, 368, 374, 375, 376, 383, 384, 385, 386, 387, 388], "explor": 0, "our": [0, 3, 4, 365, 372, 374, 383], "demo": [0, 4, 371, 373, 386], "3": [0, 3, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 60, 64, 65, 66, 67, 70, 71, 72, 73, 74, 75, 76, 77, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 100, 101, 102, 104, 105, 106, 107, 108, 110, 111, 112, 113, 114, 115, 116, 119, 120, 122, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 157, 158, 159, 160, 161, 162, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 213, 214, 215, 218, 219, 220, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 244, 246, 247, 248, 249, 250, 251, 252, 255, 256, 257, 258, 262, 263, 264, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 281, 282, 283, 284, 285, 286, 287, 288, 290, 291, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 320, 321, 322, 323, 325, 326, 328, 329, 330, 331, 332, 333, 334, 335, 339, 341, 348, 349, 350, 351, 352, 353, 358, 359, 360, 361, 362, 363, 368, 369, 374, 383, 384, 385, 386, 387, 388, 389], "tutori": [0, 376, 379, 380, 385, 389], "multi": [0, 24, 29, 78, 79, 99, 264, 356, 374, 375, 383], "head": [0, 343, 344, 345, 346, 348, 371, 375], "attent": [0, 343, 344, 346, 347, 348, 375], "simpl": [0, 371, 387, 389], "4": [0, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 26, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 61, 64, 65, 66, 67, 70, 71, 72, 73, 74, 75, 76, 77, 80, 81, 82, 84, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 101, 102, 104, 105, 106, 107, 108, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 125, 126, 127, 128, 129, 130, 131, 133, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 157, 158, 159, 160, 161, 162, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 213, 214, 215, 218, 219, 220, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 236, 237, 241, 243, 246, 247, 248, 249, 250, 251, 252, 254, 255, 256, 257, 258, 262, 263, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 279, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 320, 321, 322, 323, 325, 326, 328, 329, 330, 331, 332, 333, 334, 335, 349, 350, 351, 352, 353, 358, 359, 360, 361, 362, 363, 368, 369, 374, 383, 384, 385, 386, 387, 388, 389], "optim": [0, 374, 375, 378, 383], "where": [0, 2, 3, 4, 27, 68, 84, 85, 117, 118, 179, 211, 235, 242, 252, 338, 339, 359, 365, 369, 373, 374, 389], "To": [0, 3, 366, 369, 372, 383, 384, 390], "go": [0, 383], "from": [0, 2, 3, 4, 6, 27, 29, 59, 60, 83, 121, 122, 163, 164, 180, 242, 287, 288, 323, 338, 355, 364, 365, 367, 368, 373, 374, 375, 376, 380, 383, 385, 386, 387], "here": [0, 2, 6, 384, 389], "prerequisit": 0, "set": [0, 3, 4, 9, 60, 109, 211, 227, 244, 292, 327, 345, 348, 364, 366, 368, 369, 373, 375, 383, 386, 387, 388, 389, 390], "up": [0, 5, 110, 369, 373, 375, 384, 389], "hardwar": [0, 2, 6, 364, 365, 370, 374, 383, 390], "driver": [0, 383, 384, 385, 386, 387, 388], "firmwar": 0, "system": [0, 14, 32, 39, 107, 128, 133, 257, 267, 272, 273, 283, 325, 373], "level": [0, 373], "depend": [0, 4, 339, 365, 373, 374, 375, 387], "kmd": 0, "updat": [0, 41, 164, 365, 372, 390], "devic": [0, 5, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 60, 61, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 74, 75, 76, 77, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 165, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 349, 350, 351, 352, 353, 354, 355, 357, 358, 359, 360, 361, 362, 363, 364, 365, 367, 368, 373, 374, 375, 377, 378, 381, 386, 387, 388, 389], "flash": [0, 347], "manag": [0, 210, 265, 266, 365], "interfac": [0, 369], "smi": 0, "option": [0, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 128, 129, 130, 131, 132, 133, 134, 135, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 154, 155, 156, 157, 158, 159, 160, 161, 162, 165, 166, 167, 168, 169, 170, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 201, 203, 204, 205, 206, 208, 209, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 224, 225, 226, 227, 228, 229, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 261, 262, 263, 264, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 367, 368, 373, 389, 390], "card": 0, "configur": [0, 3, 4, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 63, 64, 65, 66, 67, 70, 71, 72, 73, 74, 75, 76, 77, 80, 81, 82, 83, 84, 85, 86, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 119, 120, 122, 123, 124, 125, 126, 128, 129, 130, 131, 132, 133, 134, 135, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 165, 166, 167, 168, 169, 170, 172, 173, 174, 175, 176, 177, 178, 179, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 201, 203, 204, 205, 206, 208, 209, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 224, 225, 226, 227, 228, 229, 231, 232, 233, 234, 235, 236, 237, 238, 239, 241, 242, 244, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 342, 343, 344, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 365, 368, 375, 377, 378, 386], "topologi": [0, 24, 99, 264], "metalium": [0, 3, 4, 6], "There": [0, 6, 211, 364, 368, 369, 374], "ar": [0, 3, 4, 9, 17, 24, 36, 44, 47, 58, 60, 75, 99, 110, 179, 211, 214, 235, 244, 250, 264, 265, 266, 283, 305, 327, 342, 348, 356, 364, 365, 366, 368, 369, 370, 373, 374, 375, 376, 379, 380, 383, 384, 385, 390], "three": 0, "sourc": [0, 3, 4, 5, 163, 375, 379], "step": [0, 27, 305, 365, 366, 372, 383, 389], "clone": [0, 151, 348, 375, 376, 383, 384, 385, 386], "repositori": [0, 1, 370], "invok": [0, 6], "script": [0, 3, 4, 365, 373], "docker": 0, "releas": [0, 69], "imag": [0, 155, 245, 365, 369, 373, 374, 389], "wheel": [0, 387], "download": [0, 375, 376, 386, 387], "latest": [0, 373], "For": [0, 6, 11, 21, 29, 42, 75, 116, 122, 130, 148, 175, 183, 185, 187, 188, 190, 193, 194, 204, 205, 211, 251, 254, 258, 263, 275, 308, 314, 348, 361, 364, 365, 368, 372, 373, 374], "user": [0, 4, 6, 151, 179, 211, 221, 222, 337, 365, 370, 372, 383, 384, 385, 386, 387, 388, 389], "onli": [0, 6, 17, 19, 20, 22, 24, 28, 37, 42, 45, 50, 54, 55, 56, 57, 60, 61, 68, 74, 75, 79, 83, 100, 110, 112, 115, 122, 128, 133, 148, 169, 174, 175, 182, 186, 193, 194, 209, 211, 213, 218, 220, 225, 247, 257, 262, 264, 267, 274, 275, 288, 318, 320, 322, 325, 327, 347, 348, 358, 361, 364, 365, 368, 369, 373, 374, 375, 376, 379, 380, 388, 390], "environ": [0, 3, 4, 369, 370, 385, 386, 387, 389, 390], "you": [0, 1, 2, 3, 4, 6, 307, 365, 366, 368, 369, 370, 373, 375, 386, 389, 390], "all": [0, 6, 24, 28, 99, 132, 211, 221, 222, 249, 253, 254, 264, 327, 364, 365, 368, 369, 372, 373, 374, 383, 385, 387], "verifi": [0, 372], "your": [0, 365, 366, 369, 373, 375], "try": [0, 365, 383, 387], "execut": [0, 3, 265, 266, 365, 366, 369, 373, 384, 385, 387, 390], "program": [0, 3, 5, 6, 24, 179, 211, 264, 343, 344, 367, 373, 375, 377, 378, 383, 386, 387, 388], "exampl": [0, 104, 244, 253, 278, 327, 338, 365, 367, 369, 370, 372, 373, 374, 383, 389], "interest": 0, "contribut": [0, 2, 370], "us": [0, 3, 4, 5, 6, 8, 9, 24, 29, 42, 60, 68, 75, 82, 83, 85, 89, 91, 95, 99, 101, 102, 103, 104, 110, 121, 122, 124, 129, 130, 148, 151, 179, 211, 214, 221, 222, 239, 240, 257, 258, 264, 265, 266, 277, 285, 292, 293, 306, 307, 336, 337, 338, 339, 340, 341, 342, 347, 348, 354, 355, 363, 364, 365, 367, 368, 369, 370, 371, 372, 373, 374, 375, 377, 378, 381, 382, 387, 389], "basic": [0, 370, 371, 375], "convert": [0, 5, 6, 29, 58, 122, 221, 222, 340, 341, 366, 367, 375, 378, 381, 384], "torch": [0, 6, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 60, 61, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 74, 75, 76, 77, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 104, 105, 106, 107, 108, 111, 112, 113, 114, 115, 116, 119, 120, 121, 122, 125, 126, 127, 128, 129, 130, 131, 132, 133, 135, 136, 137, 138, 139, 140, 141, 142, 143, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 165, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 241, 242, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 328, 329, 330, 331, 332, 333, 334, 335, 338, 339, 340, 341, 342, 348, 349, 350, 351, 352, 353, 357, 358, 359, 360, 361, 363, 364, 366, 368, 374, 375, 377, 378, 380, 381, 382, 387, 389], "tensor": [0, 4, 5, 6, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 367, 373, 375, 377, 385, 386, 388, 389], "run": [0, 5, 6, 24, 41, 99, 122, 221, 264, 265, 266, 365, 367, 369, 370, 371, 372, 373, 375, 378, 380, 383, 384, 387], "an": [0, 2, 3, 5, 6, 24, 60, 82, 99, 110, 111, 115, 132, 210, 211, 245, 252, 264, 274, 341, 365, 368, 369, 370, 371, 372, 373, 374, 383, 385, 389], "oper": [0, 5, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 62, 63, 64, 65, 66, 67, 70, 71, 72, 73, 74, 75, 76, 77, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 119, 120, 121, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 201, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 241, 242, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 339, 341, 342, 343, 344, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 365, 366, 367, 368, 370, 372, 374, 375, 379, 382, 384, 385], "__getitem__": 0, "slice": [0, 369], "enabl": [0, 3, 5, 369, 372, 373, 375, 377, 378, 383, 386, 387, 388, 389], "cach": [0, 3, 5, 29, 59, 103, 163, 164, 221, 222, 367, 373, 374, 375, 377, 378, 383, 386, 387, 388, 389], "5": [0, 27, 41, 52, 55, 57, 68, 72, 82, 84, 104, 124, 138, 139, 140, 142, 157, 203, 226, 252, 258, 309, 310, 368, 371, 383, 384, 385, 386, 387, 388, 389], "debug": [0, 5, 6, 369, 372, 383, 384, 385, 386, 388], "intermedi": 0, "6": [0, 27, 104, 226, 281, 368, 374, 383, 384, 385, 386, 387, 388, 389], "trace": [0, 5, 240, 375, 380, 382, 389], "graph": [0, 5, 221, 367, 375, 380, 386], "7": [0, 82, 123, 281, 368, 383, 384, 385, 386, 387, 388, 389], "tt_lib": [0, 109, 110, 366, 367], "8": [0, 24, 27, 68, 82, 99, 144, 264, 368, 371, 373, 374, 383, 384, 385, 386, 387, 388, 389], "log": [0, 190, 360, 367, 373, 384, 386, 387], "9": [0, 54, 56, 66, 82, 84, 303, 368, 371, 383, 384, 385, 386, 387, 388, 389], "support": [0, 1, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 26, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 60, 61, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 80, 81, 84, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 101, 102, 105, 106, 107, 108, 111, 112, 113, 114, 115, 116, 119, 120, 125, 126, 127, 128, 129, 130, 131, 133, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 157, 158, 159, 160, 161, 162, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 211, 213, 214, 215, 218, 219, 220, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 236, 237, 241, 246, 247, 248, 249, 250, 251, 252, 254, 255, 256, 257, 258, 262, 263, 267, 268, 269, 270, 271, 272, 273, 274, 275, 277, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 320, 321, 322, 323, 325, 326, 328, 329, 330, 331, 332, 333, 334, 335, 347, 349, 350, 351, 352, 353, 358, 359, 360, 361, 362, 363, 366, 368, 369, 374, 383, 388], "python": [0, 4, 5, 369, 371, 372, 373, 387, 389], "10": [0, 27, 69, 82, 121, 132, 150, 179, 211, 247, 261, 334, 338, 339, 340, 366, 368, 371, 383, 384, 385, 386, 387, 389], "chang": [0, 279, 336, 337, 354, 355, 369, 371, 383, 387], "string": [0, 42, 74, 130, 221, 222, 257, 258, 292, 369], "represent": [0, 307, 369, 374], "11": [0, 84, 383, 384, 385, 386, 387, 388], "visual": [0, 5, 386, 388, 389], "web": 0, "browser": [0, 375], "12": [0, 61, 134, 166, 280, 364, 383, 384, 385, 387, 388, 389], "regist": [0, 5, 6, 265, 266], "pre": [0, 6, 214, 266, 371, 375, 376, 378, 387], "post": [0, 265, 373], "hook": [0, 265, 266, 387], "13": [0, 371, 383, 384, 385, 387, 389], "queri": [0, 348, 369, 385], "14": [0, 3, 61, 383, 384, 385, 387], "fall": [0, 169], "back": [0, 6, 345, 366, 373, 383], "15": [0, 371, 385, 387], "captur": [0, 389], "c": [0, 5, 41, 78, 109, 110, 211, 214, 356, 369, 373, 374, 383, 384, 385, 386, 387, 388], "function": [0, 8, 9, 14, 16, 26, 29, 32, 36, 39, 49, 52, 54, 56, 63, 66, 70, 72, 80, 86, 98, 102, 104, 117, 118, 119, 122, 126, 128, 133, 135, 138, 140, 142, 144, 145, 146, 151, 153, 165, 168, 170, 172, 177, 179, 184, 191, 192, 195, 198, 199, 201, 203, 206, 211, 221, 222, 224, 226, 227, 228, 229, 236, 237, 243, 246, 247, 248, 253, 255, 260, 267, 272, 273, 283, 287, 290, 303, 307, 309, 311, 317, 323, 325, 326, 327, 331, 332, 334, 338, 349, 350, 364, 365, 366, 368, 369, 375, 382, 383], "buffer": [0, 6, 24, 69, 99, 240, 264, 367, 368, 369, 373, 374], "alloc": [0, 6, 27, 84, 85, 123, 124, 238, 239, 362, 363, 374, 383, 384, 385, 386, 387, 388], "etc": [0, 5, 6, 368], "shape": [0, 27, 41, 60, 68, 84, 85, 100, 107, 117, 118, 123, 124, 132, 179, 211, 214, 235, 238, 239, 242, 243, 244, 276, 277, 278, 279, 305, 337, 345, 348, 355, 362, 363, 364, 366, 368, 369, 373, 383, 384, 385, 388, 389], "layout": [0, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 61, 64, 65, 66, 67, 69, 70, 71, 72, 73, 74, 75, 76, 77, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 101, 102, 105, 106, 107, 108, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 157, 158, 159, 160, 161, 162, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 211, 213, 215, 218, 219, 220, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 241, 246, 247, 248, 249, 250, 251, 252, 254, 255, 256, 257, 258, 262, 263, 264, 267, 268, 269, 270, 271, 272, 273, 274, 275, 277, 278, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 320, 321, 322, 323, 325, 326, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 339, 341, 342, 349, 350, 351, 352, 353, 354, 355, 358, 359, 360, 361, 362, 363, 364, 366, 368, 369, 373, 375, 377, 381, 385, 388, 390], "data": [0, 5, 16, 27, 28, 29, 58, 78, 82, 83, 84, 85, 86, 109, 110, 122, 123, 124, 126, 132, 135, 168, 170, 179, 191, 192, 195, 199, 201, 206, 211, 228, 229, 238, 239, 257, 287, 317, 323, 336, 337, 339, 340, 341, 342, 354, 355, 356, 362, 363, 364, 366, 368, 369, 373, 375, 381, 387, 390], "type": [0, 5, 6, 16, 27, 28, 29, 42, 58, 75, 82, 83, 84, 85, 86, 109, 110, 117, 122, 123, 124, 126, 130, 132, 135, 168, 170, 179, 191, 192, 195, 199, 201, 206, 211, 228, 229, 238, 239, 240, 257, 287, 317, 323, 327, 336, 337, 338, 339, 340, 341, 342, 354, 355, 362, 363, 364, 366, 367, 368, 373, 375, 381, 387, 389], "limit": [0, 11, 21, 42, 75, 116, 130, 148, 179, 183, 185, 187, 188, 190, 193, 194, 204, 205, 251, 254, 258, 263, 275, 308, 314, 361, 365, 369], "bfloat8_b": [0, 10, 11, 12, 16, 17, 18, 19, 20, 21, 22, 23, 30, 33, 34, 35, 36, 37, 38, 42, 50, 52, 61, 64, 65, 66, 70, 71, 72, 73, 75, 77, 80, 86, 87, 89, 91, 93, 95, 96, 97, 98, 105, 106, 107, 112, 116, 119, 120, 122, 126, 127, 128, 129, 130, 131, 133, 135, 136, 137, 138, 139, 140, 142, 144, 145, 146, 147, 148, 149, 150, 151, 158, 159, 160, 161, 162, 168, 169, 170, 171, 172, 173, 174, 175, 176, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 204, 205, 206, 207, 208, 209, 213, 215, 218, 219, 220, 225, 228, 229, 230, 231, 232, 233, 234, 249, 250, 251, 252, 254, 255, 256, 258, 262, 263, 267, 268, 269, 271, 272, 275, 281, 284, 285, 286, 287, 288, 290, 291, 293, 294, 295, 296, 299, 300, 301, 302, 303, 308, 309, 310, 313, 314, 315, 316, 317, 318, 320, 321, 322, 323, 325, 326, 328, 329, 330, 332, 333, 341, 349, 350, 351, 353, 358, 359, 361, 364, 368, 373, 385], "storag": [0, 367, 369, 375, 381], "shard": [0, 17, 60, 68, 179, 211, 214, 340, 348], "memori": [0, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 62, 63, 64, 65, 66, 67, 70, 71, 72, 73, 74, 75, 76, 77, 80, 81, 82, 83, 84, 85, 86, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 122, 123, 124, 125, 126, 128, 129, 130, 131, 132, 133, 134, 135, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 165, 166, 167, 168, 169, 170, 172, 173, 174, 175, 176, 177, 178, 179, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 201, 203, 204, 205, 206, 208, 209, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 224, 225, 226, 227, 228, 229, 231, 232, 233, 234, 235, 236, 237, 238, 239, 241, 242, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 368, 373, 383, 385], "config": [0, 24, 62, 82, 83, 99, 117, 118, 211, 245, 264, 279, 305, 343, 344, 345, 364, 371, 375, 377, 383, 385, 386, 387, 388, 390], "api": [0, 4, 6, 346, 364, 367, 370, 371, 372, 385, 390], "rank": [0, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 26, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 61, 64, 65, 66, 67, 70, 71, 72, 73, 74, 75, 76, 77, 80, 81, 84, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 101, 102, 105, 106, 107, 108, 111, 112, 113, 114, 115, 116, 119, 120, 125, 126, 127, 128, 129, 130, 131, 133, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 157, 158, 159, 160, 161, 162, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 213, 215, 218, 219, 220, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 236, 237, 241, 246, 247, 248, 249, 250, 251, 252, 254, 255, 256, 257, 258, 262, 263, 267, 268, 269, 270, 271, 272, 273, 274, 275, 277, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 320, 321, 322, 323, 325, 326, 328, 329, 330, 331, 332, 333, 334, 335, 341, 349, 350, 351, 352, 353, 358, 359, 360, 361, 362, 363, 368, 374], "to_rank": [0, 374], "open_devic": [0, 9, 59, 69, 82, 83, 121, 180, 261, 327, 338, 339, 340, 368, 383, 384, 385, 386, 390], "close_devic": [0, 383, 384, 385, 386, 388, 390], "manage_devic": [0, 390], "synchronize_devic": 0, "setdefaultdevic": [0, 366, 369], "getdefaultdevic": 0, "format_input_tensor": 0, "format_output_tensor": 0, "pad_to_tile_shap": 0, "create_sharded_memory_config": [0, 374], "core": [0, 6, 68, 179, 211, 240, 354, 364, 369, 373, 374, 384, 385, 387], "as_tensor": [0, 383, 384, 385, 386, 388], "from_torch": [0, 6, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 60, 61, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 74, 75, 76, 77, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 100, 101, 102, 105, 106, 107, 108, 111, 112, 113, 114, 115, 116, 119, 120, 121, 125, 126, 127, 128, 129, 130, 131, 132, 133, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 165, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 241, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 328, 329, 330, 331, 332, 333, 334, 335, 338, 339, 340, 341, 349, 350, 351, 352, 353, 357, 358, 359, 360, 361, 363, 364, 383, 384, 385, 386, 388, 390], "to_torch": [0, 6, 364, 368, 383, 384, 385, 386, 388, 390], "to_devic": [0, 24, 25, 26, 62, 63, 69, 77, 82, 121, 152, 153, 154, 155, 156, 165, 179, 211, 212, 214, 216, 217, 224, 235, 244, 245, 246, 248, 259, 260, 261, 284, 306, 319, 324, 339, 340, 357, 383, 384, 385, 386, 388], "from_devic": [0, 383, 384, 385, 386, 387, 388], "to_layout": [0, 69, 122, 341, 374, 383, 384, 385], "dump_tensor": [0, 383, 384, 385, 386, 388], "load_tensor": [0, 383, 384, 385, 386, 387, 388], "dealloc": [0, 261, 374, 383, 384, 385, 386, 388, 390], "realloc": [0, 383, 384, 385, 386, 388], "to_memory_config": [0, 374, 388], "creation": [0, 122, 368], "arang": [0, 368, 383, 384, 385, 386], "empti": [0, 6, 369, 389, 390], "empty_lik": 0, "zero": [0, 60, 74, 101, 111, 124, 210, 235, 244, 262, 305, 306, 368, 369, 374, 386, 390], "zeros_lik": [0, 100], "ones": [0, 79, 117, 369], "ones_lik": 0, "full": [0, 365, 367, 369, 373, 390], "full_lik": 0, "matrix": [0, 5, 82, 179, 211, 371, 374, 375, 377, 383, 390], "multipl": [0, 5, 6, 58, 69, 110, 123, 128, 133, 167, 211, 221, 238, 267, 325, 339, 362, 368, 369, 373, 375, 377], "matmul": [0, 179, 375, 383, 384, 385, 386, 387], "linear": [0, 24, 99, 264, 307, 364, 367, 369, 383, 384, 385, 386], "pointwis": 0, "unari": [0, 6, 258, 353], "ab": [0, 11], "aco": [0, 13], "acosh": [0, 15], "asin": [0, 31], "asinh": [0, 33], "atan": [0, 38], "atanh": [0, 40], "bitwise_not": [0, 367, 369], "bitwise_left_shift": 0, "bitwise_right_shift": 0, "cbrt": 0, "ceil": [0, 51, 214, 367, 369], "celu": [0, 53, 383, 384, 385, 386], "clamp": [0, 55], "clip": [0, 57, 383, 384, 385, 386], "co": 0, "cosh": [0, 67], "deg2rad": [0, 71], "digamma": [0, 73], "experiment": [0, 4, 365, 367, 387], "dropout": 0, "gelu_bw": 0, "elu": [0, 81], "eqz": 0, "erf": [0, 90], "erfc": [0, 92], "erfinv": [0, 94], "exp": [0, 353, 366, 369, 386, 390], "exp2": [0, 97], "expm1": [0, 106], "fill": [0, 108, 110, 111, 123, 124, 238, 239, 276, 305, 362, 363, 369, 373], "floor": [0, 74, 75, 113, 114, 257, 258, 274, 367, 369], "frac": [0, 36, 74, 120, 369], "geglu": [0, 383, 384, 385, 386], "gelu": [0, 102, 128, 130, 364, 369], "glu": [0, 383, 384, 385, 386], "gez": 0, "gtz": 0, "hardshrink": [0, 139, 173, 383, 384, 385, 386], "hardsigmoid": [0, 141], "hardswish": [0, 143], "hardtanh": [0, 145], "heavisid": 0, "i0": [0, 150], "ident": [0, 388], "isfinit": 0, "isinf": 0, "isnan": 0, "isneginf": 0, "isposinf": 0, "leaky_relu": [0, 173], "lez": 0, "lgamma": [0, 178], "log10": [0, 183], "log1p": [0, 185], "log2": [0, 187], "log_sigmoid": 0, "logical_not": [0, 198], "logical_not_": 0, "logit": [0, 204, 383, 384, 385, 386], "ltz": 0, "mish": 0, "multigammaln": 0, "neg": [0, 232, 251, 369], "nez": 0, "normalize_glob": 0, "normalize_hw": 0, "polygamma": [0, 248, 383, 384, 385, 386], "prelu": 0, "rad2deg": [0, 256], "rdiv": [0, 258], "reciproc": [0, 263, 286, 374], "reglu": [0, 383, 384, 385, 386], "relu": [0, 16, 172, 228, 267, 271, 272, 273, 307, 323, 353, 366, 388], "relu_max": 0, "relu_min": 0, "relu6": [0, 270], "remaind": [0, 275], "round": [0, 34, 74, 75, 257, 258, 282, 374], "rsqrt": 0, "selu": [0, 291], "sigmoid": [0, 190, 295, 369], "sigmoid_accur": 0, "sign": [0, 297, 369], "signbit": 0, "silu": [0, 300, 325, 366, 367, 369, 390], "sin": [0, 302], "sinh": [0, 304], "softmax": [0, 343, 344, 367, 369, 385], "softplu": [0, 308], "softshrink": [0, 310, 383, 384, 385, 386], "softsign": [0, 312], "sqrt": [0, 147, 369], "squar": [0, 224, 286, 314, 316, 317, 343, 344, 374], "swiglu": [0, 383, 384, 385, 386], "swish": [0, 369], "tan": [0, 329], "tanh": [0, 42, 102, 130, 331], "tanhshrink": [0, 333], "threshold": [0, 307, 308, 335, 383, 384, 385, 386], "tril": 0, "triu": 0, "trunc": [0, 74, 75, 115, 257, 258, 367, 369], "unary_chain": 0, "clamp_bw": 0, "clip_bw": 0, "hardtanh_bw": 0, "threshold_bw": 0, "softplus_bw": 0, "rdiv_bw": 0, "pow_bw": 0, "exp_bw": 0, "tanh_bw": 0, "sqrt_bw": 0, "multigammaln_bw": 0, "lgamma_bw": 0, "fill_bw": 0, "hardsigmoid_bw": 0, "cos_bw": 0, "acosh_bw": 0, "acos_bw": 0, "atan_bw": 0, "rad2deg_bw": 0, "frac_bw": 0, "trunc_bw": 0, "log_sigmoid_bw": 0, "fill_zero_bw": 0, "i0_bw": 0, "tan_bw": 0, "sigmoid_bw": 0, "rsqrt_bw": 0, "neg_bw": 0, "relu_bw": 0, "logit_bw": 0, "hardshrink_bw": 0, "softshrink_bw": 0, "leaky_relu_bw": 0, "elu_bw": 0, "celu_bw": 0, "rpow_bw": 0, "floor_bw": 0, "round_bw": 0, "log_bw": 0, "relu6_bw": 0, "abs_bw": 0, "silu_bw": 0, "selu_bw": 0, "square_bw": 0, "prod_bw": 0, "hardswish_bw": 0, "tanhshrink_bw": 0, "atanh_bw": 0, "asin_bw": 0, "asinh_bw": 0, "sin_bw": 0, "sinh_bw": 0, "log10_bw": 0, "log1p_bw": 0, "erfc_bw": 0, "ceil_bw": 0, "softsign_bw": 0, "cosh_bw": 0, "logiteps_bw": 0, "log2_bw": 0, "sign_bw": 0, "div_no_nan_bw": 0, "exp2_bw": 0, "expm1_bw": 0, "reciprocal_bw": 0, "digamma_bw": 0, "erfinv_bw": 0, "erf_bw": 0, "deg2rad_bw": 0, "polygamma_bw": 0, "repeat_bw": 0, "real": [0, 156, 245, 260, 365, 368], "angl": [0, 26], "is_imag": 0, "is_real": 0, "polar_bw": 0, "imag_bw": 0, "real_bw": 0, "angle_bw": 0, "conj_bw": 0, "conj": [0, 63], "polar": [0, 246], "binari": [0, 369], "add": [0, 17, 242, 343, 344, 365, 369, 371, 372, 375, 385, 387, 388, 390], "addalpha": [0, 19], "subalpha": [0, 322], "multipli": [0, 18, 20, 22, 179, 211, 225, 321, 356, 369, 375, 377, 390], "subtract": [0, 3, 287, 288, 320, 348, 390], "div": [0, 115, 274], "div_no_nan": [0, 77], "floor_div": 0, "fmod": [0, 116], "gcd": 0, "lcm": 0, "logical_and_": 0, "logical_or_": 0, "logical_xor_": 0, "rpow": [0, 284], "rsub": 0, "ldexp": [0, 169], "logical_and": 0, "logical_or": 0, "logical_xor": [0, 383, 384, 385, 386], "bitwise_and": [0, 44, 47], "bitwise_or": 0, "bitwise_xor": 0, "logaddexp": [0, 194], "logaddexp2": [0, 193], "hypot": [0, 148, 383, 384, 385, 386], "xlogi": [0, 361, 383, 384, 385, 386], "squared_differ": [0, 318], "gt": [0, 383, 384, 385, 386, 387, 388, 389], "gt_": 0, "lt_": 0, "ge_": 0, "le_": 0, "eq_": 0, "ne_": 0, "ge": 0, "lt": [0, 383, 384, 385, 386, 387, 388, 389], "le": 0, "eq": 0, "ne": 0, "isclos": [0, 383, 384, 385, 386], "nextaft": [0, 383, 384, 385, 386], "maximum": [0, 28, 54, 55, 56, 57, 145, 213, 214, 369, 383, 384, 385, 386], "minimum": [0, 6, 54, 55, 56, 57, 145, 218, 369, 374, 383, 384, 385, 386], "outer": 0, "pow": [0, 366], "polyv": [0, 383, 384, 385, 386], "scatter": [0, 264], "atan2": [0, 37, 383, 384, 385, 386], "add_bw": 0, "assign_bw": 0, "atan2_bw": 0, "bias_gelu_bw": 0, "div_bw": 0, "embedding_bw": 0, "fmod_bw": 0, "remainder_bw": 0, "addalpha_bw": 0, "subalpha_bw": 0, "xlogy_bw": 0, "hypot_bw": 0, "ldexp_bw": 0, "logaddexp_bw": 0, "logaddexp2_bw": 0, "mul_bw": 0, "sub_bw": 0, "squared_difference_bw": 0, "concat_bw": 0, "rsub_bw": 0, "min_bw": 0, "max_bw": 0, "ternari": 0, "addcdiv": [0, 21, 383, 384, 385, 386], "addcmul": [0, 23, 383, 384, 385, 386], "mac": [0, 383, 384, 385, 386], "lerp": [0, 175, 383, 384, 385, 386], "addcmul_bw": 0, "addcdiv_bw": 0, "where_bw": 0, "lerp_bw": 0, "loss": [0, 165, 224], "l1_loss": [0, 383, 384, 385, 386], "mse_loss": [0, 383, 384, 385, 386], "reduct": [0, 5, 28, 165, 224], "cumprod": 0, "max": [0, 54, 55, 56, 57, 144, 145, 214, 272, 369, 387, 389], "mean": [0, 41, 165, 224, 366, 367, 369, 374], "min": [0, 54, 55, 56, 57, 144, 145, 272, 273, 389], "std": [0, 6, 292, 368, 369, 388], "sum": [0, 223, 369], "var": [0, 369], "argmax": [0, 369], "prod": [0, 68, 254], "topk": 0, "sort": [0, 342], "movement": 0, "concat": [0, 61, 367, 369, 372], "nonzero": 0, "pad": [0, 58, 82, 110, 117, 118, 122, 214, 243, 244, 279, 305, 337, 339, 366, 367, 368, 369, 374, 383, 388], "permut": [0, 214, 348, 385, 388], "reshap": [0, 214, 348, 366, 367, 368, 369, 383, 384, 385, 386, 387, 388], "repeat": [0, 5, 277, 278, 367, 369], "repeat_interleav": [0, 367, 369], "tiliz": [0, 29, 41, 369, 384, 385], "tilize_with_val_pad": 0, "fill_rm": [0, 109], "fill_ones_rm": 0, "until": [0, 100, 121, 341, 355], "untilize_with_unpad": 0, "indexed_fil": 0, "normal": [0, 41, 151, 369, 387, 389], "group_norm": [0, 367, 369], "layer_norm": [0, 367, 369], "rms_norm": 0, "batch_norm": 0, "moreh": [0, 223, 369], "moreh_sum": 0, "transform": [0, 5, 179, 245, 364, 385, 386, 387], "split_query_key_value_and_split_head": [0, 385], "concatenate_head": [0, 385], "attention_softmax": 0, "attention_softmax_": [0, 385], "rotary_embed": 0, "scaled_dot_product_attent": 0, "scaled_dot_product_attention_decod": 0, "ccl": [0, 5], "all_gath": [0, 383, 384, 385, 386], "reduce_scatt": 0, "all_reduc": 0, "embed": [0, 83, 103, 383], "pool": [0, 132, 214, 369], "global_avg_pool2d": 0, "max_pool2d": 0, "vision": 0, "upsampl": [0, 369], "downsampl": [0, 388], "kv": 0, "kv_cach": 0, "fill_cache_for_user_": 0, "update_cache_for_token_": 0, "convers": [0, 71, 256, 339, 366, 368, 383], "model_preprocess": [0, 364, 385, 386, 387, 388], "preprocess_model": [0, 386, 387, 388], "preprocess_model_paramet": [0, 364, 386, 388], "report": [0, 211, 365, 383, 384, 385, 386, 387, 388, 390], "set_printopt": [0, 390], "register_pre_operation_hook": [0, 390], "register_post_operation_hook": [0, 390], "creat": [0, 4, 6, 27, 58, 68, 84, 85, 104, 107, 123, 124, 238, 239, 362, 363, 366, 368, 369, 372, 374, 375, 380, 381, 385, 389], "host": [0, 29, 121, 242, 305, 327, 338, 339, 366, 368, 369, 371, 373, 374, 375, 381, 384, 385, 386, 387, 388], "borrow": [0, 368, 374, 375, 381], "v": [0, 29, 110, 347, 371, 375, 381], "own": [0, 368, 374, 375, 381], "open": [0, 5, 210, 240, 375, 381, 384, 385, 386, 387, 388, 389, 390], "initi": [0, 85, 221, 222, 364, 366, 369, 375, 377, 378, 381, 386, 387, 388], "b": [0, 6, 103, 115, 211, 274, 278, 346, 347, 375, 377, 381], "random": [0, 244, 366, 375, 377, 381], "valu": [0, 18, 19, 20, 21, 22, 23, 27, 28, 41, 52, 53, 54, 55, 56, 57, 60, 62, 72, 77, 81, 84, 85, 104, 107, 109, 110, 117, 122, 123, 124, 132, 138, 139, 140, 142, 144, 145, 146, 154, 155, 156, 157, 163, 164, 173, 177, 205, 212, 214, 216, 217, 227, 238, 239, 242, 244, 247, 250, 251, 257, 272, 273, 279, 283, 284, 290, 305, 307, 308, 309, 310, 319, 321, 322, 324, 334, 335, 337, 342, 348, 349, 350, 357, 362, 363, 365, 366, 368, 369, 374, 375, 377, 381, 385, 390], "inspect": [0, 375, 377, 381], "output": [0, 3, 6, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 60, 61, 62, 63, 64, 65, 66, 67, 70, 71, 72, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 118, 119, 120, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 241, 242, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 262, 263, 264, 265, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 339, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 363, 364, 365, 366, 367, 368, 373, 374, 375, 377, 378, 381, 386, 390], "attribut": [0, 6, 364, 369, 373, 374, 375, 381, 387], "close": [0, 59, 210, 366, 371, 375, 377, 378, 381, 386, 387, 388], "result": [0, 3, 27, 29, 41, 110, 122, 124, 151, 179, 211, 214, 239, 257, 262, 363, 366, 369, 373, 374, 375, 377], "more": [0, 1, 5, 6, 11, 21, 42, 75, 116, 130, 148, 183, 185, 187, 188, 190, 193, 194, 204, 205, 211, 251, 254, 258, 262, 263, 275, 308, 314, 361, 369, 370, 371, 373, 374, 375, 377, 385, 387, 389], "perform": [0, 11, 13, 14, 15, 17, 19, 21, 23, 24, 25, 26, 31, 32, 33, 34, 37, 38, 39, 40, 42, 43, 44, 46, 47, 48, 49, 51, 52, 53, 54, 55, 56, 57, 61, 63, 65, 66, 67, 70, 71, 72, 73, 75, 77, 81, 87, 90, 92, 94, 97, 98, 99, 106, 108, 111, 113, 115, 116, 119, 120, 127, 128, 130, 132, 133, 136, 138, 139, 140, 141, 142, 143, 144, 145, 148, 150, 151, 152, 153, 169, 171, 173, 175, 177, 178, 183, 184, 185, 187, 188, 190, 193, 194, 198, 203, 204, 205, 207, 210, 213, 214, 218, 225, 226, 227, 230, 232, 236, 237, 242, 245, 246, 247, 248, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 263, 264, 267, 270, 271, 274, 275, 277, 282, 283, 284, 286, 288, 290, 291, 295, 297, 300, 302, 303, 304, 305, 308, 309, 310, 311, 312, 314, 316, 318, 320, 322, 325, 326, 329, 331, 332, 333, 334, 335, 349, 350, 352, 359, 361, 364, 365, 368, 369, 370, 371, 372, 373, 375, 377, 385], "write": [0, 1, 3, 4, 6, 68, 179, 211, 364, 374, 375, 378, 387], "activ": [0, 4, 16, 86, 126, 135, 145, 168, 170, 179, 191, 192, 195, 199, 201, 206, 211, 228, 229, 287, 317, 323, 364, 371, 375, 378, 388], "weight": [0, 41, 82, 83, 134, 166, 174, 179, 252, 280, 364, 369, 375, 378, 386, 387, 388], "first": [0, 3, 122, 128, 133, 179, 211, 235, 242, 267, 325, 364, 366, 368, 369, 371, 373, 375, 378, 384, 390], "iter": [0, 375, 378], "subsequ": [0, 375, 378, 384, 390], "version": [0, 221, 222, 347, 371, 373, 375, 378, 380, 383, 384, 386, 387], "process": [0, 60, 373, 375, 378, 387], "paramet": [0, 5, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 241, 242, 243, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 369, 375, 378, 380, 386], "check": [0, 2, 6, 211, 365, 368, 370, 371, 375, 378, 387], "match": [0, 58, 211, 221, 222, 279, 368, 369, 374, 375, 378, 383, 384, 386, 387, 388], "origin": [0, 104, 221, 222, 243, 253, 365, 372, 375, 378, 383], "implement": [0, 3, 100, 211, 346, 347, 348, 364, 369, 372, 373, 375, 378, 380], "tracer": [0, 375, 386, 388, 389, 390], "modul": [0, 6, 221, 222, 364, 369, 370, 375, 380, 382, 383, 384, 385, 389], "written": [0, 6, 163, 164, 179, 211, 375, 382, 385], "profil": [0, 3, 151, 292, 367, 371, 375, 390], "resnet": [0, 370, 373, 375, 387], "block": [0, 68, 121, 210, 211, 214, 262, 369, 374, 375], "torchvis": [0, 375, 380, 387, 389], "preprocess": [0, 6, 29, 221, 222, 374, 375, 380], "displai": [0, 371, 375, 376, 380], "pass": [0, 6, 102, 103, 110, 265, 266, 348, 364, 366, 369, 372, 375, 380, 383, 386, 387], "constructor": [0, 368, 375, 380], "dit_xl_2": [0, 375, 389], "With": [0, 375], "pytorch": [0, 3, 5, 28, 104, 211, 242, 342, 346, 348, 367, 369, 375, 376, 387], "base": [0, 3, 4, 6, 41, 60, 68, 211, 243, 369, 371, 374, 375, 376, 385], "librari": [0, 4, 5, 366, 367, 368, 375, 376], "http": [0, 24, 264, 370, 371, 375, 376, 387], "github": [0, 2, 24, 264, 370, 371, 375, 376], "com": [0, 24, 264, 370, 371, 375, 376], "facebookresearch": [0, 375, 376], "dit": [0, 375, 376], "git": [0, 221, 222, 371, 375, 376, 387], "xl": [0, 375, 376], "sampl": [0, 369, 375, 376], "train": [0, 41, 369, 375, 376], "onboard": 0, "new": [0, 85, 124, 239, 276, 279, 363, 365, 367, 373, 383, 386], "rewrit": 0, "switch": [0, 307], "ad": [0, 179, 368, 369, 372, 383], "faq": 0, "need": [0, 1, 2, 179, 211, 365, 366, 369, 373, 374, 383, 384, 385, 390], "bind": [0, 387], "golden": [0, 364, 390], "perf": [0, 383, 384, 385, 386, 387, 388], "header": [0, 3], "profile_thi": [0, 387], "descript": [0, 109, 110, 368, 369, 372], "lib": [0, 4, 367, 386, 387, 389], "overview": [0, 367], "infrastructur": [0, 367], "member": [0, 2, 367, 368], "input": [0, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 119, 120, 122, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 241, 242, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 363, 365, 366, 367, 368, 373, 374, 384, 385, 388, 389, 390], "fast": [0, 89, 91, 95, 129, 285, 293, 367], "dispatch": [0, 240, 367, 368, 373], "cpu": [0, 3, 366, 367, 368, 369, 371, 373, 387, 389], "through": [0, 367, 371, 389], "primari": [0, 367], "softmax_backward": [0, 367, 369], "softmin": [0, 367, 369], "softmin_backward": [0, 367, 369], "logsoftmax": [0, 367, 369], "logsoftmax_backward": [0, 367, 369], "mean_backward": [0, 367, 369], "group_norm_backward": [0, 367, 369], "norm": [0, 41, 367, 369], "norm_backward": [0, 367, 369], "enum": [0, 367], "bcastopmath": [0, 367, 369], "bcastopdim": [0, 367, 369], "fallback": [0, 366, 367, 372], "tensor_slic": [0, 367, 369], "chunk": [0, 99, 264, 346, 347, 367, 369, 384, 389], "conv2d": [0, 367, 369, 383, 384, 385, 386, 388], "interpol": [0, 174, 367, 369], "batchnorm2d": [0, 367, 369, 388], "groupnorm": [0, 367, 369], "layernorm": [0, 367, 369], "maxpool2d": [0, 367, 369], "adaptiveavgpool2d": [0, 367, 369], "unary_fmod": [0, 367, 369], "binary_fmod": [0, 367, 369], "unary_bitwise_or": [0, 367, 369], "unary_bitwise_and": [0, 367, 369], "unary_bitwise_xor": [0, 367, 369], "binary_bitwise_or": [0, 367, 369], "binary_bitwise_and": [0, 367, 369], "binary_bitwise_xor": [0, 367, 369], "unary_bitwise_left_shift": [0, 367, 369], "unary_bitwise_right_shift": [0, 367, 369], "binary_bitwise_left_shift": [0, 367, 369], "binary_bitwise_right_shift": [0, 367, 369], "torch_argmax": [0, 367, 369], "torch_argmin": [0, 367, 369], "fuse": [0, 5, 364, 367, 385], "mini": [0, 367], "addandnorm": [0, 367, 369], "complex": [0, 25, 26, 62, 63, 152, 153, 245, 246, 259, 260, 367], "__init__": [0, 364, 367, 368, 388], "get_dtyp": [0, 6, 367, 368], "get_layout": [0, 6, 367, 368], "pad_to_til": [0, 367, 368], "storage_typ": [0, 367, 368], "unpad": [0, 58, 118, 339, 355, 366, 367, 368, 369], "unpad_from_til": [0, 367, 368], "memoryconfig": [0, 6, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 80, 81, 82, 83, 84, 85, 86, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 122, 123, 124, 125, 126, 128, 129, 130, 131, 132, 133, 134, 135, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 165, 166, 167, 168, 169, 170, 172, 173, 174, 175, 176, 177, 178, 179, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 201, 203, 204, 205, 206, 208, 209, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 224, 225, 226, 227, 228, 229, 231, 232, 233, 234, 235, 236, 237, 238, 239, 241, 242, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 342, 343, 344, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 367, 369], "between": [0, 6, 27, 41, 211, 340, 367, 369], "one": [0, 6, 54, 55, 56, 57, 367, 369], "op": [0, 6, 8, 28, 68, 83, 214, 306, 327, 346, 347, 353, 367, 369, 372, 373, 387, 390], "acceler": [0, 336, 337, 354, 355, 367, 368, 369, 384, 387], "odd": [0, 367], "size": [0, 27, 78, 82, 83, 211, 214, 240, 252, 253, 305, 346, 347, 348, 356, 367, 368, 369, 374, 383, 390], "last": [0, 28, 58, 104, 123, 128, 133, 238, 267, 279, 325, 339, 342, 348, 362, 367, 368, 369, 373, 374], "dim": [0, 24, 28, 60, 61, 68, 99, 100, 104, 128, 133, 154, 212, 216, 217, 244, 253, 254, 264, 267, 278, 305, 306, 319, 324, 325, 342, 348, 357, 367, 368, 369, 374, 385, 389], "uplift": 0, "next": [0, 233, 369, 374], "file": [0, 2, 4, 6, 29, 79, 180, 365, 371, 373, 387, 389, 390], "bug": 0, "featur": [0, 5, 369, 372, 379, 390], "propos": [0, 372], "request": [0, 339, 372, 387, 389], "troubleshoot": [0, 365], "tip": 0, "commun": 0, "develop": [0, 4, 5, 370, 371, 372, 373, 387], "index": [0, 163, 164, 342, 347, 368, 373, 387, 389], "search": 0, "page": [0, 371, 375], "If": [1, 2, 6, 28, 58, 68, 104, 179, 211, 221, 222, 240, 242, 244, 305, 306, 327, 341, 342, 347, 348, 368, 369, 370, 371, 372, 373, 386, 390], "would": [1, 372, 373, 374], "like": [1, 111, 307, 364, 366, 374, 383, 390], "thi": [1, 3, 4, 6, 8, 9, 28, 41, 58, 60, 83, 102, 103, 104, 107, 117, 118, 132, 151, 169, 214, 242, 243, 272, 273, 307, 346, 364, 365, 366, 368, 369, 371, 372, 373, 374, 375, 376, 379, 380, 383, 384, 385, 389, 390], "project": [1, 2, 4, 5, 370], "pleas": [1, 2, 211, 365, 369, 370, 372, 375, 390], "review": [1, 370, 372], "standard": [1, 2, 273, 365, 370], "gain": 1, "access": [1, 2, 5, 371, 389], "read": [1, 68, 369, 370, 374], "section": [1, 2, 365, 369, 374], "detail": [1, 6, 11, 21, 42, 75, 116, 130, 148, 183, 185, 187, 188, 190, 193, 194, 204, 205, 251, 254, 258, 263, 275, 308, 314, 361, 370, 389, 390], "contact": 1, "u": [1, 372], "have": [2, 3, 4, 6, 28, 69, 83, 118, 174, 211, 257, 336, 337, 342, 354, 355, 365, 368, 369, 371, 373, 374, 375, 383, 389], "formal": 2, "permiss": 2, "cloud": 2, "issu": [2, 211, 307, 365, 372, 373, 383, 384, 385, 386, 387, 388], "can": [2, 3, 4, 5, 6, 27, 42, 54, 55, 56, 57, 74, 75, 85, 124, 130, 211, 239, 257, 258, 265, 266, 307, 327, 363, 364, 366, 368, 369, 370, 371, 373, 374, 375, 383, 384, 385, 386, 388, 389, 390], "out": [2, 6, 100, 101, 104, 273, 305, 342, 369, 371, 383, 385, 388], "relev": [2, 365], "ever": 2, "help": [2, 372, 375], "we": [2, 3, 4, 118, 122, 211, 341, 365, 366, 369, 372, 374, 376, 379, 380, 383, 384, 389, 390], "offici": 2, "discord": 2, "channel": [2, 24, 41, 78, 99, 109, 110, 132, 214, 264, 356, 369, 373], "repres": [2, 5, 368, 373, 374, 383], "both": [2, 17, 211, 250, 364, 365, 368, 369, 373, 374, 383, 388], "tenstorr": [2, 6, 24, 264, 364, 365, 370, 371, 375, 383, 384, 389, 390], "metal": [2, 5, 24, 264, 368, 370, 371, 375, 383, 384, 385, 386, 387, 388, 389], "join": [2, 387], "discuss": [2, 365], "board": 2, "bounc": 2, "idea": [2, 365], "off": [2, 272, 281, 364, 374], "each": [2, 3, 41, 60, 110, 132, 214, 242, 276, 278, 305, 368, 369, 373, 374], "other": [2, 6, 364, 365, 369, 370, 374, 376, 379, 380, 390], "refer": [2, 3, 4, 5, 11, 21, 42, 69, 75, 85, 107, 116, 130, 148, 183, 185, 187, 188, 190, 193, 194, 204, 205, 211, 227, 251, 254, 258, 263, 275, 308, 314, 361, 368, 372, 374, 390], "code": [2, 6, 28, 104, 242, 265, 266, 342, 348, 366, 369, 370, 371, 372, 373, 383, 387, 390], "conduct": 2, "when": [2, 6, 8, 9, 41, 60, 68, 75, 103, 148, 210, 211, 221, 244, 250, 258, 339, 365, 368, 369, 372, 374, 383, 385, 386, 388, 390], "interact": 2, "ensur": [3, 4, 58, 104, 327, 365, 371, 374], "tt": [3, 4, 9, 24, 264, 336, 337, 354, 355, 356, 367, 375, 376, 379, 380, 383, 384, 385, 386, 387, 388, 389], "requir": [3, 6, 17, 19, 34, 41, 61, 68, 75, 109, 110, 123, 221, 222, 225, 238, 288, 320, 322, 359, 362, 365, 368, 369, 371, 373, 379, 387, 389], "model": [3, 5, 221, 222, 365, 372, 373, 375, 376, 378, 382, 383, 384, 387, 388], "follow": [3, 6, 104, 110, 211, 279, 364, 366, 368, 369, 370, 371, 372, 373, 374, 375, 389, 390], "instruct": [3, 4, 365, 370, 371, 375, 390], "readi": [3, 4, 348, 365], "come": [3, 370, 373], "typic": [3, 132, 374], "found": [3, 6, 364, 375, 387, 389], "under": [3, 4, 365, 366, 372, 373, 375, 390], "your_model": 3, "perf_model": 3, "py": [3, 6, 364, 365, 373, 386, 387, 390], "pytest": [3, 4, 364, 365, 373, 387, 390], "test": [3, 4, 6, 364, 365, 372, 373, 385, 387, 390], "python_api_test": 3, "perf_your_model": 3, "csv": [3, 373, 387], "perf_your_model_d": 3, "contain": [3, 4, 6, 27, 82, 214, 262, 368, 374, 383], "tabl": [3, 387], "two": [3, 58, 60, 128, 133, 211, 267, 279, 325, 339, 348, 364, 368, 369, 374], "row": [3, 51, 108, 110, 111, 113, 120, 235, 282, 297, 352, 368, 373, 374, 383, 384, 387], "batch": [3, 41, 78, 109, 110, 154, 179, 211, 214, 347, 368, 369, 373], "sec": 3, "second": [3, 122, 128, 133, 179, 211, 242, 252, 267, 279, 325, 368, 369, 373, 385, 387, 390], "compil": [3, 369, 384, 387, 390], "time": [3, 100, 157, 221, 276, 365, 369, 373, 384, 385, 387, 390], "infer": [3, 41, 365, 373, 387], "g": [3, 211, 369, 371, 373, 374], "throughput": 3, "inf": [3, 220, 226], "vit": 3, "patch16": 3, "30": [3, 61, 84, 387], "51": [3, 386], "16": [3, 305, 374, 385, 387, 388, 389], "05": [3, 41, 369, 389], "46": [3, 387], "0": [3, 6, 9, 10, 12, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 34, 35, 41, 43, 44, 45, 46, 47, 48, 50, 54, 55, 56, 57, 59, 60, 61, 62, 64, 69, 72, 74, 75, 80, 82, 83, 84, 85, 86, 88, 89, 91, 93, 95, 96, 98, 100, 101, 102, 105, 107, 108, 109, 110, 112, 117, 121, 122, 123, 124, 126, 129, 130, 131, 135, 137, 138, 139, 140, 142, 146, 149, 151, 152, 154, 155, 156, 157, 158, 159, 160, 161, 162, 165, 168, 170, 172, 173, 174, 176, 177, 180, 181, 182, 186, 189, 191, 192, 195, 197, 199, 201, 203, 205, 206, 208, 210, 211, 214, 220, 224, 225, 228, 229, 231, 232, 234, 235, 238, 239, 240, 242, 244, 245, 249, 250, 251, 253, 254, 257, 258, 259, 261, 262, 264, 268, 269, 272, 273, 278, 279, 281, 285, 286, 287, 288, 293, 294, 296, 298, 299, 300, 301, 305, 306, 307, 309, 310, 313, 314, 315, 317, 320, 321, 322, 323, 327, 328, 330, 331, 334, 335, 336, 337, 338, 339, 340, 341, 342, 346, 347, 348, 349, 350, 351, 353, 354, 355, 358, 359, 362, 363, 364, 366, 368, 369, 371, 373, 374, 383, 384, 385, 386, 387, 388, 389, 390], "0623": 3, "29": [3, 371, 387], "4960": 3, "includ": [3, 5, 6, 58, 365, 374], "without": [3, 85, 366, 368, 369, 371], "ani": [3, 109, 110, 365, 366, 369, 387], "abovement": 3, "grayskul": [3, 6, 29, 281, 364, 371, 375, 376, 379, 380, 385, 386, 390], "It": [3, 5, 6, 221, 222, 341, 368, 369, 383], "sinc": [3, 151, 366], "dure": [3, 60, 122, 157, 341, 374], "do": [3, 365, 366, 385], "pai": 3, "name": [3, 6, 29, 79, 180, 221, 222, 346, 364, 368, 369, 372, 373, 387, 388, 389, 390], "suggest": 3, "calcul": [3, 68, 100, 257, 373], "comput": [3, 5, 6, 18, 20, 22, 36, 58, 74, 76, 114, 125, 132, 134, 147, 157, 166, 167, 168, 174, 179, 191, 192, 195, 196, 199, 200, 201, 202, 209, 211, 215, 219, 233, 241, 249, 253, 280, 289, 306, 317, 321, 343, 344, 347, 348, 358, 360, 369, 373, 383, 384], "": [3, 4, 6, 58, 100, 221, 222, 346, 347, 365, 368, 369, 371, 374, 383, 384, 385, 389], "also": [3, 6, 211, 227, 365, 366, 368, 369, 370, 373, 389], "maintain": [3, 372], "run_perform": [3, 365], "sh": [3, 4, 365, 371, 373, 387], "facilit": 3, "easi": [3, 383], "wai": [3, 5, 221, 222, 364, 369, 371, 383], "attempt": [3, 369, 387], "fastest": 3, "command": [3, 10, 12, 16, 17, 19, 28, 30, 34, 35, 41, 43, 44, 45, 46, 47, 48, 50, 60, 61, 64, 74, 75, 80, 82, 83, 86, 88, 89, 91, 93, 95, 96, 98, 100, 101, 102, 105, 107, 108, 109, 110, 112, 121, 122, 123, 124, 126, 129, 130, 131, 135, 137, 146, 149, 151, 154, 158, 159, 160, 161, 162, 165, 168, 170, 172, 176, 181, 182, 186, 189, 191, 192, 195, 197, 199, 201, 206, 208, 220, 224, 225, 228, 229, 231, 232, 234, 235, 239, 242, 244, 250, 251, 257, 262, 268, 269, 272, 273, 279, 281, 285, 286, 287, 288, 293, 294, 296, 298, 299, 300, 301, 307, 313, 314, 315, 317, 320, 322, 323, 327, 328, 330, 331, 336, 337, 338, 341, 342, 346, 347, 351, 353, 354, 355, 358, 359, 363, 371, 373, 390], "merg": [3, 372], "built": [4, 5, 371, 387, 389], "now": [4, 100, 122, 341, 356, 374, 383, 385], "root": [4, 286, 314, 343, 344], "provid": [4, 24, 28, 41, 100, 107, 115, 179, 211, 221, 222, 257, 264, 274, 306, 327, 330, 337, 342, 365, 368, 369, 372, 373, 374, 383, 385, 390], "virtual": [4, 371], "which": [4, 44, 47, 68, 104, 123, 124, 179, 211, 214, 221, 222, 238, 239, 257, 305, 306, 346, 347, 362, 363, 364, 368, 369, 373, 374], "ll": 4, "work": [4, 211, 305, 364, 365, 375, 376, 379, 380, 390], "python_env": [4, 371, 386, 387, 389], "bin": [4, 79, 180, 371, 387], "python_env_dir": 4, "variabl": [4, 6, 369, 371, 386, 388, 390], "create_venv": [4, 371], "control": [4, 369, 383], "pythonpath": [4, 371, 389], "common": [4, 125, 167, 365, 369], "practic": 4, "export": [4, 369, 371, 390], "pwd": [4, 371], "folder": [4, 365, 370, 373, 387], "split": [4, 60, 128, 133, 267, 325, 348, 369, 374], "them": [4, 348, 369, 371, 373, 383], "sub": [4, 327, 354, 369], "In": [4, 6, 101, 211, 279, 339, 344, 364, 366, 369, 373, 374, 383, 389], "find": [4, 371, 383, 384, 385, 386, 387, 388], "prepar": [4, 365, 387], "readm": [4, 365, 371, 387, 389], "md": [4, 24, 264, 365, 371], "give": [4, 373], "how": [4, 6, 365, 368, 369, 373, 374, 384, 385, 390], "progress": [4, 389], "yet": 4, "mani": [4, 6, 364, 384, 389], "part": [4, 128, 133, 267, 325, 365, 369, 373, 385], "entir": [4, 132, 327], "path_to_test_fil": 4, "test_in_fil": 4, "ttnn": [4, 6, 368, 369, 371, 372, 374, 375, 378, 380, 381, 384, 389, 390], "friendli": [4, 370], "top": [4, 342, 368, 375], "doc": [4, 6, 387], "document": [4, 6, 365, 372, 387], "neural": 5, "network": [5, 369], "design": 5, "feel": 5, "familiar": 5, "experienc": 5, "kei": [5, 348, 364, 374, 383, 385, 387], "than": [5, 60, 72, 109, 110, 126, 127, 135, 136, 170, 171, 177, 206, 207, 211, 227, 369, 371, 373, 385, 390], "200": 5, "convolut": [5, 60, 214, 369, 388], "A": [5, 6, 27, 123, 124, 211, 238, 239, 307, 347, 362, 363, 365, 366, 368, 369, 371, 372, 374], "differ": [5, 211, 317, 368, 374, 383, 390], "distribut": [5, 68, 79, 179, 211, 374], "The": [5, 6, 8, 9, 24, 27, 29, 58, 59, 78, 79, 80, 82, 83, 84, 85, 99, 102, 104, 107, 110, 122, 123, 124, 132, 146, 172, 179, 210, 211, 214, 227, 238, 239, 240, 243, 264, 265, 266, 272, 273, 276, 327, 338, 341, 342, 346, 347, 356, 362, 363, 364, 365, 366, 368, 369, 370, 372, 373, 374, 383, 384, 389, 390], "abil": [5, 369], "custom": [5, 6, 265, 266, 385], "nativ": 5, "mesh": [5, 24, 122, 264, 339, 341, 371], "tool": [5, 371, 373, 387], "util": [5, 368, 371, 374, 383, 384, 389], "significantli": [5, 384], "speed": [5, 384], "load": [5, 180, 371, 386, 387, 389], "comparison": [5, 157, 372], "mode": [5, 34, 41, 42, 75, 89, 91, 95, 121, 129, 130, 165, 214, 224, 258, 285, 293, 369, 383, 384, 385, 386, 387, 388], "long": [5, 372], "sequenc": [5, 6, 346, 347], "against": [5, 365, 390], "known": [5, 369], "meant": 6, "contributor": 6, "Not": [6, 203, 230, 281, 364, 390], "mai": [6, 69, 75, 148, 169, 211, 258, 262, 364, 369, 374, 390], "wormhol": [6, 29, 364, 371, 375, 390], "take": [6, 345, 365, 368, 369, 370, 374, 383], "produc": [6, 262, 365, 366, 369, 383, 384], "call": [6, 100, 122, 265, 266, 341, 366, 368, 369, 372, 373, 374, 383, 385, 387, 390], "optiona": 6, "composit": 6, "struct": [6, 369], "specifi": [6, 27, 58, 79, 84, 85, 104, 123, 124, 179, 211, 221, 222, 238, 239, 242, 243, 244, 253, 276, 327, 337, 346, 347, 362, 363, 364, 368, 369, 384, 385], "simpli": [6, 339, 368, 369, 383], "defin": [6, 29, 368, 369, 372, 374], "method": [6, 369, 371, 387], "register_oper": 6, "exist": [6, 240, 368, 387, 389], "bind_registered_oper": 6, "auto": [6, 369], "attach": [6, 221, 222, 387], "attach_golden_funct": 6, "let": [6, 374, 383, 385], "just": [6, 385, 389], "copi": [6, 58, 100, 121, 151, 338, 368, 369, 383], "order": [6, 68, 104, 211, 248, 288, 342, 368, 369, 373, 374, 375, 383, 385, 390], "directori": [6, 375, 389], "structur": [6, 364], "shown": [6, 374], "below": [6, 100, 211, 365, 369, 373, 374, 375], "cpp": 6, "categori": 6, "operation_nam": 6, "_device_oper": 6, "hpp": 6, "program_factory_0": 6, "_program_factori": 6, "factori": 6, "But": [6, 366], "concret": [6, 211], "example_device_oper": 6, "spdx": [6, 390], "filecopyrighttext": [6, 390], "2023": [6, 387, 389], "inc": [6, 390], "licens": [6, 390], "identifi": [6, 387, 390], "apach": [6, 390], "pragma": 6, "onc": [6, 371, 384], "variant": 6, "device_oper": 6, "decor": [6, 372, 383, 384, 385, 386, 387, 388], "namespac": [6, 389], "exampledeviceoper": 6, "store": [6, 41, 368, 373, 374, 383], "aren": [6, 8], "t": [6, 8, 99, 151, 211, 221, 222, 264, 366, 368, 369, 373, 383, 385, 387], "operation_attributes_t": 6, "bool": [6, 17, 19, 29, 34, 41, 61, 68, 69, 74, 75, 89, 91, 95, 121, 129, 134, 157, 165, 179, 211, 214, 221, 222, 224, 225, 253, 254, 285, 288, 293, 320, 322, 336, 337, 342, 343, 344, 346, 347, 348, 354, 355, 359, 369], "int": [6, 10, 12, 16, 17, 19, 24, 27, 28, 30, 34, 35, 41, 43, 44, 45, 46, 47, 48, 50, 60, 61, 64, 68, 74, 75, 80, 82, 83, 84, 86, 88, 89, 91, 93, 95, 96, 98, 99, 100, 101, 102, 103, 105, 107, 108, 109, 110, 112, 118, 121, 122, 123, 124, 126, 128, 129, 131, 133, 134, 135, 137, 146, 149, 151, 154, 158, 159, 160, 161, 162, 163, 164, 165, 168, 170, 172, 176, 179, 181, 182, 186, 189, 191, 192, 195, 197, 199, 201, 206, 208, 210, 211, 214, 220, 224, 225, 228, 229, 231, 232, 234, 235, 239, 240, 242, 243, 244, 247, 250, 251, 253, 254, 257, 262, 264, 267, 268, 269, 272, 273, 277, 281, 285, 287, 288, 293, 294, 296, 298, 299, 300, 301, 305, 307, 313, 314, 315, 317, 320, 322, 323, 325, 327, 328, 330, 331, 336, 337, 338, 341, 342, 343, 344, 346, 347, 348, 351, 353, 354, 355, 356, 358, 359, 363, 368, 369, 374, 388, 389], "some_other_attribut": 6, "argument": [6, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 60, 61, 62, 63, 64, 65, 66, 67, 70, 71, 72, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83, 85, 86, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 119, 120, 121, 122, 125, 126, 128, 129, 130, 131, 132, 133, 134, 135, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 165, 166, 167, 168, 169, 170, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 201, 203, 204, 205, 206, 208, 209, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 224, 225, 226, 227, 228, 229, 231, 232, 233, 234, 235, 236, 237, 240, 241, 242, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 368, 369, 383, 386, 388], "don": [6, 211, 369, 383], "thei": [6, 211, 365, 369, 384, 385], "tensor_args_t": 6, "const": [6, 292, 369], "input_tensor": [6, 10, 11, 12, 13, 14, 15, 24, 25, 26, 28, 30, 31, 32, 33, 34, 35, 38, 39, 40, 41, 42, 45, 49, 50, 51, 52, 53, 54, 55, 56, 57, 60, 62, 63, 64, 65, 66, 67, 70, 71, 72, 73, 75, 77, 78, 80, 81, 82, 83, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 111, 112, 113, 117, 119, 120, 127, 128, 129, 130, 131, 132, 133, 134, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 149, 150, 151, 152, 153, 155, 156, 158, 159, 160, 161, 162, 163, 166, 171, 172, 173, 176, 177, 178, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 197, 198, 203, 204, 205, 207, 208, 214, 220, 226, 227, 230, 231, 232, 234, 235, 236, 237, 239, 242, 244, 245, 246, 247, 248, 249, 250, 251, 253, 254, 255, 256, 257, 258, 259, 260, 262, 263, 264, 267, 268, 269, 270, 271, 272, 273, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 290, 291, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 325, 326, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 342, 343, 344, 345, 348, 349, 350, 351, 352, 353, 354, 355, 356, 363, 369, 386, 388, 390], "howev": [6, 369, 383], "show": [6, 211, 368, 374, 384, 385], "els": [6, 74, 368, 386, 388, 389], "done": [6, 365, 368, 371, 373, 383, 387], "io_tensor": 6, "optional_output_tensor": [6, 179, 211], "vector": [6, 249, 368, 369], "vector_of_tensor": 6, "tupl": [6, 68, 242, 348, 369], "tuple_of_tensor": 6, "vector_of_optional_tensor": 6, "some_crazy_tuple_of_tensor": 6, "return": [6, 8, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 321, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 366, 368, 369, 385, 386, 387, 388, 389], "spec": [6, 68, 368, 369], "singl": [6, 347, 364, 372, 373, 374, 383], "tensorspec": [6, 369], "spec_return_value_t": 6, "tensor_return_value_t": 6, "note": [6, 63, 100, 153, 211, 238, 239, 260, 279, 366, 368, 369, 371, 373, 374, 383, 385, 389, 390], "should": [6, 121, 128, 133, 151, 157, 174, 211, 214, 235, 242, 250, 267, 325, 339, 365, 368, 372, 373, 383, 384, 385, 386, 388], "same": [6, 16, 18, 20, 22, 36, 43, 44, 46, 47, 48, 74, 76, 83, 85, 86, 87, 100, 107, 109, 114, 124, 125, 126, 127, 135, 136, 147, 151, 157, 167, 168, 170, 171, 174, 179, 191, 192, 195, 196, 199, 200, 201, 202, 206, 207, 209, 211, 215, 219, 221, 222, 228, 229, 230, 233, 235, 239, 241, 249, 250, 287, 289, 317, 321, 323, 339, 342, 346, 358, 360, 363, 369, 373, 374, 383, 385], "pattern": [6, 60, 221, 372], "e": [6, 211, 369, 373, 374, 390], "singlecor": 6, "share": [6, 368, 374], "override_runtime_argu": 6, "shared_variables_t": 6, "tt_metal": [6, 356, 369, 371, 373, 387], "kernelhandl": 6, "unary_reader_kernel_id": [6, 369], "unary_writer_kernel_id": [6, 369], "cached_program_t": 6, "cachedprogram": 6, "static": 6, "operation_attribut": 6, "tensor_arg": 6, "tensor_return_valu": 6, "void": [6, 369], "cached_program": 6, "multicor": [6, 336, 337, 354, 355], "size_t": 6, "num_cor": 6, "num_cores_i": 6, "program_factory_t": 6, "mandatori": [6, 369], "select": [6, 41, 368, 372], "arg": [6, 110, 244, 265, 266, 327, 338, 368, 390], "select_program_factori": 6, "valid": [6, 24, 99, 109, 110, 211, 221, 222, 264, 364, 365, 368, 369, 373, 374, 387], "usual": 6, "validate_on_program_cache_miss": 6, "reus": 6, "less": [6, 170, 171, 206, 207, 369, 371, 373, 390], "validate_on_program_cache_hit": 6, "compute_output_spec": [6, 369], "create_output_tensor": [6, 369], "map": [6, 29, 383, 384, 385, 386, 387, 388], "abl": 6, "prim": 6, "after": [6, 242, 265, 366, 372, 373, 383, 390], "keep": [6, 253, 364, 374], "mind": [6, 385], "overload": [6, 327, 338, 368, 383], "queue_id": [6, 10, 12, 16, 17, 19, 28, 30, 34, 35, 41, 43, 44, 45, 46, 47, 48, 50, 60, 61, 64, 74, 75, 80, 82, 83, 86, 88, 89, 91, 93, 95, 96, 98, 100, 101, 102, 105, 107, 108, 109, 110, 112, 123, 124, 126, 129, 130, 131, 135, 137, 146, 149, 151, 154, 158, 159, 160, 161, 162, 165, 168, 170, 172, 176, 181, 182, 186, 189, 191, 192, 195, 197, 199, 201, 206, 208, 214, 220, 224, 225, 228, 229, 231, 232, 234, 235, 239, 242, 244, 250, 251, 257, 262, 268, 269, 272, 273, 279, 281, 285, 286, 287, 288, 293, 294, 296, 298, 299, 300, 301, 305, 307, 313, 314, 315, 317, 320, 322, 323, 328, 330, 331, 336, 337, 342, 346, 347, 351, 353, 354, 355, 358, 359, 363], "automat": [6, 210, 339, 365, 366, 369, 373, 374, 383, 384], "primit": 6, "so": [6, 110, 364, 366, 368, 369, 383, 389], "case": [6, 74, 122, 157, 211, 233, 252, 339, 364, 365, 369, 374, 383, 390], "hash": [6, 221, 222, 373], "stl": 6, "hash_t": 6, "compute_program_hash": 6, "create_op_performance_model": 6, "opperformancemodel": 6, "make": [6, 221, 222, 307, 342, 348, 364, 374, 387, 390], "avail": [6, 203, 356, 366, 368, 375, 376, 379, 380, 390], "constexpr": 6, "some_condition_based_on_operation_attributes_and_or_tensor_arg": 6, "true": [6, 11, 13, 15, 17, 19, 21, 23, 31, 33, 34, 37, 38, 40, 41, 42, 51, 53, 55, 57, 61, 65, 67, 68, 69, 71, 73, 74, 75, 81, 83, 89, 90, 91, 92, 94, 95, 97, 98, 102, 104, 106, 108, 111, 113, 116, 120, 121, 129, 130, 134, 139, 141, 143, 145, 148, 150, 169, 173, 175, 178, 183, 185, 187, 188, 190, 193, 194, 204, 205, 213, 218, 225, 227, 232, 251, 254, 256, 258, 263, 270, 271, 275, 277, 282, 285, 286, 288, 291, 293, 295, 297, 300, 302, 304, 308, 310, 312, 314, 316, 318, 320, 322, 329, 331, 333, 335, 336, 337, 342, 346, 347, 348, 352, 354, 355, 359, 361, 364, 368, 369, 383, 384, 385, 386, 387, 388, 389, 390], "get_logical_shap": 6, "tensorlayout": 6, "pageconfig": 6, "output_spec": 6, "create_device_tensor": 6, "42": [6, 384, 387, 389], "single_core_program_factori": 6, "work_split": 6, "output_tensor": [6, 10, 12, 16, 28, 30, 35, 43, 44, 45, 46, 47, 48, 50, 60, 64, 74, 80, 82, 83, 86, 88, 89, 91, 93, 95, 96, 98, 101, 105, 107, 108, 112, 118, 123, 124, 126, 129, 130, 131, 135, 137, 146, 149, 151, 158, 159, 160, 161, 162, 165, 168, 170, 172, 176, 181, 182, 186, 189, 191, 192, 195, 197, 199, 201, 206, 208, 220, 224, 228, 229, 231, 232, 234, 239, 250, 251, 257, 262, 268, 269, 272, 273, 281, 285, 286, 287, 293, 294, 296, 298, 299, 300, 301, 307, 313, 314, 315, 317, 323, 328, 330, 331, 342, 351, 353, 358, 359, 363, 369, 383, 388, 390], "src_buffer": 6, "dst_buffer": 6, "dataformat": 6, "cb_data_format": 6, "datatype_to_dataformat_convert": 6, "uint32_t": [6, 101], "single_tile_s": 6, "tiles": 6, "cb_data_format_output": 6, "single_tile_size_output": 6, "num_til": 6, "volum": 6, "constant": [6, 369, 374], "tile_hw": 6, "idevic": [6, 8, 9, 117, 118, 240, 327, 338, 368], "corecoord": [6, 369], "compute_with_storage_grid_s": 6, "num_cores_x": [6, 364, 385], "x": [6, 36, 179, 211, 245, 346, 347, 368, 369, 371, 373, 374, 384, 385, 388, 389], "y": [6, 36, 245, 368, 369, 373, 374, 384, 385, 389], "all_cor": 6, "core_group_1": 6, "core_group_2": 6, "num_tiles_per_core_group_1": 6, "num_tiles_per_core_group_2": 6, "split_work_to_cor": 6, "src0_cb_index": 6, "cbindex": 6, "c_0": 6, "num_input_til": 6, "circularbufferconfig": 6, "cb_src0_config": 6, "set_page_s": 6, "cb_src0": 6, "createcircularbuff": 6, "output_cb_index": 6, "c_2": 6, "num_output_til": 6, "cb_output_config": 6, "cb_output": 6, "src_is_dram": 6, "buffer_typ": [6, 368, 369], "buffertyp": [6, 368, 369], "dram": [6, 340, 368, 369, 374, 383], "reader_compile_time_arg": 6, "dst_is_dram": 6, "writer_compile_time_arg": 6, "createkernel": 6, "eltwis": [6, 115, 252, 274, 369], "kernel": [6, 58, 102, 179, 211, 214, 306, 369, 371, 373, 384], "dataflow": 6, "reader_unary_interleaved_start_id": 6, "readerdatamovementconfig": 6, "writer_unary_interleaved_start_id": 6, "writerdatamovementconfig": 6, "compute_kernel_args_group_1": 6, "per_core_block_cnt": 6, "per_core_block_s": 6, "math_approx_mod": 6, "fals": [6, 29, 41, 68, 69, 74, 89, 91, 95, 104, 121, 129, 157, 179, 211, 214, 253, 285, 293, 330, 342, 343, 344, 353, 364, 369, 383, 384, 385, 386, 387, 388, 389, 390], "eltwise_unary_kernel_group_1_id": 6, "eltwise_sfpu": 6, "computeconfig": 6, "math_fidel": [6, 388], "mathfidel": [6, 388], "hifi4": [6, 373, 387], "compile_arg": 6, "rang": [6, 27, 44, 45, 47, 66, 109, 110, 125, 150, 167, 169, 184, 220, 226, 247, 281, 283, 303, 328, 329, 330, 368, 369, 374], "compute_kernel_args_group_2": 6, "eltwise_unary_kernel_group_2_id": 6, "num_tiles_written": 6, "num_tiles_per_cor": 6, "tt_assert": 6, "setruntimearg": 6, "address": [6, 369], "move": [6, 117, 118, 365, 366, 368, 369, 383, 385, 386, 387], "shared_vari": 6, "runtime_arg": [6, 369], "getruntimearg": [6, 369], "multi_core_program_factori": 6, "compositeexampleoper": 6, "composite_exampl": 6, "another_copi": 6, "_pybind": 6, "example_pybind": 6, "pybind11": 6, "h": [6, 41, 78, 109, 110, 214, 356, 369, 374], "bind_example_oper": 6, "r": [6, 245, 371, 387], "pybind": 6, "expos": 6, "logic": [6, 195, 196, 199, 200, 202, 211, 221, 222], "self": [6, 364, 368, 369, 374, 388], "correct": 6, "specif": [6, 28, 104, 211, 347, 365, 369, 374, 375, 387], "pybind_overload_t": 6, "decltyp": 6, "examples_pybind": 6, "py_modul": 6, "final": [6, 211, 242, 364, 365, 366, 372, 383], "wherev": 6, "want": [6, 366, 371, 384, 386, 390], "compar": [6, 86, 126, 135, 170, 206, 211, 229, 384], "its": [6, 58, 85, 242, 305, 364, 365, 368, 369, 372, 373, 374, 390], "equival": [6, 28, 104, 242, 342, 348, 374], "import": [6, 104, 214, 364, 365, 366, 371, 373, 383, 384, 385, 386, 387, 388, 389, 390], "signatur": 6, "And": [6, 364, 368, 369, 374, 383, 384], "ignor": [6, 254], "kwarg": [6, 265, 266, 327, 338, 368, 390], "def": [6, 364, 385, 386, 387, 388, 389, 390], "golden_funct": 6, "befor": [6, 29, 242, 266, 307, 365, 369, 374], "some": [6, 327, 369, 390], "postprocess": 6, "manual": [6, 364, 390], "pack": [6, 354, 355], "preprocess_golden_function_input": 6, "ttnn_input_tensor": 6, "postprocess_golden_function_output": 6, "torch_output_tensor": [6, 390], "dtype": [6, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 105, 106, 107, 108, 110, 111, 112, 113, 114, 115, 116, 117, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 165, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 241, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 267, 268, 269, 270, 271, 272, 273, 274, 275, 277, 278, 279, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 349, 350, 351, 352, 353, 357, 358, 359, 360, 361, 362, 363, 364, 368, 369, 374, 383, 384, 385, 386, 388, 390], "becaus": [6, 100, 374, 383, 384, 385], "wa": [6, 100, 365, 373, 374, 383, 389], "_ttnn": [8, 9, 82, 117, 118, 240, 327, 338, 368, 369, 374], "default": [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 128, 129, 130, 131, 132, 133, 134, 135, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 154, 155, 156, 157, 158, 159, 160, 161, 162, 165, 166, 167, 168, 169, 170, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 201, 203, 204, 205, 206, 208, 209, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 224, 225, 226, 227, 228, 229, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 244, 245, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 261, 262, 263, 264, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 366, 368, 369, 373, 384, 390], "plan": [8, 9, 117, 118, 243], "deprec": [8, 9, 117, 118, 243, 386], "futur": [8, 9, 117, 118, 243], "arg0": [9, 368, 374], "none": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 128, 129, 130, 131, 132, 133, 134, 135, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 154, 155, 156, 157, 158, 159, 160, 161, 162, 165, 166, 167, 168, 169, 170, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 201, 203, 204, 205, 206, 208, 209, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 231, 232, 233, 234, 235, 236, 237, 238, 239, 241, 242, 244, 245, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 368, 369, 383, 384, 385, 386, 387, 388], "device_id": [9, 59, 69, 82, 83, 210, 240, 261, 327, 338, 339, 340, 368, 383, 384, 385, 386, 388, 390], "complextensor": [10, 11, 17, 25, 26, 62, 63, 74, 75, 152, 153, 155, 156, 225, 245, 246, 259, 260, 262, 263, 320], "memory_config": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 62, 63, 64, 65, 66, 67, 70, 71, 72, 73, 74, 75, 76, 77, 80, 81, 82, 83, 84, 85, 86, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 119, 120, 122, 123, 124, 125, 126, 128, 129, 130, 131, 132, 133, 134, 135, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 165, 166, 167, 168, 169, 170, 172, 173, 174, 175, 176, 177, 178, 179, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 201, 203, 204, 205, 206, 208, 209, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 224, 225, 226, 227, 228, 229, 231, 232, 233, 234, 235, 236, 237, 238, 239, 241, 242, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 368, 369, 384, 385, 388, 390], "appli": [10, 12, 16, 30, 35, 41, 45, 50, 58, 64, 80, 86, 88, 89, 91, 93, 95, 96, 101, 102, 103, 105, 107, 112, 126, 128, 129, 131, 132, 133, 135, 137, 146, 149, 158, 159, 160, 161, 162, 168, 170, 172, 176, 179, 181, 182, 186, 189, 191, 192, 195, 197, 199, 201, 206, 208, 211, 214, 220, 228, 229, 231, 234, 242, 262, 267, 268, 269, 272, 273, 281, 285, 287, 293, 294, 296, 298, 299, 301, 307, 313, 315, 317, 323, 325, 328, 330, 351, 353, 365, 369], "element": [10, 12, 28, 30, 35, 45, 50, 60, 64, 80, 88, 89, 91, 93, 95, 96, 101, 104, 105, 107, 112, 128, 129, 131, 132, 133, 137, 146, 149, 158, 159, 160, 161, 162, 172, 176, 181, 182, 186, 189, 197, 208, 220, 231, 234, 235, 242, 249, 250, 257, 262, 267, 268, 269, 272, 273, 278, 281, 285, 293, 294, 296, 298, 299, 301, 307, 313, 315, 325, 328, 330, 342, 351, 353, 355, 369, 374, 383], "wise": [10, 12, 30, 35, 45, 50, 64, 80, 88, 89, 91, 93, 95, 96, 101, 105, 107, 112, 128, 129, 131, 133, 137, 146, 149, 158, 159, 160, 161, 162, 172, 176, 181, 182, 186, 189, 197, 208, 220, 231, 234, 250, 257, 262, 267, 268, 269, 272, 273, 281, 285, 293, 294, 296, 298, 299, 301, 307, 313, 315, 325, 328, 330, 351, 353, 369], "mathrm": [10, 12, 16, 18, 30, 35, 36, 43, 44, 45, 46, 47, 48, 50, 64, 74, 76, 80, 86, 87, 88, 89, 91, 93, 95, 96, 101, 105, 107, 112, 114, 125, 126, 127, 128, 129, 131, 132, 133, 135, 136, 137, 146, 147, 149, 151, 157, 158, 159, 160, 161, 162, 167, 168, 170, 171, 172, 174, 176, 181, 182, 186, 189, 191, 192, 195, 196, 197, 199, 200, 201, 202, 206, 207, 208, 215, 219, 220, 228, 229, 230, 231, 233, 234, 241, 249, 250, 252, 253, 262, 267, 268, 269, 272, 273, 281, 285, 287, 289, 293, 294, 296, 298, 299, 301, 307, 313, 315, 317, 321, 323, 325, 328, 330, 351, 353, 360, 369], "_tensor": [10, 12, 16, 18, 30, 35, 36, 43, 44, 45, 46, 47, 48, 50, 64, 74, 76, 80, 86, 87, 88, 89, 91, 93, 95, 96, 101, 105, 107, 112, 114, 125, 126, 127, 128, 129, 131, 132, 133, 135, 136, 137, 146, 147, 149, 151, 157, 158, 159, 160, 161, 162, 167, 168, 170, 171, 172, 174, 176, 181, 182, 186, 189, 191, 192, 195, 196, 197, 199, 200, 201, 202, 206, 207, 208, 215, 219, 220, 228, 229, 230, 231, 233, 234, 241, 249, 250, 252, 253, 262, 267, 268, 269, 272, 273, 281, 285, 287, 289, 293, 294, 296, 298, 299, 301, 307, 313, 315, 317, 321, 323, 325, 328, 330, 351, 353, 360], "_i": [10, 12, 16, 30, 35, 36, 43, 44, 45, 46, 47, 48, 50, 64, 74, 80, 86, 88, 89, 91, 93, 95, 96, 101, 105, 107, 112, 125, 126, 128, 129, 131, 132, 133, 135, 137, 146, 147, 149, 151, 158, 159, 160, 161, 162, 167, 170, 172, 176, 181, 182, 186, 189, 195, 196, 197, 199, 200, 201, 202, 206, 208, 220, 228, 229, 231, 233, 234, 249, 250, 253, 262, 267, 268, 269, 272, 273, 281, 285, 287, 289, 293, 294, 296, 298, 299, 301, 307, 313, 315, 323, 325, 328, 330, 351, 353, 360], "verb": [10, 12, 30, 35, 43, 44, 45, 46, 47, 48, 50, 64, 76, 80, 93, 96, 101, 105, 107, 112, 114, 125, 128, 133, 146, 149, 151, 158, 159, 160, 161, 162, 167, 168, 172, 174, 181, 182, 186, 189, 191, 192, 215, 219, 220, 231, 252, 262, 267, 268, 269, 272, 273, 281, 289, 294, 296, 298, 299, 301, 313, 315, 317, 325, 328, 353], "keyword": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 60, 61, 62, 63, 64, 65, 66, 67, 70, 71, 72, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83, 85, 86, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 119, 120, 121, 122, 125, 126, 128, 129, 130, 131, 132, 133, 134, 135, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 165, 166, 167, 168, 169, 170, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 201, 203, 204, 205, 206, 208, 209, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 224, 225, 226, 227, 228, 229, 231, 232, 233, 234, 235, 236, 237, 240, 241, 242, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361], "prealloc": [10, 12, 16, 17, 19, 28, 30, 34, 35, 41, 43, 44, 45, 46, 47, 48, 50, 60, 61, 64, 74, 75, 80, 82, 83, 86, 88, 89, 91, 93, 95, 96, 98, 100, 101, 102, 104, 105, 107, 108, 112, 123, 124, 126, 129, 130, 131, 135, 137, 146, 149, 151, 158, 159, 160, 161, 162, 165, 168, 170, 172, 176, 181, 182, 186, 189, 191, 192, 195, 197, 199, 201, 206, 208, 220, 224, 225, 228, 229, 231, 232, 234, 239, 250, 251, 257, 262, 268, 269, 272, 273, 281, 285, 286, 287, 288, 293, 294, 296, 298, 299, 300, 301, 307, 313, 314, 315, 317, 320, 322, 323, 328, 330, 331, 342, 351, 353, 358, 359, 363], "queue": [10, 12, 16, 17, 19, 28, 30, 34, 35, 41, 43, 44, 45, 46, 47, 48, 50, 60, 61, 64, 74, 75, 80, 82, 83, 86, 88, 89, 91, 93, 95, 96, 98, 100, 101, 102, 105, 107, 108, 109, 110, 112, 121, 122, 123, 124, 126, 129, 130, 131, 135, 137, 146, 149, 151, 154, 158, 159, 160, 161, 162, 165, 168, 170, 172, 176, 181, 182, 186, 189, 191, 192, 195, 197, 199, 201, 206, 208, 214, 220, 224, 225, 228, 229, 231, 232, 234, 235, 239, 242, 244, 250, 251, 257, 262, 268, 269, 272, 273, 279, 281, 285, 286, 287, 288, 293, 294, 296, 298, 299, 300, 301, 307, 313, 314, 315, 317, 320, 322, 323, 327, 328, 330, 331, 336, 337, 338, 341, 342, 346, 347, 351, 353, 354, 355, 358, 359, 363], "id": [10, 12, 16, 17, 19, 28, 30, 34, 35, 41, 43, 44, 45, 46, 47, 48, 50, 60, 61, 64, 74, 75, 80, 82, 83, 86, 88, 89, 91, 93, 95, 96, 98, 100, 101, 102, 105, 107, 108, 109, 110, 112, 121, 122, 123, 124, 126, 129, 130, 131, 135, 137, 146, 149, 151, 154, 158, 159, 160, 161, 162, 165, 168, 170, 172, 176, 181, 182, 186, 189, 191, 192, 195, 197, 199, 201, 206, 208, 210, 214, 220, 224, 225, 228, 229, 231, 232, 234, 235, 239, 240, 242, 244, 250, 251, 257, 262, 268, 269, 272, 273, 279, 281, 285, 286, 287, 288, 293, 294, 296, 298, 299, 300, 301, 307, 313, 314, 315, 317, 320, 322, 323, 327, 328, 330, 331, 336, 337, 338, 341, 342, 346, 347, 351, 353, 354, 355, 358, 359, 363, 368, 373], "bfloat16": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 60, 61, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 74, 75, 76, 77, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 105, 106, 107, 108, 111, 112, 113, 114, 115, 116, 117, 119, 120, 121, 122, 123, 124, 126, 127, 128, 129, 130, 131, 132, 133, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 165, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 241, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 267, 268, 269, 270, 271, 272, 273, 274, 275, 277, 278, 279, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 349, 350, 351, 352, 353, 354, 355, 357, 358, 359, 360, 361, 362, 363, 364, 366, 368, 369, 373, 374, 383, 384, 385, 387, 388, 390], "tile": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 26, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 61, 64, 65, 66, 67, 70, 71, 72, 73, 74, 75, 76, 77, 80, 81, 84, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 101, 102, 105, 106, 107, 108, 110, 111, 112, 113, 114, 115, 116, 119, 120, 122, 125, 126, 127, 128, 129, 130, 131, 133, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 157, 158, 159, 160, 161, 162, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 211, 213, 215, 218, 219, 220, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 236, 237, 241, 243, 244, 246, 247, 248, 249, 250, 251, 252, 254, 255, 256, 257, 258, 262, 263, 267, 268, 269, 270, 271, 272, 273, 274, 275, 277, 279, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 320, 321, 322, 323, 325, 326, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 341, 346, 347, 349, 350, 351, 352, 353, 354, 355, 358, 359, 360, 361, 362, 363, 368, 369, 373, 374, 383, 384, 387], "tile_layout": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 61, 64, 65, 66, 67, 69, 70, 71, 72, 73, 74, 75, 76, 80, 81, 83, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 101, 102, 105, 106, 107, 108, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 122, 123, 125, 126, 127, 128, 129, 130, 131, 133, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 157, 158, 159, 160, 161, 162, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 211, 213, 215, 218, 219, 220, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 236, 237, 238, 239, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 267, 268, 269, 270, 271, 272, 273, 274, 275, 277, 278, 281, 282, 283, 285, 286, 287, 288, 289, 290, 291, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 320, 321, 322, 323, 325, 326, 328, 329, 330, 331, 332, 333, 334, 335, 339, 342, 349, 350, 351, 352, 353, 358, 359, 360, 361, 362, 363, 364, 368, 374, 383, 384, 385, 388, 390], "grad_tensor": [11, 13, 15, 17, 19, 21, 23, 26, 31, 33, 34, 37, 38, 40, 42, 51, 53, 55, 57, 61, 63, 65, 67, 71, 73, 75, 77, 81, 83, 90, 92, 94, 97, 98, 102, 106, 108, 111, 113, 116, 120, 130, 139, 141, 143, 145, 148, 150, 153, 169, 173, 175, 178, 183, 185, 187, 188, 190, 193, 194, 204, 205, 213, 218, 225, 227, 232, 246, 248, 251, 254, 256, 258, 260, 263, 270, 271, 275, 277, 282, 284, 286, 288, 291, 295, 297, 300, 302, 304, 308, 310, 312, 314, 316, 318, 320, 322, 329, 331, 333, 335, 352, 359, 361], "list": [11, 13, 15, 16, 17, 19, 21, 23, 26, 28, 31, 33, 34, 37, 38, 40, 42, 51, 53, 55, 57, 60, 61, 63, 65, 67, 68, 71, 73, 75, 77, 78, 81, 82, 84, 86, 90, 92, 94, 97, 98, 106, 108, 111, 113, 116, 118, 120, 126, 130, 135, 139, 141, 143, 145, 148, 150, 153, 168, 169, 170, 173, 175, 178, 179, 183, 185, 187, 188, 190, 191, 192, 193, 194, 195, 199, 201, 204, 205, 206, 211, 213, 214, 218, 225, 227, 228, 229, 232, 235, 242, 243, 244, 246, 248, 251, 252, 253, 254, 256, 258, 260, 263, 270, 271, 275, 277, 282, 284, 286, 287, 288, 291, 295, 297, 300, 302, 304, 305, 308, 310, 312, 314, 316, 317, 318, 320, 322, 323, 327, 329, 331, 333, 335, 342, 347, 352, 353, 354, 355, 359, 361, 368, 369], "backward": [11, 13, 15, 17, 19, 21, 23, 26, 31, 33, 34, 37, 38, 40, 42, 51, 53, 55, 57, 61, 63, 65, 67, 71, 73, 75, 77, 81, 83, 90, 92, 94, 97, 98, 102, 106, 108, 111, 113, 116, 120, 130, 139, 141, 143, 145, 148, 150, 153, 169, 173, 175, 178, 183, 185, 187, 188, 190, 193, 194, 204, 205, 213, 218, 225, 227, 232, 246, 248, 251, 254, 256, 258, 260, 263, 270, 271, 275, 277, 282, 284, 286, 288, 291, 295, 297, 300, 302, 304, 308, 310, 312, 314, 316, 318, 320, 322, 329, 331, 333, 335, 352, 359, 361, 369], "given": [11, 13, 15, 17, 19, 21, 23, 26, 28, 31, 33, 34, 37, 38, 40, 42, 51, 53, 55, 57, 61, 63, 65, 67, 71, 73, 75, 77, 78, 81, 85, 90, 92, 94, 97, 98, 100, 106, 108, 111, 113, 116, 120, 130, 139, 141, 143, 145, 148, 150, 153, 169, 173, 175, 178, 183, 185, 187, 188, 190, 193, 194, 204, 205, 213, 218, 221, 222, 225, 227, 232, 240, 243, 246, 248, 251, 254, 256, 258, 260, 263, 270, 271, 275, 277, 278, 282, 284, 286, 288, 291, 295, 297, 300, 302, 304, 308, 310, 312, 314, 316, 318, 320, 322, 329, 331, 333, 335, 342, 347, 352, 356, 359, 361, 364, 368, 369, 373, 383], "gradient": [11, 13, 15, 17, 19, 21, 23, 31, 33, 34, 37, 38, 40, 42, 51, 53, 55, 57, 61, 65, 67, 71, 73, 75, 77, 81, 83, 90, 92, 94, 97, 98, 102, 106, 108, 111, 113, 116, 120, 130, 139, 141, 143, 145, 148, 150, 169, 173, 175, 178, 183, 185, 187, 188, 190, 193, 194, 204, 205, 213, 218, 225, 227, 232, 248, 251, 254, 256, 258, 263, 270, 271, 275, 277, 282, 284, 286, 288, 291, 295, 297, 300, 302, 304, 308, 310, 312, 314, 316, 318, 320, 322, 329, 331, 333, 335, 352, 359, 361], "about": [11, 21, 42, 116, 130, 183, 185, 187, 188, 190, 193, 194, 204, 205, 251, 254, 262, 263, 275, 308, 314, 361, 374, 384], "requires_grad": [11, 13, 15, 17, 19, 21, 23, 31, 33, 34, 37, 38, 40, 42, 51, 53, 55, 57, 61, 65, 67, 71, 73, 75, 81, 83, 90, 92, 94, 97, 98, 102, 106, 108, 111, 113, 116, 120, 130, 139, 141, 143, 145, 148, 150, 169, 173, 175, 178, 183, 185, 187, 188, 190, 193, 194, 204, 205, 213, 218, 225, 227, 232, 251, 254, 256, 258, 263, 270, 271, 275, 277, 282, 286, 288, 291, 295, 297, 300, 302, 304, 308, 310, 312, 314, 316, 318, 320, 322, 329, 331, 333, 335, 352, 359, 361], "rand": [12, 14, 30, 32, 35, 39, 41, 49, 50, 61, 64, 66, 70, 82, 88, 93, 96, 105, 112, 119, 128, 131, 133, 137, 149, 158, 159, 160, 161, 162, 176, 177, 181, 182, 184, 186, 189, 197, 198, 208, 220, 231, 234, 236, 237, 241, 252, 254, 255, 267, 268, 269, 277, 278, 289, 296, 298, 299, 301, 303, 311, 313, 315, 325, 326, 328, 332, 349, 350, 383, 386, 388, 390], "input_tensor_a": [13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 31, 33, 34, 36, 37, 38, 40, 42, 43, 44, 46, 47, 48, 51, 53, 61, 65, 67, 71, 73, 74, 75, 76, 81, 86, 87, 90, 92, 94, 97, 106, 111, 113, 114, 115, 116, 120, 125, 126, 127, 135, 136, 139, 141, 143, 147, 148, 150, 154, 157, 167, 168, 169, 170, 171, 173, 175, 178, 179, 183, 185, 187, 188, 190, 191, 192, 193, 194, 195, 196, 199, 200, 201, 202, 204, 205, 206, 207, 209, 211, 213, 214, 215, 218, 219, 225, 227, 228, 229, 230, 233, 241, 249, 251, 252, 256, 258, 263, 270, 271, 274, 275, 282, 287, 288, 289, 291, 295, 297, 302, 304, 310, 312, 316, 317, 318, 320, 321, 322, 323, 329, 333, 352, 358, 359, 360, 361, 383, 390], "invers": [13, 15, 31, 33, 38, 40], "cosin": [13, 15, 65, 67, 103], "hyperbol": [15, 33, 40, 67, 304, 331], "input_tensor_b": [16, 17, 18, 19, 20, 21, 22, 23, 34, 36, 37, 42, 43, 44, 46, 47, 48, 61, 74, 75, 76, 86, 87, 114, 115, 116, 125, 126, 127, 135, 136, 147, 148, 154, 157, 167, 168, 169, 170, 171, 175, 179, 191, 192, 193, 194, 195, 196, 199, 200, 201, 202, 206, 207, 209, 211, 213, 215, 218, 219, 225, 228, 229, 230, 233, 241, 252, 274, 275, 287, 288, 289, 317, 318, 320, 321, 322, 323, 358, 359, 360, 361, 383, 390], "number": [16, 17, 20, 22, 24, 42, 60, 74, 75, 76, 86, 87, 99, 109, 110, 114, 115, 116, 126, 127, 135, 136, 168, 170, 171, 175, 191, 192, 195, 199, 201, 206, 207, 209, 211, 212, 214, 215, 216, 217, 219, 225, 228, 229, 230, 235, 242, 244, 251, 252, 264, 274, 275, 276, 278, 279, 287, 317, 319, 320, 323, 324, 337, 342, 343, 344, 357, 358, 368, 369, 372, 373, 374, 385, 389], "datatyp": [16, 27, 29, 78, 82, 83, 84, 85, 86, 122, 123, 124, 126, 132, 134, 135, 168, 170, 179, 191, 192, 195, 199, 201, 206, 211, 228, 229, 238, 239, 287, 317, 323, 339, 340, 362, 363, 366, 368, 369, 383, 384, 388], "str": [16, 29, 34, 75, 79, 86, 102, 126, 135, 168, 170, 179, 180, 191, 192, 195, 199, 201, 206, 211, 221, 222, 228, 229, 287, 292, 317, 323, 368, 369, 389], "_a": [16, 18, 36, 43, 44, 46, 47, 48, 74, 76, 86, 87, 114, 125, 126, 127, 135, 136, 147, 157, 167, 168, 170, 171, 191, 192, 195, 196, 199, 200, 201, 202, 206, 207, 215, 219, 228, 229, 230, 233, 241, 252, 287, 289, 317, 321, 323, 360], "_b": [16, 18, 36, 43, 44, 46, 47, 48, 74, 76, 86, 87, 114, 125, 126, 127, 135, 136, 147, 157, 167, 168, 170, 171, 191, 192, 195, 196, 199, 200, 201, 202, 206, 207, 215, 219, 228, 229, 230, 233, 241, 252, 287, 289, 317, 321, 323, 360], "broadcast": [16, 17, 75, 86, 126, 135, 168, 170, 179, 191, 192, 195, 199, 201, 206, 211, 228, 229, 278, 317, 320, 323, 346, 390], "int32": [16, 43, 44, 45, 46, 47, 48, 125, 167, 386], "tensor1": [16, 17, 18, 19, 20, 21, 22, 23, 34, 36, 37, 42, 43, 44, 46, 47, 48, 60, 61, 74, 75, 76, 86, 87, 114, 115, 116, 125, 126, 127, 135, 136, 147, 148, 154, 157, 167, 168, 169, 170, 171, 174, 175, 191, 192, 193, 194, 195, 196, 199, 200, 201, 202, 206, 207, 209, 211, 213, 215, 218, 219, 225, 228, 229, 230, 233, 241, 252, 274, 275, 287, 288, 289, 317, 318, 320, 321, 322, 323, 358, 359, 360, 361], "tensor2": [16, 17, 18, 19, 20, 21, 22, 23, 34, 36, 37, 42, 43, 44, 46, 47, 48, 60, 61, 74, 75, 76, 86, 87, 114, 115, 116, 125, 126, 127, 135, 136, 147, 148, 154, 157, 167, 168, 169, 170, 171, 174, 175, 191, 192, 193, 194, 195, 196, 199, 200, 201, 202, 206, 207, 209, 211, 213, 215, 218, 219, 225, 228, 229, 230, 233, 241, 252, 274, 275, 287, 288, 289, 317, 318, 320, 321, 322, 323, 358, 359, 360, 361], "scalar": [16, 17, 20, 22, 43, 44, 46, 47, 48, 74, 75, 76, 77, 86, 87, 114, 115, 116, 123, 124, 126, 127, 135, 136, 168, 170, 171, 174, 175, 191, 192, 195, 199, 201, 206, 207, 209, 215, 219, 225, 228, 229, 230, 248, 252, 257, 258, 274, 275, 287, 317, 320, 323, 366, 369, 390], "are_required_output": [17, 19, 34, 61, 75, 225, 288, 320, 322, 359], "input_grad": [17, 19, 34, 61, 75, 102, 225, 288, 320, 322], "other_grad": [17, 19, 34, 61, 75, 225, 288, 320, 322], "bfloat4_b": [17, 19, 20, 22, 37, 42, 61, 75, 122, 148, 169, 174, 175, 193, 194, 209, 213, 218, 225, 288, 318, 320, 322, 341, 358, 361, 368], "alpha": [18, 19, 21, 23, 52, 53, 75, 80, 81, 290, 321, 322], "float": [18, 19, 20, 21, 22, 23, 41, 52, 53, 54, 55, 56, 57, 77, 80, 81, 101, 107, 110, 117, 122, 123, 124, 134, 138, 139, 140, 142, 144, 145, 146, 157, 166, 172, 173, 174, 203, 205, 244, 248, 249, 250, 252, 257, 258, 272, 273, 280, 283, 284, 290, 307, 308, 309, 310, 321, 322, 334, 335, 346, 347, 349, 350, 366, 368, 369, 374], "input_tensor_c": [20, 21, 22, 23, 175, 209, 358, 359], "tensor3": [20, 21, 22, 23, 174, 175, 209, 358, 359], "cluster_axi": [24, 264], "mesh_devic": [24, 99, 264, 368], "meshdevic": [24, 84, 85, 123, 124, 238, 239, 264, 327, 338, 339, 362, 363, 368], "num_link": [24, 99, 264], "num_work": [24, 99, 264], "num_buffers_per_channel": [24, 99, 264], "ring": [24, 99, 264], "gather": [24, 99], "across": [24, 99, 132, 264, 368, 373, 374], "dimens": [24, 28, 58, 60, 61, 104, 123, 128, 132, 133, 154, 211, 212, 216, 217, 238, 244, 252, 253, 254, 264, 267, 276, 278, 279, 305, 306, 319, 324, 325, 339, 341, 342, 345, 346, 347, 348, 357, 362, 366, 368, 369, 374], "meshtensor": [24, 264], "axi": [24, 100, 264, 278, 369], "correspond": [24, 82, 83, 264, 347, 368, 369], "line": [24, 264, 387], "applic": [24, 79, 264, 374], "guid": [24, 264, 370, 371], "blob": [24, 264, 389], "main": [24, 264, 371, 372, 375, 387, 389, 390], "tech_report": [24, 264], "20mesh": [24, 264], "20of": [24, 264], "20devic": [24, 264], "20with": [24, 264], "20tt": [24, 264], "nn": [24, 221, 222, 264, 368, 375, 376, 379, 380, 388], "link": [24, 99, 264], "worker": [24, 99, 240, 264, 339, 368, 385, 386], "per": [24, 99, 264, 369, 387], "full_tensor": [24, 99, 264], "randn": [24, 29, 69, 83, 85, 99, 121, 122, 132, 179, 211, 214, 239, 261, 264, 338, 339, 340, 341, 353, 363, 366, 368, 384, 385, 389], "32": [24, 58, 60, 61, 69, 110, 121, 123, 128, 132, 133, 179, 211, 235, 236, 237, 238, 241, 244, 252, 254, 261, 267, 277, 278, 289, 305, 306, 325, 338, 339, 340, 353, 362, 366, 368, 369, 374, 383, 384, 386, 387, 388, 390], "256": [24, 99, 214, 264, 388, 389], "physical_device_id": [24, 99, 264, 383, 384, 385, 386, 387, 388], "get_t3k_physical_device_ids_r": [24, 99, 264], "open_mesh_devic": [24, 99, 264], "meshshap": [24, 99, 264], "ttnn_tensor": [24, 341, 383], "input_dtyp": [24, 99, 264], "mem_config": [24, 99, 264, 368], "mesh_mapp": [24, 29, 122], "shardtensor2dmesh": 24, "mesh_shap": 24, "end": [27, 74, 157, 174, 233, 305, 355, 365, 368, 369, 373, 385, 387], "dram_memory_config": [27, 58, 84, 85, 179, 211, 374], "inclus": [27, 368, 374], "exclus": [27, 242], "consecut": [27, 374], "evenli": 27, "space": [27, 369, 374], "within": [27, 44, 47, 214, 365, 368, 369, 374, 375], "float32": [27, 41, 84, 85, 107, 125, 151, 167, 250, 362, 363, 364, 368, 373, 374, 383, 388, 390], "print": [27, 29, 60, 84, 85, 122, 123, 124, 179, 210, 211, 238, 239, 240, 244, 276, 278, 292, 305, 306, 339, 341, 362, 363, 366, 368, 369, 373, 374, 383, 384, 385, 387, 389, 390], "00000": [27, 383], "row_major": [27, 28, 68, 84, 85, 122, 123, 124, 236, 237, 238, 336, 337, 354, 355, 362, 363, 366, 368, 369, 373, 383, 384, 388], "indic": [28, 82, 83, 104, 235, 305, 355, 368, 369], "currenli": 28, "must": [28, 41, 83, 227, 251, 257, 279, 305, 336, 337, 342, 354, 355, 366, 368, 369, 371, 374], "uint32": [28, 82, 83, 151, 154, 368, 373, 374, 383], "reduc": [28, 264, 342, 369], "row_major_layout": [29, 58, 79, 82, 117, 118, 122, 123, 238, 339, 362, 374, 383, 384, 385], "cache_file_nam": 29, "pathlib": [29, 79, 180], "path": [29, 79, 180, 373, 387, 389, 390], "callabl": [29, 221, 222], "tensortomesh": [29, 122], "use_device_til": 29, "serial": 29, "flag": [29, 369, 387], "toggl": 29, "whether": [29, 121, 179, 211, 214, 221, 222, 336, 337, 342, 347, 348, 354, 355, 369], "truncat": [29, 352, 369], "mantissa": 29, "bit": [29, 369], "bfp": [29, 374], "format": [29, 36, 117, 118, 122, 214, 262, 372, 374], "rais": [29, 341, 388], "runtim": 29, "error": [29, 165, 210, 211, 224, 341, 365, 371], "rte": 29, "bfp8": [29, 383], "bfp4": 29, "375": [29, 122, 386], "30469": [29, 122], "714844": [29, 122], "761719": [29, 122], "53125": [29, 122], "652344": [29, 122], "sine": [31, 33, 103, 304], "round_mod": [34, 74, 75, 257, 258], "assign": [34, 366], "other_tensor": [34, 75], "arctan": 36, "left": [36, 74, 125, 167, 289, 368, 369, 372], "right": [36, 74, 125, 167, 289, 368, 369], "tangenr": 38, "tangent": [40, 331], "ep": [41, 203, 205, 369], "1e": [41, 134, 157, 166, 280, 369], "momentum": [41, 369], "running_mean": [41, 369], "running_var": [41, 369], "bia": [41, 42, 134, 166, 179, 280, 364, 369, 385, 388], "spatial": [41, 78, 132, 356, 369], "over": [41, 132, 134, 166, 280, 306, 346, 347, 369], "interleav": [41, 60, 340, 348, 368, 369, 374], "n": [41, 60, 78, 109, 110, 214, 235, 248, 249, 356, 369, 371, 373, 383, 384, 387, 389], "w": [41, 78, 109, 110, 214, 356, 368, 369, 373], "epsilon": [41, 134, 166, 280, 369], "place": [41, 87, 127, 136, 163, 164, 171, 207, 214, 230, 281, 344, 368, 374], "varianc": [41, 369, 374], "gamma": [41, 227, 369], "beta": [41, 307, 308, 369], "evalu": [41, 387], "approxim": [42, 89, 91, 95, 102, 129, 130, 285, 293], "bias_gelu": 42, "integ": [43, 44, 46, 47, 48, 242, 347, 369], "ha": [44, 47, 211, 221, 222, 327, 348, 364, 365, 366, 368, 369, 371, 373, 374, 383, 389, 390], "shift_bit": [44, 47], "31": [44, 47, 366, 368, 371, 387, 389], "2147483647": 45, "wormhole_b0": [45, 50, 107, 112, 203, 220, 257, 371], "major": [51, 108, 110, 111, 113, 120, 235, 282, 297, 352, 368, 374, 383, 384], "formula": [53, 81, 115, 139, 173, 174, 205, 274, 308, 310], "min_tensor": [54, 56], "max_tensor": [54, 56], "doe": [58, 122, 341, 365, 369, 373], "alter": 58, "width": [58, 68, 109, 110, 123, 132, 211, 214, 238, 339, 345, 362, 369, 373], "height": [58, 60, 68, 109, 110, 123, 132, 211, 214, 238, 339, 362, 369, 373, 374], "current": [58, 60, 68, 221, 222, 347, 369, 371, 374, 375, 383, 390], "adjust": [58, 307], "necessari": [58, 365], "param": [58, 78, 254, 306, 369, 389], "target": [58, 117, 118, 338, 365, 368, 369], "l1_memory_config": [58, 364, 374, 384, 385, 390], "compute_kernel_config": [58, 103, 166, 179, 211, 280, 306, 346, 347], "remov": [59, 341, 355, 368, 383, 386, 387, 389], "success": 59, "group": [60, 369, 374, 388], "concaten": [60, 61, 345, 348, 369], "greater": [60, 72, 109, 110, 126, 127, 135, 136, 177, 227, 369, 390], "partit": 60, "independ": 60, "altern": [60, 211, 371, 390], "recombin": 60, "residu": 60, "64": [60, 68, 69, 121, 128, 133, 179, 211, 244, 261, 267, 305, 306, 325, 338, 339, 340, 368, 374, 385, 386, 387, 388, 390], "conjug": 62, "core_grid": [68, 134, 179, 211, 364, 384, 385], "coregrid": [68, 134, 179, 211, 384, 385], "corerang": 68, "strategi": [68, 79, 211, 368, 369, 373, 374, 387], "shardstrategi": 68, "orient": [68, 374], "shardorient": 68, "use_height_and_width_as_shard_shap": 68, "object": [68, 240, 364, 368, 372, 389], "l1": [68, 179, 211, 240, 340, 368, 374, 385], "either": [68, 211, 339, 365, 369, 371, 374, 375], "travers": 68, "seen": [68, 383, 384], "math": [68, 373, 387], "320": [68, 369], "forc": [69, 386], "resourc": [69, 387], "explicitli": [69, 383], "even": [69, 210, 211, 366, 369], "degre": [71, 256], "radian": [71, 256], "accurate_mod": 74, "begin": [74, 157, 233, 369, 373], "text": [74, 157, 233, 241, 365, 369], "_mode": 74, "non": [74, 121, 211, 214, 235, 251, 262, 283], "divid": [75, 343, 344], "pcc": [75, 148, 169, 258, 365, 372], "degrad": [75, 148, 169, 258], "denomin": [77, 257, 369], "downsample_param": 78, "2d": [78, 132, 211, 356, 369, 374], "assum": [78, 103, 118, 327, 356, 375], "form": [78, 356], "conv": [78, 388], "stride": [78, 214, 369, 383, 388], "file_nam": [79, 180, 371, 389, 390], "dict": [79, 221, 222, 368, 389], "dump": [79, 221, 386, 389, 390], "save": [79, 387, 389], "padding_idx": 82, "embeddings_typ": 82, "embeddingstyp": 82, "gener": [82, 101, 110, 211, 245, 365, 369, 373, 383, 384, 385, 386, 387, 388], "retriev": 82, "word": 82, "token": [82, 164, 347, 387], "106445": 82, "988281": 82, "59375": 82, "212891": 82, "964844": 82, "199219": 82, "996094": 82, "78362e": 82, "38": [82, 374, 387], "89785e": 82, "39": [82, 383, 384, 385, 386, 387, 388, 389], "04479e": 82, "25815e": 82, "71833e": 82, "59995e": 82, "60398e": 82, "83671e": 82, "22242e": 82, "88263e": 82, "35917e": 82, "49994e": 82, "output_gradient_tensor": 83, "respect": [83, 211, 374], "extract": 83, "vocabulari": 83, "previou": [83, 366], "batch_siz": [83, 132, 214, 345, 348, 364, 385, 386, 387, 388], "seq_len": [83, 103], "embedding_dim": 83, "num_embed": 83, "1024": [83, 125, 167, 384], "4096": [83, 368], "3200": 83, "input_shap": [83, 214, 388], "input_index": 83, "randint": [83, 366, 386], "weights_shap": 83, "weights_ttnn": 83, "grad_shap": 83, "grad_data": 83, "uniniti": [84, 85], "bfloat_8": 84, "21": [84, 387, 389], "67": 84, "whose": [85, 339], "desir": [85, 109, 110, 117, 118, 122, 339, 340, 341], "87": 85, "45": [85, 328, 329, 387], "22": [85, 371, 387], "60": [85, 387], "75": 85, "25": [85, 339, 373, 387, 389], "equal": [86, 87, 104, 126, 127, 157, 170, 171, 229, 230, 369, 390], "input_a": [87, 127, 136, 154, 171, 207, 212, 216, 217, 230, 319, 324, 357], "input_b": [87, 127, 136, 154, 171, 207, 230], "_tensor_i": [88, 131, 137, 176, 197, 208, 234], "fast_and_approximate_mod": [89, 91, 95, 129, 285, 293], "exponenti": 98, "num_devic": [99, 264], "tt_input_tensor": [99, 264], "enumer": [99, 264], "append": [99, 264, 373, 389], "get_devic": [99, 264], "input_tensor_mesh": [99, 264], "aggregate_as_tensor": [99, 264], "witth": 100, "cumul": 100, "product": [100, 211, 346, 347], "along": [100, 104, 242, 253, 254, 305, 306, 342, 345, 368, 369, 374], "scaffold": 100, "ref": [100, 371], "fed": 100, "being": [100, 265, 266, 373, 374], "actual": [100, 257, 366, 374, 383], "assert": [100, 385, 390], "dtyoe": 100, "anoth": 100, "uint8": [100, 130, 151, 286, 305], "tensor_copi": 100, "seed": [101, 389], "probabl": 101, "scale": [101, 140, 142, 290, 307, 346, 347, 369], "rng": 101, "averag": [101, 132, 369, 373, 387], "total_elem": 101, "124": 101, "prob": 101, "algorithm": [102, 356, 369], "cod_cach": 103, "sin_cach": 103, "token_index": 103, "devicecomputekernelconfig": [103, 166, 179, 211, 280, 346, 347], "rotari": 103, "cos_cach": 103, "token_idx": 103, "transpos": [103, 179, 211, 244, 348], "head_dim": 103, "ascend": 104, "descend": 104, "stabl": 104, "preserv": 104, "sorted_tensor": 104, "sorted_tensor_desc": 104, "indices_desc": 104, "input_tensor_2d": 104, "sorted_tensor_dim": 104, "indices_dim": 104, "fill_valu": [107, 123, 124, 369], "hone": [109, 110], "wone": [109, 110], "val_hi": [109, 110], "val_lo": [109, 110], "count": [109, 110, 368, 373, 387], "ye": [109, 110, 368, 369], "high": [109, 110, 371, 374, 390], "region": [109, 110, 240], "nchw": 110, "hw": [110, 369], "rest": 110, "hfill": 110, "wfill": 110, "hi": 110, "lo": 110, "expect": [110, 214, 348, 365, 368, 369, 372, 383], "low": 110, "divis": [114, 128, 133, 257, 267, 325, 368, 369, 374], "rounding_mod": [115, 257, 274], "wh_b0": [115, 274], "padded_shap": [117, 243], "pad_valu": [117, 122, 244, 279, 305, 337, 368], "target_layout": [117, 118, 368, 369], "target_mem_config": [117, 118], "padded_tensor": 117, "output_mem_config": [117, 118, 387], "unpadded_tensor": 118, "cq_id": [121, 122, 327, 338, 341, 368, 369], "complet": [121, 327, 369], "device_tensor": [121, 338], "host_tensor": 121, "itself": [122, 368], "twice": [122, 373], "purpos": [122, 365, 372, 374], "mapper": 122, "uint16": [123, 151, 238, 342, 362, 374], "filled_tensor": [123, 124], "templat": [124, 239, 363, 373], "greatest": 125, "divisor": [125, 258], "adapt": [132, 369], "signal": [132, 369, 387], "compos": [132, 341, 369], "sever": [132, 369], "plane": [132, 369, 388], "input_mask": 134, "inplac": [134, 196, 198, 200, 202, 388], "output_layout": [134, 369], "num_group": [134, 369], "lambd": [138, 139, 309, 310], "lambda": [139, 310, 364, 386, 388], "16666667": [140, 142], "shift": [140, 142, 369], "min_val": 144, "max_val": 144, "sfpu": 151, "shouldn": 151, "instead": [151, 273, 369, 383, 385, 389], "lower": 151, "float16": [151, 383], "imaginari": 153, "batch_id": 154, "replac": [154, 334], "denot": [154, 369], "boolean": [155, 156, 330, 342, 369], "rtol": 157, "05f": 157, "atol": 157, "08f": 157, "equal_nan": 157, "leq": 157, "otherwis": [157, 327, 342, 368, 385, 390], "rel": 157, "toler": 157, "absolut": [157, 165, 369], "nan": [157, 387], "treat": [157, 211], "batch_index": 163, "popul": [163, 221, 373], "update_index": 164, "batch_offset": 164, "input_refer": [165, 224], "input_predict": [165, 224], "residual_input_tensor": [166, 280], "program_config": [166, 179, 211, 280, 343, 344, 346, 347], "programconfig": [166, 280], "least": [167, 211, 373], "recommend": [169, 364, 371, 383, 390], "80": [169, 371], "outsid": 169, "negative_slop": [172, 173], "slope": 172, "leaki": 172, "01": 173, "point": [174, 307, 366, 369, 373, 374], "transpose_a": [179, 211], "transpose_b": [179, 211], "matmulprogramconfig": [179, 211], "output_til": [179, 211], "behaviour": [179, 211], "grid": [179, 211, 346, 347, 354, 364, 374], "128": [179, 211, 390], "whb0": [182, 186, 275], "logarithm": [188, 227], "AND": [195, 196, 369], "OR": [199, 200, 279, 369], "land": [201, 202], "lnot": [201, 202], "lor": [201, 202], "xor": [202, 369], "logitep": 205, "context": [210, 265, 266], "exit": 210, "occur": 210, "therefor": [211, 383, 385], "dimension": [211, 227, 369, 374, 383], "addit": [211, 364, 369], "front": [211, 373, 389], "These": [211, 365, 366, 369, 371, 375, 390], "d": [211, 387], "allow": [211, 365, 368, 369, 372], "possibl": [211, 341, 364, 369, 386], "although": 211, "combin": 211, "most": [211, 383], "variou": 211, "align": [211, 368, 369, 373], "appropri": [211, 365, 371], "abov": [211, 371], "criteria": 211, "messag": [211, 373], "unexpect": [211, 369], "obviou": [211, 383], "except": [211, 369, 383, 385, 387], "scenario": [211, 374], "relat": [211, 369], "swap": 211, "j": 211, "implicitli": 211, "extend": 211, "patch": 211, "leverag": [211, 369], "accord": [211, 244, 276], "1d": [211, 235, 252, 387], "look": [211, 342, 365, 370, 374, 387, 389], "determin": [211, 221, 222, 369, 374, 383, 384, 385, 386, 387, 388], "n_size": 211, "m_size": 211, "k_size": 211, "p": [211, 374], "though": [211, 366], "constraint": [211, 374, 383], "chosen": [211, 373, 389], "carefulli": 211, "fix": 211, "problem": 211, "input_h": 214, "input_w": 214, "kernel_s": [214, 369, 388], "dilat": [214, 369, 388], "applied_shard_schem": 214, "tensormemorylayout": [214, 368, 369, 388], "ceil_mod": [214, 369], "in_plac": 214, "window": [214, 369], "nhw": 214, "scheme": 214, "convolv": [214, 369], "halo": 214, "createdevic": [214, 366, 368, 388], "l1_small_siz": [214, 240, 385, 386, 388], "8192": [214, 385, 386], "kernel_h": 214, "kernel_w": 214, "stride_h": 214, "stride_w": 214, "pad_h": 214, "pad_w": 214, "dilation_h": 214, "dilation_w": 214, "nchw_shape": 214, "40": [214, 387], "in_n": 214, "in_c": 214, "in_h": 214, "in_w": 214, "input_perm": 214, "input_reshap": 214, "tt_input": 214, "tt_input_dev": 214, "tt_output": [214, 366, 368], "block_shard": [214, 388], "in_place_halo": 214, "20": [220, 307, 308, 373, 387, 389], "initialize_model": [221, 222, 364, 386, 388], "model_nam": [221, 222, 364, 386, 387, 388], "convert_to_ttnn": [221, 222, 364], "custom_preprocessor": [221, 222, 364, 386, 388], "parameterdict": [221, 222], "prefix": [221, 222], "run_model": [221, 388], "reader_patterns_cach": 221, "disabl": [221, 222, 369, 383, 384, 385, 386, 387, 388, 390], "doesn": [221, 222, 383], "invalid": [221, 222], "alreadi": [221, 222, 240, 366, 375, 387, 389], "been": [221, 222, 327, 365, 371, 389], "preprocessor": [221, 222], "put": [221, 222, 364, 383, 385, 388], "submodul": [221, 222, 371], "appear": [221, 222, 371], "ttnn_module_arg": [221, 388], "tmp": [221, 383, 384, 385, 386, 388, 389], "model_graph": 221, "svg": [221, 388, 389, 390], "reader": 221, "avoid": [221, 307, 373, 374], "recomput": [221, 369], "multivari": 227, "mvlgamma": 227, "5f": 227, "_float": 233, "neq": 233, "well": [235, 368, 372], "trace_region_s": 240, "dispatch_core_config": [240, 385, 386], "dispatchcoreconfig": [240, 385, 386], "0x7f72c595df30": 240, "small": [240, 387], "default_l1_small_s": 240, "default_trace_region_s": 240, "dispatch_core_typ": [240, 385, 386], "dispatchcoretyp": [240, 385, 386], "0x7fbac5bfc1b0": 240, "otim": 241, "output_tensor_shap": [242, 337, 368], "input_tensor_start": [242, 368], "locat": [242, 369, 370, 373, 387, 390], "mutual": 242, "describ": [242, 365, 369], "unpadded_shap": 243, "annot": [243, 365, 368], "fixeds": [243, 368], "tthe": 244, "broken": 244, "garbag": 244, "cartesian": 245, "theta": 245, "k": [247, 342, 346, 347, 369, 384], "decim": [247, 281], "coeff": 249, "coeffici": [249, 365], "sum_": [249, 369], "polynomi": 249, "expon": [250, 251, 283, 284, 366, 374], "power": [251, 353, 366], "length": [252, 346, 347], "arrai": 252, "all_dimens": [253, 254], "keepdim": [253, 369], "particular": [254, 364, 383, 390], "numer": [257, 307, 369, 374], "new_tensor": 261, "my_memory_config": 261, "inaccur": [262, 374], "due": [262, 373, 374, 383], "characterist": 262, "fp": 262, "inform": [262, 369, 371, 375], "reduce0scatt": 264, "upper_limit": 272, "cap": 272, "lower_limit": 273, "carri": 273, "modulu": 274, "repetition_vector": 276, "smallvector": 276, "repetit": [276, 278, 369], "he": 278, "fit": 278, "expand": [278, 374], "torch_input_tensor": [278, 388, 390], "torch_result": 278, "cost": [279, 369], "view": [279, 390], "condit": [279, 365], "met": 279, "new_shap": 279, "kwtype": 279, "upto": 283, "28": [283, 387, 389], "posit": [283, 347, 366, 369], "subract": 288, "revers": 288, "0507": 290, "67326": 290, "modifi": [292, 307, 366, 383], "short": [292, 387, 390], "slice_start": 305, "slice_end": 305, "slice_step": 305, "input_tensor_shap": [305, 368], "unmodifi": 305, "undefin": [305, 369], "0310059": 306, "By": [307, 372, 384], "steep": 307, "higher": [307, 371, 373, 374], "steeper": 307, "approach": [307, 364, 372, 390], "hard": [307, 383], "larg": [307, 364, 374, 386], "improv": [307, 365], "stabil": [307, 369, 372], "veri": [307, 373, 383, 385], "queueid": [327, 338, 368, 369], "sub_device_id": 327, "subdeviceid": 327, "synchron": [327, 390], "wait": [327, 369, 373], "associ": 327, "ran": [327, 373, 385], "chip": 327, "set_sub_device_stall_group": 327, "queu": 327, "multi_devic": [327, 338, 368], "accuraci": [330, 365], "better": [330, 369], "use_multicor": [336, 337, 354, 355], "organ": [339, 365, 374], "becom": 339, "thread": [339, 368, 387, 390], "42188": 339, "398438": 339, "torch_rank": [341, 390], "mesh_compos": 341, "meshtotensor": 341, "Will": 341, "squeez": [341, 383, 384, 385, 386, 388], "reach": 341, "torch_tensor": [341, 383], "3008": 341, "8438": 341, "3242": 341, "9023": 341, "5820": 341, "5312": 341, "largest": [342, 369, 374], "smallest": [342, 369], "sure": [342, 375, 390], "bfloat8": 342, "head_siz": [343, 344, 345, 348, 385], "attention_mask": [343, 344, 385], "softmaxprogramconfig": [343, 344], "softmaxdefaultprogramconfig": [343, 344], "causal_mask": [343, 344], "mask": [343, 344, 347], "causal": [343, 344, 346], "num_head": [345, 348, 385], "sequence_s": [345, 348, 364, 385, 386, 387], "input_tensor_q": [346, 347], "input_tensor_k": [346, 347], "input_tensor_v": [346, 347], "attn_mask": [346, 347], "is_casu": 346, "sdpaprogramconfig": [346, 347], "dot": [346, 347], "mimick": 346, "flashattent": 346, "accept": [346, 347, 365, 372], "q": [346, 347], "parallel": [346, 347, 369, 373, 387], "nqh": 346, "dh": [346, 347], "nkv": [346, 347], "impli": 346, "is_caus": 347, "cur_po": 347, "cur_pos_tensor": 347, "decod": [347, 389], "mqa": 347, "sdpamulticoreprogramconfig": 347, "nh": 347, "pnh": 347, "skip": [347, 383, 384, 385, 386, 387, 388, 389], "kv_input_tensor": 348, "num_kv_head": 348, "transpose_kei": 348, "hidden_s": [348, 364, 385, 386], "Then": [348, 364, 371], "score": [348, 387], "q1": 348, "k1": 348, "v1": [348, 371], "qn": 348, "kn": 348, "vn": 348, "cat": [348, 385, 389], "contigu": [348, 369, 383], "num": 348, "diagon": [349, 350], "ops_chain": 353, "unarywithparam": 353, "chain": 353, "unaryoptyp": [353, 369], "use_pack_until": [354, 355], "sub_core_grid": 354, "corerangeset": 354, "output_tensor_end": [355, 368], "scale_factor": [356, 369], "array2d": 356, "nearest": [356, 369], "cdot": 360, "basi": 364, "re": [364, 369, 370, 384], "rewritten": 364, "bert": [364, 370, 386, 387], "modeling_bert": [364, 386], "bertintermedi": 364, "class": [364, 365, 368, 369, 373, 374, 387, 388, 389], "super": [364, 388], "dens": 364, "intermediate_s": 364, "forward": [364, 369, 388], "hidden_st": [364, 385, 386], "tdd": 364, "torch_bert": 364, "utility_funct": 364, "torch_random": 364, "utils_for_test": 364, "assert_with_pcc": 364, "mark": [364, 365], "parametr": 364, "phiyodr": [364, 386], "finetun": [364, 386], "squad2": [364, 386], "384": [364, 385, 386], "test_bert_intermedi": 364, "manual_se": [364, 383, 384, 385, 388, 389, 390], "bertconfig": [364, 386], "from_pretrain": [364, 386, 389], "eval": [364, 386, 387, 388, 389], "torch_hidden_st": [364, 385], "torch_output": [364, 385], "_": [364, 373, 385, 389], "bert_intermedi": 364, "9999": [364, 383, 384, 385, 386, 388, 390], "dictionari": 364, "turn": 364, "ttnn_bert": [364, 386], "999": 364, "bias": [364, 369, 385], "someth": 364, "ttnn_optimized_bert": [364, 386], "isinst": 364, "preprocess_linear_weight": [364, 385], "preprocess_linear_bia": [364, 385], "ff1_weight": 364, "ff1_bia": 364, "local": [364, 365, 374], "best": [364, 383], "integr": [364, 365, 366, 369], "incredibli": 365, "excit": 365, "exploratori": 365, "freedom": 365, "showcas": 365, "few": [365, 374, 383], "question": 365, "answer": 365, "see": [365, 369, 372, 383, 387, 389], "highlight": [365, 374], "successfulli": [365, 375, 387, 389], "migrat": [365, 383, 384, 385, 386, 388, 390], "good": 365, "documen": 365, "credit": 365, "author": 365, "might": [365, 369, 384], "encount": 365, "demonstr": [365, 373], "adequ": 365, "achiev": [365, 372], "pearson": 365, "correl": [365, 369], "ci": 365, "pipelin": [365, 373], "unit": [365, 369], "metric": 365, "meet": 365, "continu": [365, 371, 372], "upon": 365, "everi": [365, 368, 373, 385, 390], "commit": [365, 387], "ongo": 365, "complianc": 365, "catch": 365, "regress": 365, "earli": 365, "collect": [365, 373, 375, 387], "usag": [365, 371], "varieti": [365, 369], "measur": 365, "special": [365, 369, 374], "run_device_perf_model": 365, "models_device_performance_bare_met": 365, "schedul": 365, "clear": [365, 372, 383, 384, 385, 386, 387, 388], "incorpor": 365, "autom": 365, "extern": [365, 372, 374], "servic": 365, "impact": 365, "run_perf_models_oth": 365, "run_perf_models_llm_javelin": 365, "run_perf_models_cnn_javelin": 365, "models_performance_bare_met": 365, "run_demos_single_card_n150_test": 365, "run_demos_single_card_n300_test": 365, "run_t3000_demo_test": 365, "test_ttnn_functional_resnet50": 365, "resnet50testinfra": 365, "setup": [365, 370, 371, 387, 389], "handl": [366, 383], "machin": [366, 368, 369, 373, 375, 389], "send": [366, 368, 369], "__name__": 366, "__main__": [366, 387], "pci": [366, 383, 384, 385, 386, 387, 388], "slot": 366, "tt_devic": [366, 368, 369], "py_tensor": [366, 368], "tt_tensor": [366, 368, 369], "tolist": [366, 368], "tt_relu_out": 366, "closedevic": 366, "power_fp": 366, "suppli": [366, 368, 369], "lastli": 366, "fallback_op": [366, 369], "py_tensor_exp": 366, "py_relu_out": 366, "py_pow_out": 366, "tt_pow_out": 366, "behav": [366, 369], "regular": 366, "hood": 366, "tt_silu_out": 366, "tt_exp_out": 366, "leav": 366, "anyth": 366, "manipul": 368, "sent": 368, "receiv": [368, 373], "platform": [368, 370, 387], "ttdnn": 368, "z": [368, 369, 373, 389], "construct": [368, 390], "nor": 368, "subsect": 368, "insid": [368, 390], "63": [368, 387], "65": [368, 387, 389], "66": 368, "127": [368, 387], "3968": 368, "3969": 368, "3970": 368, "4031": 368, "4032": 368, "4033": 368, "4034": 368, "4095": 368, "4097": 368, "4098": 368, "4159": 368, "4160": 368, "4161": 368, "6462": 368, "4223": 368, "8064": 368, "8065": 368, "8066": 368, "8127": 368, "8128": 368, "8129": 368, "8130": 368, "8191": 368, "95": 368, "1984": 368, "1985": 368, "2015": [368, 387], "33": [368, 384, 387], "96": [368, 387, 390], "97": [368, 387], "2016": 368, "2017": [368, 387, 389], "2047": 368, "2080": 368, "2081": 368, "2111": 368, "2144": 368, "2145": 368, "2175": 368, "4064": 368, "4065": 368, "fourth": [368, 369], "6111": 368, "6176": 368, "ownedstorag": [368, 369], "borrowedstorag": 368, "devicestorag": [368, 369], "pointer": 368, "That": [368, 374, 384], "underli": 368, "numpi": [368, 374, 387, 389], "reason": 368, "data_typ": [368, 385], "No": [368, 369], "bank": 368, "arg1": 368, "arg2": 368, "arg3": 368, "divisbl": [368, 369], "arg4": 368, "arg5": 368, "single_bank": 368, "memory_layout": [368, 369], "shard_spec": 368, "nullopt": [368, 388], "ptr": 368, "np": 368, "owned_buffer_for_uint8_t": 368, "owned_buffer_for_uint16_t": 368, "owned_buffer_for_int32_t": 368, "owned_buffer_for_uint32_t": 368, "owned_buffer_for_float32_t": 368, "owned_buffer_for_bfloat16_t": 368, "borrowed_buffer_for_uint8_t": 368, "borrowed_buffer_for_uint16_t": 368, "borrowed_buffer_for_int32_t": 368, "borrowed_buffer_for_uint32_t": 368, "borrowed_buffer_for_float32_t": 368, "borrowed_buffer_for_bfloat16_t": 368, "everywher": 368, "inp": 368, "tt_tensor_pad": 368, "npad": 368, "bottom": [368, 375], "storagetyp": 368, "cq": 368, "uint8_t": 368, "ti": 368, "output_tensor_start": 368, "tt_tensor_unpad": 368, "nunpad": 368, "apart": 368, "restrict": 368, "eight": 368, "shardspec": 368, "dram_channel": 368, "rememb": 368, "py_output": 368, "unifi": 369, "tt_eager": [369, 387], "caller": 369, "launch": [369, 375], "plug": 369, "declar": 369, "newoper": 369, "programwithcallback": 369, "create_program": 369, "some_memb": 369, "optional_input_tensor": 369, "validate_with_output_tensor": 369, "programwithoptionaloutputtensor": 369, "box": [369, 371], "preferred_nam": 369, "parallelization_strategi": 369, "get_parallelization_strategi": 369, "parallelizationstrategyenum": 369, "enqueu": 369, "finish": [369, 373, 387], "asynchron": 369, "reload": 369, "program_cach": 369, "disable_and_clear": 369, "entri": 369, "num_entri": 369, "cachabl": 369, "override_runtime_args_callback": 369, "input_buff": 369, "output_buff": 369, "src_dram_buff": 369, "dst_dram_buff": 369, "tt_metal_logger_typ": [369, 390], "tt_metal_logger_level": [369, 390], "1280": 369, "layoutconversiononhost": 369, "miss": [369, 390], "eltwiseunari": 369, "op_typ": 369, "_tt": 369, "mul": [369, 385], "ellipsi": 369, "output_on_devic": 369, "third": 369, "fewer": 369, "four": 369, "side": [369, 373, 383, 384, 385, 386, 387, 388], "connect": 369, "paper": 369, "separ": 369, "normalized_shap": 369, "layer": [369, 373, 388], "much": [369, 373, 384], "m": [369, 371, 384], "reflect": [369, 372], "replic": 369, "circular": 369, "align_corn": 369, "recompute_scale_factor": 369, "antialia": 369, "down": 369, "bilinear": 369, "bicub": 369, "trilinear": 369, "area": 369, "exact": [369, 374, 385], "center": 369, "corner": 369, "pixel": 369, "anti": 369, "alias": 369, "output_s": 369, "total": [369, 373], "sigma": 369, "logist": 369, "x_": 369, "x_i": 369, "sum_j": 369, "x_j": 369, "lie": 369, "in_channel": [369, 388], "out_channel": [369, 388], "padding_mod": 369, "simplest": 369, "c_": 369, "h_": 369, "w_": 369, "precis": [369, 374], "n_i": 369, "_j": 369, "star": 369, "cross": 369, "learnabl": 369, "num_batches_track": 369, "num_featur": 369, "affin": 369, "track_running_stat": 369, "4d": 369, "deep": 369, "intern": [369, 370], "covari": 369, "track": 369, "num_channel": 369, "lernabl": 369, "elementwise_affin": 369, "return_indic": 369, "channels_last": [369, 373], "reshape_2d": 369, "kh": 369, "kw": 369, "c_j": 369, "max_": 369, "ldot": 369, "implicit": 369, "infin": [369, 374], "mod": 369, "dividend": 369, "bitwis": 369, "NOT": [369, 383, 384, 385, 386, 388], "immedi": 369, "arithmet": 369, "operand": 369, "promot": 369, "behavior": [369, 374], "retain": 369, "argmin": 369, "fusion": 369, "togeth": 369, "fused_op": 369, "in_featur": 369, "out_featur": 369, "num_dim": 369, "moment": 369, "add_and_norm": 369, "flexibl": 369, "earlier": 369, "while": [369, 372], "ml": 370, "workload": 370, "falcon": 370, "7b": 370, "navig": [370, 371], "mistral": 370, "llama2": 370, "70b": 370, "soon": 370, "t3000": [370, 371], "learn": [370, 374, 384, 387], "dive": 370, "deeper": 370, "jupyt": [370, 375, 387], "notebook": [370, 375, 387, 389], "softwar": [371, 375, 383, 384, 385, 386, 388], "packag": [371, 386, 387, 389], "asset": 371, "tag": 371, "compat": [371, 376, 379, 380], "o": [371, 373, 383, 384, 385, 386, 387, 389], "ubuntu": [371, 383, 384, 385, 386, 387, 388, 389], "04": [371, 389], "fw_pack": 371, "v80": 371, "v2": 371, "blackhol": 371, "v3": 371, "wget": 371, "raw": [371, 389], "githubusercont": 371, "install_depend": 371, "chmod": 371, "sudo": [371, 373], "dkm": 371, "debian": 371, "apt": 371, "fedora": 371, "dnf": 371, "enterpris": 371, "linux": [371, 387], "epel": 371, "cd": [371, 373, 387], "modprob": 371, "visit": 371, "pip": [371, 387, 389], "reboot": [371, 373], "curl": 371, "fwbundl": 371, "l": 371, "fw": [371, 373, 383, 384, 385, 386, 388], "tar": 371, "correctli": 371, "telemetri": 371, "loudbox": 371, "quietbox": 371, "closer": 371, "quickest": 371, "ai": [371, 383, 384, 385, 386, 387, 388], "quick": 371, "who": [371, 374], "recurs": 371, "build_met": [371, 373, 387], "pandoc": [371, 387], "libtbb": 371, "dev": [371, 387], "libcapston": 371, "pkg": 371, "doxygen": 371, "registri": 371, "pull": [371, 372], "ghcr": 371, "io": [371, 373], "amd64": 371, "rc": 371, "rm": 371, "hugepag": [371, 383, 384, 385, 386, 387, 388], "1g": 371, "bash": [371, 387], "architectur": [371, 376, 379, 380, 390], "choic": [371, 383], "wheel_fil": 371, "whl": [371, 387, 389], "governor": 371, "txt": [371, 387], "cpufrequtil": 371, "cpupow": 371, "frequenc": 371, "arch_nam": [371, 385, 386], "tt_metal_hom": [371, 373, 387], "python3": [371, 386, 387, 389], "run_op_on_devic": 371, "intend": 372, "reliabl": 372, "simultan": 372, "fine": 372, "tune": 372, "themselv": [372, 374], "goal": 372, "ask": 372, "driven": 372, "popular": 372, "kent": 372, "beck": 372, "term": 372, "benefit": 372, "submit": 372, "label": [372, 374], "fulli": [372, 373], "branch": 372, "brief": 372, "4730": 372, "rst": 372, "referenc": [372, 387], "sweep": 372, "codeown": 372, "pr": 372, "comment": 372, "test_perf_resnet": 373, "test_perf_bare_met": 373, "0185": 373, "consol": 373, "similar": [373, 383, 385], "shorter": 373, "cli": 373, "explain": 373, "reset": 373, "tt_smi": 373, "tensix_reset": 373, "tensix": 373, "skew": 373, "timer": 373, "wh": 373, "analyz": 373, "1000": [373, 388, 389], "fixtur": 373, "ttl": 373, "dumpdeviceprofil": 373, "drop": 373, "around": 373, "120": [373, 387], "eighth": 373, "warn": [373, 383, 384, 385, 386, 387, 388], "mention": 373, "risc": 373, "faster": [373, 385], "those": 373, "analysi": 373, "affect": 373, "flow": 373, "column": [373, 374, 387], "python_fallback": [373, 387], "tt_dnn_cpu": 373, "tt_dnn_devic": [373, 387], "global": [373, 387], "fidel": [373, 387], "field": 373, "lofi": [373, 388], "hifi2": 373, "hifi3": 373, "clock": 373, "stamp": 373, "durat": [373, 385, 387, 390], "nanosecond": 373, "end_t": 373, "start_t": 373, "cycl": 373, "earliest": 373, "core_frequ": 373, "marker": 373, "brisc": 373, "ncrisc": 373, "trisc0": 373, "trisc1": 373, "trisc2": 373, "cb": 373, "spent": 373, "cb_wait_front": 373, "reserv": 373, "cb_reserve_back": 373, "datamov": 373, "input_0_memori": 373, "dev_0_dram": 373, "dec_0_l1": 373, "tgz": 373, "filenam": [373, 389], "item": [373, 387], "aggreg": 373, "timestamp": 373, "4x4": 374, "32x32": 374, "still": 374, "transit": 374, "2x2": 374, "illustr": 374, "byte": 374, "sizeof": 374, "introduc": 374, "observ": 374, "magnitud": [374, 385], "smaller": 374, "flush": 374, "instabl": 374, "extrem": 374, "domin": 374, "caus": 374, "lose": 374, "7014118346046923e": 374, "dataset": [374, 387], "frequent": 374, "occurr": 374, "uniform": 374, "deal": 374, "critic": 374, "homogen": 374, "unsuit": 374, "inher": 374, "owned_host_storag": 374, "borrowed_host_storag": 374, "device_storag": 374, "ideal": 374, "abstract": 374, "awai": 374, "compress": 374, "upper": 374, "remain": 374, "128x128": 374, "subset": 374, "know": 374, "understand": 374, "unshard": 374, "coordin": 374, "physic": 374, "pybind11_object": 374, "properti": 374, "certain": 375, "ramp": 375, "skillset": 375, "tree": 375, "lab": 375, "port": 375, "8888": 375, "hint": 375, "Be": 375, "alwai": [375, 383, 386, 387], "cell": 375, "central": 383, "sens": 383, "sram": 383, "concept": 383, "2024": [383, 384, 385, 386, 387, 388, 389, 390], "07": [383, 384, 385, 386, 388], "18": [383, 384, 385, 386, 387, 389], "48": [383, 386, 387], "818": 383, "136": [383, 384, 385, 386], "cache_path": [383, 384, 385, 386, 388], "posixpath": [383, 384, 385, 386], "home": [383, 384, 385, 386, 387, 388, 389], "comparison_mode_pcc": [383, 384, 385, 386, 388, 390], "enable_comparison_mod": [383, 384, 385, 386, 388, 390], "enable_detailed_buffer_report": [383, 384, 385, 386, 388, 390], "enable_detailed_tensor_report": [383, 384, 385, 386, 388, 390], "enable_fast_runtime_mod": [383, 384, 385, 386, 388, 390], "enable_graph_report": [383, 384, 385, 386, 388, 390], "enable_log": [383, 384, 385, 386, 388, 390], "enable_model_cach": [383, 384, 385, 386, 388], "model_cache_path": [383, 384, 385, 386, 388], "report_nam": [383, 384, 385, 386, 388, 390], "root_report_path": [383, 384, 385, 386, 388], "throw_exception_on_fallback": [383, 384, 385, 386, 388], "tmp_dir": [383, 384, 385, 386, 388, 389], "905": 383, "operation_decor": [383, 384, 385, 386, 388], "758": [383, 384, 385, 386], "906": 383, "907": [383, 385], "908": [383, 385], "909": [383, 385], "910": [383, 385], "911": [383, 385], "914": [383, 385], "915": [383, 385], "pearson_correlation_coeffici": [383, 384, 385, 386, 388], "919": [383, 385], "920": 383, "921": [383, 385], "unsqueeze_to_4d": [383, 384, 385, 386, 388], "922": [383, 385], "923": [383, 385], "924": [383, 385], "925": [383, 385], "926": [383, 385], "allocate_tensor_on_devic": [383, 384, 385, 386, 388], "copy_host_to_device_tensor": [383, 384, 385, 386, 388], "927": [383, 385], "928": [383, 385], "929": [383, 385], "930": [383, 385], "931": [383, 385], "934": [383, 385], "935": 383, "936": [383, 385, 386], "937": 383, "938": [383, 385], "941": [383, 385], "942": [383, 385], "943": [383, 385], "948": [383, 385], "949": [383, 385, 386], "950": [383, 385], "951": [383, 385, 386], "952": [383, 385], "953": [383, 385], "954": [383, 385], "955": [383, 385], "958": [383, 385], "959": 383, "960": [383, 385, 386], "f": [383, 384, 385, 387, 389, 390], "As": [383, 384], "1234": 383, "again": 383, "action": 383, "98300": 383, "11301": 383, "37592": 383, "64318": 383, "53437": 383, "59434": 383, "69190": 383, "04268": 383, "33346": 383, "20231": 383, "15127": 383, "58303": 383, "pick": 383, "80078": 383, "69531": 383, "71484": 383, "33398": 383, "60156": 383, "36523": 383, "73047": 383, "90625": 383, "59766": 383, "83203": 383, "61719": 383, "53516": 383, "effici": [383, 384], "transfer": 383, "intuit": 383, "nshape": 383, "nlayout": 383, "21680": 383, "24316": 383, "19336": 383, "40625": 383, "81641": 383, "50781": 383, "09961": 383, "54688": 383, "70703": 383, "93359": 383, "06787": 383, "75781": 383, "insert": 383, "cale": 383, "info": [383, 384, 385, 386, 387, 388, 389], "49": [383, 387, 389], "027": [383, 384], "silicondriv": [383, 384, 385, 386, 387, 388], "detect": [383, 384, 385, 386, 387, 388], "040": 383, "init_detect_tt_device_numanod": [383, 384, 385, 386, 387, 388], "could": [383, 384, 385, 386, 387, 388], "numanodeset": [383, 384, 385, 386, 387, 388], "pci_bus_id": [383, 384, 385, 386, 387, 388], "0000": [383, 384, 385, 386, 387, 388], "00": [383, 384, 385, 386, 387, 388, 389], "041": 383, "bind_area_memory_nodeset": [383, 384, 385, 386, 387, 388], "unabl": [383, 384, 385, 386, 387, 388], "numanod": [383, 384, 385, 386, 387, 388], "membind": [383, 384, 385, 386, 387, 388], "ttsilicondevic": [383, 384, 385, 386, 387, 388], "init_hugepag": [383, 384, 385, 386, 387, 388], "bind_area_to_memory_nodeset": [383, 384, 385, 386, 387, 388], "fail": [383, 384, 385, 386, 387, 388], "ch": [383, 384, 385, 386, 387, 388], "effect": [383, 384, 385, 386, 387, 388], "decreas": [383, 384, 385, 386, 387, 388], "893": [383, 384, 385, 386, 387, 388], "082": 383, "ethernet": [383, 384, 385, 386, 388], "clk": [383, 384, 385, 386, 387, 388], "800": [383, 384, 385, 386], "mhz": [383, 384, 385, 386, 387, 388], "torch_input_tensor_a": [383, 390], "torch_input_tensor_b": [383, 390], "stai": 383, "unless": [383, 390], "explicit": 383, "figur": 383, "hang": 383, "properli": 383, "41": [384, 387], "903": 384, "989": [384, 385], "990": [384, 385], "991": 384, "992": 384, "993": 384, "994": 384, "995": 384, "996": 384, "001": 384, "002": 384, "003": 384, "004": 384, "005": 384, "006": 384, "007": 384, "008": 384, "009": 384, "010": 384, "011": 384, "012": 384, "013": 384, "015": 384, "016": 384, "017": 384, "018": 384, "020": 384, "021": 384, "022": 384, "028": 384, "029": 384, "030": 384, "031": 384, "032": 384, "033": 384, "035": 384, "036": 384, "037": 384, "053": 384, "066": 384, "067": 384, "094": 384, "repeatedli": 384, "enable_program_cach": [384, 385, 390], "torch_a": 384, "torch_b": 384, "longer": 384, "signific": 384, "aslo": 384, "why": 384, "conver": 384, "todo": 384, "75000": 384, "25000": 384, "50000": 384, "62500": 384, "effeci": 384, "further": 384, "enjoi": 384, "massiv": 384, "eth": [385, 386], "54": [385, 387], "821": 385, "912": 385, "939": 385, "976": 385, "55": [385, 387], "014": 385, "fashion": 385, "multi_head_attent": 385, "query_weight": 385, "query_bia": 385, "key_weight": 385, "key_bia": 385, "value_weight": 385, "value_bia": 385, "output_weight": 385, "output_bia": 385, "fallback_reshap": 385, "get_fallback_funct": [385, 390], "attention_scor": 385, "attention_prob": 385, "context_lay": 385, "self_output": 385, "torch_attention_mask": [385, 386], "torch_query_weight": 385, "torch_query_bia": 385, "torch_key_weight": 385, "torch_key_bia": 385, "torch_value_weight": 385, "torch_value_bia": 385, "torch_output_weight": 385, "torch_output_bia": 385, "00607705116272": 385, "250946044921875": 385, "ahead": 385, "optimized_multi_head_attent": 385, "fused_qkv_weight": 385, "fused_qkv_bia": 385, "self_output_weight": 385, "self_output_bia": 385, "fused_qkv_output": 385, "context_layer_after_concatenate_head": 385, "qkv": 385, "torch_qkv_weight": 385, "torch_qkv_bia": 385, "qkv_weight": 385, "qkv_bia": 385, "optimized_output": 385, "474989175796509": 385, "17": [385, 386, 387, 388, 389], "020017147064208984": 385, "torch_optimized_output": 385, "allclos": 385, "19": [385, 387, 389], "ttnn_config_overrid": [386, 388, 390], "47": [386, 387], "183": 386, "133": [386, 387], "overrid": [386, 387, 390], "184": 386, "354": 386, "355": 386, "356": 386, "357": 386, "358": 386, "359": 386, "360": [386, 389], "362": 386, "366": 386, "367": 386, "368": 386, "369": 386, "370": 386, "371": 386, "372": 386, "373": [386, 387], "374": 386, "378": 386, "379": 386, "380": [386, 387], "381": 386, "383": 386, "385": 386, "390": 386, "391": 386, "392": 386, "393": 386, "394": 386, "395": 386, "396": 386, "397": 386, "398": 386, "399": 386, "set_verbosity_error": 386, "100": [386, 387], "412": 386, "442": 386, "447": 386, "googl": [386, 389], "bert_uncased_l": 386, "4_h": 386, "256_a": 386, "bertselfoutput": 386, "site": [386, 387, 389], "huggingface_hub": [386, 387], "file_download": 386, "1132": 386, "futurewarn": 386, "resume_download": 386, "resum": 386, "force_download": 386, "874": 386, "num_hidden_lay": 386, "bertforquestionansw": 386, "input_id": 386, "vocab_s": 386, "torch_token_type_id": 386, "torch_position_id": 386, "ttnn_bert_input": 386, "preprocess_input": 386, "bert_for_question_answ": 386, "50": 386, "339": 386, "manage_config": [386, 388, 390], "144": 386, "340": 386, "341": 386, "555": 386, "_paramet": [386, 388], "env": [386, 387, 388, 389], "34": [386, 387, 388], "343": 386, "634": 386, "636": 386, "147": [386, 387], "restor": [386, 388], "02": [386, 387], "947": 386, "unset": 387, "silent": 387, "nuke": 387, "jobserv": 387, "unavail": 387, "j1": 387, "parent": 387, "rule": 387, "artifact": 387, "conf": 387, "backend": 387, "pypi": [387, 389], "org": [387, 389], "satisfi": [387, 389], "setuptool": 387, "44": 387, "py3": 387, "kb": 387, "edit": 387, "obtain": 387, "statu": 387, "metadata": [387, 389], "click": 387, "loguru": 387, "58": 387, "ipywidget": 387, "139": 387, "90": [387, 388], "db": 387, "290ab3a34f2ef0b5a0f89235dc2d40fea83e77de84ed2dc05c": 387, "pyyaml": [387, 389], "cp38": 387, "linux_x86_64": 387, "jupyterlab": 387, "mb": 387, "pyelftool": 387, "py2": 387, "174": 387, "4f": 387, "ed": 387, "863cf4386fe6db3c09333712009ec1c5146a36f3904b469d13": 387, "curtsi": 387, "91": 387, "b7": 387, "0c117d73912c6c2beb1eb0d7d6884f4e79e6e5b5e91eeb34f5": 387, "torchtrail": 387, "manylinux_2_12_x86_64": 387, "manylinux2010_x86_64": 387, "matplotlib": 387, "toolz": 387, "pillow": [387, 389], "manylinux_2_17_x86_64": 387, "manylinux2014_x86_64": 387, "panda": 387, "2bcpu": 387, "199": 387, "dash": 387, "rich": 387, "238": 387, "seaborn": 387, "293": 387, "plotli": 387, "traitlet": 387, "85": 387, "widgetsnbextens": 387, "ipython": [387, 388, 389], "798": 387, "widget": 387, "jupyterlab_widget": 387, "215": 387, "comm": 387, "async": 387, "lru": 387, "async_lru": 387, "tomli": 387, "python_vers": 387, "server": 387, "jupyter_serv": 387, "jinja2": [387, 389], "ipykernel": 387, "116": 387, "shim": 387, "notebook_shim": 387, "jupyterlab_serv": 387, "lsp": 387, "jupyter_lsp": 387, "68": 387, "23": 387, "53": 387, "importlib": [387, 389], "importlib_resourc": 387, "importlib_metadata": 387, "jupyter_cor": 387, "tornado": 387, "abi3": 387, "manylinux_2_5_x86_64": 387, "manylinux1_x86_64": 387, "435": 387, "bless": 387, "cwcwidth": 387, "92": 387, "pyrsist": 387, "121": 387, "graphviz": 387, "networkx": [387, 389], "pypars": 387, "103": 387, "kiwisolv": 387, "contourpi": 387, "301": 387, "fonttool": 387, "dateutil": 387, "python_dateutil": 387, "247": [387, 388], "cycler": 387, "pytz": 387, "2020": 387, "505": 387, "extens": [387, 389], "typing_extens": 387, "html": 387, "compon": 387, "dash_html_compon": 387, "dash_tabl": 387, "flask": 387, "101": 387, "dash_core_compon": 387, "pygment": 387, "markdown": 387, "markdown_it_pi": 387, "84": 387, "tenac": 387, "24": [387, 389], "pickleshar": 387, "prompt": 387, "toolkit": 387, "37": 387, "prompt_toolkit": 387, "43": 387, "386": 387, "stack": 387, "stack_data": 387, "backcal": 387, "jedi": 387, "pexpect": 387, "sys_platform": 387, "win32": 387, "inlin": 387, "matplotlib_inlin": 387, "send2trash": 387, "anyio": 387, "termin": 387, "jupyter_server_termin": 387, "client": 387, "jupyter_cli": 387, "105": 387, "nbformat": 387, "77": 387, "nbconvert": 387, "257": [387, 388], "event": 387, "jupyter_ev": 387, "websocket": 387, "websocket_cli": 387, "pyzmq": 387, "prometheu": 387, "prometheus_cli": 387, "argon2": 387, "cffi": 387, "argon2_cffi": 387, "terminado": 387, "markupsaf": [387, 389], "26": 387, "nest": 387, "asyncio": 387, "nest_asyncio": 387, "psutil": 387, "cp36": 387, "288": 387, "debugpi": 387, "babel": 387, "62": 387, "jsonschema": 387, "json5": 387, "zipp": [387, 389], "platformdir": 387, "six": 387, "wcwidth": 387, "itsdanger": 387, "blinker": 387, "werkzeug": 387, "226": 387, "mdurl": 387, "pure": 387, "pure_ev": 387, "asttoken": 387, "27": [387, 389], "parso": 387, "ptyprocess": 387, "exceptiongroup": 387, "idna": [387, 389], "61": 387, "sniffio": 387, "fastjsonschema": 387, "defusedxml": 387, "beautifulsoup4": 387, "jupyterlab_pyg": 387, "pandocfilt": 387, "mistun": 387, "tinycss2": 387, "bleach": 387, "162": 387, "nbclient": 387, "rfc3986": 387, "rfc3986_valid": 387, "json": [387, 390], "logger": 387, "python_json_logg": 387, "rfc3339": 387, "rfc3339_valid": 387, "argon2_cffi_bind": 387, "86": 387, "urllib3": [387, 389], "charset": [387, 389], "charset_norm": 387, "141": 387, "certifi": [387, 389], "163": 387, "03": [387, 388], "jsonschema_specif": 387, "pkgutil": 387, "resolv": 387, "pkgutil_resolve_nam": 387, "attr": 387, "rpd": 387, "rpds_py": 387, "soupsiev": 387, "36": 387, "webencod": 387, "444": 387, "pycpars": 387, "118": 387, "pre_commit": 387, "202": 387, "black": 387, "twine": 387, "yamllint": 387, "docutil": 387, "570": 387, "sphinx": 387, "rtd": 387, "theme": 387, "sphinx_rtd_them": 387, "sphinxcontrib": 387, "email": 387, "sphinxcontrib_email": 387, "lxml": 387, "manylinux_2_24_x86_64": 387, "breath": 387, "35": 387, "nbsphinx": 387, "jqueri": 387, "sphinxcontrib_jqueri": 387, "3a": 387, "a8": 387, "3237a93e3a6261bd24edabf3277ca59f64c1710b3d8c7c72a0": 387, "317": 387, "timeout": 387, "pytest_timeout": 387, "6c": 387, "5706d21e6b4dff52e7af12bff9ca126a3f15beb4dff386aa29": 387, "jsbeautifi": 387, "462": 387, "xlsxwriter": 387, "152": 387, "tiktoken": 387, "tqdm": [387, 389], "sentencepiec": 387, "numba": 387, "56": [387, 388], "librosa": 387, "252": [387, 388], "timm": [387, 389], "549": 387, "opencv": 387, "headless": 387, "74": 387, "opencv_python_headless": 387, "cp37": 387, "diffus": [387, 389], "604": 387, "219": 387, "ftfy": 387, "gitpython": 387, "188": 387, "einop": 387, "multiprocess": 387, "70": 387, "py38": 387, "132": 387, "81": 387, "bert_scor": 387, "fsspec": [387, 389], "173": 387, "nodeenv": 387, "cfgv": 387, "98": 387, "virtualenv": 387, "pathspec": 387, "mypi": 387, "mypy_extens": 387, "pyproject_hook": 387, "render": 387, "readme_render": 387, "pkginfo": 387, "toolbelt": 387, "requests_toolbelt": 387, "keyr": 387, "images": 387, "serializinghtml": 387, "sphinxcontrib_serializinghtml": 387, "94": 387, "jsmath": 387, "sphinxcontrib_jsmath": 387, "snowballstemm": 387, "93": [387, 388], "htmlhelp": 387, "sphinxcontrib_htmlhelp": 387, "99": 387, "alabast": 387, "applehelp": 387, "sphinxcontrib_applehelp": 387, "devhelp": 387, "sphinxcontrib_devhelp": 387, "qthelp": 387, "sphinxcontrib_qthelp": 387, "ply": 387, "plumbum": 387, "iniconfig": 387, "pluggi": 387, "0rc8": 387, "editorconfig": 387, "respons": 387, "pyarrow": 387, "xxhash": 387, "194": 387, "huggingfac": [387, 389], "hub": [387, 389], "330": 387, "aiohttp": 387, "dill": 387, "110": 387, "regex": [387, 389], "2019": [387, 389], "777": 387, "filelock": [387, 389], "llvmlite": 387, "0dev0": 387, "soxr": 387, "soundfil": 387, "pooch": 387, "lazi": 387, "loader": 387, "lazy_load": 387, "scipi": 387, "joblib": 387, "302": 387, "audioread": 387, "scikit": 387, "scikit_learn": 387, "msgpack": 387, "534": 387, "gitdb": 387, "distlib": 387, "468": 387, "nh3": 387, "secretstorag": 387, "jeepnei": 387, "jaraco": 387, "frozenlist": 387, "240": [387, 388], "async_timeout": 387, "aiosign": 387, "yarl": 387, "308": 387, "multidict": 387, "129": 387, "threadpoolctl": 387, "smmap": 387, "cryptographi": 387, "itertool": 387, "more_itertool": 387, "57": 387, "pyproject": 387, "uninstal": 387, "msg": 387, "t5": 387, "integration_test": 387, "test_perform": 387, "test_t5_for_conditional_gener": 387, "functional_t5": 387, "ttnn_functional_t5": 387, "09": [387, 388], "ops_devic": 387, "session": 387, "cachedir": 387, "pytest_cach": 387, "rootdir": 387, "configfil": 387, "ini": 387, "plugin": 387, "600": 387, "func_onli": 387, "670": 387, "681": 387, "08": 387, "684": [387, 388], "1202": 387, "llruntim": 387, "watcher": 387, "watch": 387, "109": 387, "465": 387, "ttnn_t5": 387, "6ba823894": 387, "149": 387, "484": 387, "487": 387, "216": 387, "489": 387, "721": 387, "359902381896973": 387, "07123565673828": 387, "722": 387, "102": 387, "44269247283137575": 387, "detach": 387, "summari": 387, "627": 387, "638": 387, "639": 387, "458": 387, "224": 387, "460": 387, "292": 387, "164": 387, "22393798828125": 387, "165": 387, "322504758834839": 387, "407821983919596": 387, "pd": 387, "glob": 387, "getenv": 387, "get_latest_report": 387, "base_path": 387, "latest_dir": 387, "listdir": 387, "isdir": 387, "getmtim": 387, "valueerror": [387, 388], "latest_profile_report": 387, "df": 387, "read_csv": 387, "2024_02_09_01_38_37": 387, "ops_perf_results_resnet_2024_02_09_01_38_37": 387, "output_0_w": 387, "output_0_z": 387, "output_0_i": 387, "output_0_x": 387, "output_0_layout": 387, "output_0_data": 387, "output_0_memori": 387, "depth": 387, "compileprogram": 387, "load_tensor_ttnn": 387, "137428381893955": 387, "137428382188762": 387, "294807": 387, "137428382500949": 387, "137428399402163": 387, "16901214": 387, "137428399802068": 387, "137428399873758": 387, "71690": 387, "137428400102635": 387, "137428400351033": 387, "248398": 387, "137428400548071": 387, "137428400792528": 387, "244457": 387, "4391": 387, "reshape_ttnn": 387, "4392": 387, "137450414555424": 387, "137450414599894": 387, "44470": 387, "4393": 387, "137450414740752": 387, "137450414782422": 387, "41670": 387, "4394": 387, "bcast_batch": 387, "tt_me": 387, "108": 387, "matmulparallelizationstrategi": 387, "multi_cor": 387, "137450414881851": 387, "137450414983440": 387, "101589": 387, "32128": 387, "dev_0_dram_interleav": 387, "4395": 387, "137450415113099": 387, "137450415158748": 387, "45649": 387, "from_device_ttnn": 387, "4396": 387, "137450415235897": 387, "137450453493048": 387, "38257151": 387, "fold_batch_norm2d_into_conv2d": 388, "168": 388, "82": 388, "768": 388, "242": 388, "conv1d": 388, "246": 388, "248": 388, "249": 388, "250": [388, 389], "251": 388, "253": 388, "254": 388, "255": 388, "258": 388, "262": 388, "avg_pool2d": 388, "266": 388, "268": 388, "269": 388, "device_param": 388, "24576": 388, "310": 388, "324": 388, "325": 388, "363": 388, "conv3x3": 388, "in_plan": 388, "out_plan": 388, "3x3": 388, "torchbasicblock": 388, "expans": 388, "inplan": 388, "base_width": 388, "norm_lay": 388, "basicblock": 388, "notimplementederror": 388, "conv1": 388, "bn1": 388, "conv2": 388, "bn2": 388, "torch_model": 388, "state_dict": [388, 389], "create_custom_preprocessor": 388, "conv_weight_1": 388, "conv_bias_1": 388, "conv_weight_2": 388, "conv_bias_2": 388, "682": 388, "683": 388, "499": 388, "_initialize_model_and_preprocess_paramet": 388, "449": 388, "717": 388, "718": 388, "model_resnet_block_graph": 388, "conv_param": 388, "act_block_h": 388, "reshard": 388, "height_shard": 388, "shard_layout": 388, "__call__": 388, "conv_config": 388, "conv2dconfig": 388, "weights_dtyp": 388, "fp32_dest_acc_en": 388, "packer_l1_accum_en": 388, "input_channels_align": 388, "deallocate_activ": 388, "act_block_h_overrid": 388, "_out_height": 388, "_out_width": 388, "weight_tensor": 388, "bias_tensor": 388, "input_height": 388, "input_width": 388, "return_output_s": 388, "return_prepared_device_weight": 388, "ttnnbasicblock": 388, "get_memory_config": 388, "ttnn_model": 388, "12638": 388, "walk": 389, "mirror": 389, "colab": 389, "research": 389, "run_dit": 389, "ipynb": 389, "tab": 389, "ov": 389, "assumpt": 389, "chdir": 389, "content": 389, "upgrad": 389, "save_imag": 389, "create_diffus": 389, "autoencoderkl": 389, "find_model": 389, "collis": 389, "pil": 389, "set_grad_en": 389, "cuda": 389, "is_avail": 389, "gpu": 389, "322": 389, "fatal": 389, "destin": 389, "date": 389, "safetensor": 389, "sympi": 389, "mpmath": 389, "image_s": 389, "512": 389, "vae_model": 389, "stabilityai": 389, "sd": 389, "vae": 389, "ft": 389, "ema": 389, "mse": 389, "latent_s": 389, "input_s": 389, "pt": 389, "load_state_dict": 389, "num_sampling_step": 389, "slider": 389, "cfg_scale": 389, "class_label": 389, "207": 389, "387": 389, "974": 389, "88": 389, "979": 389, "417": 389, "279": 389, "samples_per_row": 389, "nois": 389, "len": 389, "classifi": 389, "free": 389, "guidanc": 389, "y_null": 389, "model_kwarg": 389, "p_sample_loop": 389, "forward_with_cfg": 389, "clip_denois": 389, "null": 389, "18215": 389, "dit_model_graph": 389, "png": 389, "nrow": 389, "value_rang": 389, "06": 389, "987": 389, "210": 389, "show_svg": 389, "snippet": 390, "matmul_output_tensor": 390, "torch_matmul_output_tensor": 390, "unlik": 390, "start_tim": 390, "end_tim": 390, "stdout": 390, "6391518115997314": 390, "0007393360137939453": 390, "9998": 390, "too": 390, "exp_trac": 390, "substitut": 390, "disk": 390, "implementaiton": 390, "addition": 390, "ttnn_config_path": 390, "2048": 390, "app": 390, "pre_hook_to_print_args_and_kwarg": 390, "post_hook_to_print_output": 390, "query_registered_oper": 390, "begin_graph_captur": 390, "runmod": 390, "no_dispatch": 390, "captured_graph": 390, "end_graph_captur": 390, "pretty_print": 390}, "objects": {"tt_lib.fallback_ops": [[369, 0, 1, "", "AdaptiveAvgPool2d"], [369, 0, 1, "", "BatchNorm2d"], [369, 0, 1, "", "Conv2d"], [369, 0, 1, "", "GroupNorm"], [369, 0, 1, "", "LayerNorm"], [369, 0, 1, "", "MaxPool2d"], [369, 0, 1, "", "binary_bitwise_and"], [369, 0, 1, "", "binary_bitwise_left_shift"], [369, 0, 1, "", "binary_bitwise_or"], [369, 0, 1, "", "binary_bitwise_right_shift"], [369, 0, 1, "", "binary_bitwise_xor"], [369, 0, 1, "", "binary_fmod"], [369, 0, 1, "", "bitwise_not"], [369, 0, 1, "", "ceil"], [369, 1, 1, "", "chunk"], [369, 1, 1, "", "concat"], [369, 1, 1, "", "conv2d"], [369, 0, 1, "", "floor"], [369, 1, 1, "", "full"], [369, 1, 1, "", "group_norm"], [369, 1, 1, "", "interpolate"], [369, 1, 1, "", "layer_norm"], [369, 1, 1, "", "pad"], [369, 1, 1, "", "repeat"], [369, 1, 1, "", "repeat_interleave"], [369, 1, 1, "", "reshape"], [369, 1, 1, "", "silu"], [369, 1, 1, "", "softmax"], [369, 1, 1, "", "tensor_slice"], [369, 0, 1, "", "torch_argmax"], [369, 0, 1, "", "torch_argmin"], [369, 0, 1, "", "trunc"], [369, 0, 1, "", "unary_bitwise_and"], [369, 0, 1, "", "unary_bitwise_left_shift"], [369, 0, 1, "", "unary_bitwise_or"], [369, 0, 1, "", "unary_bitwise_right_shift"], [369, 0, 1, "", "unary_bitwise_xor"], [369, 0, 1, "", "unary_fmod"]], "tt_lib.fused_ops.add_and_norm": [[369, 1, 1, "", "AddAndNorm"]], "tt_lib.fused_ops.layernorm": [[369, 1, 1, "", "Layernorm"]], "tt_lib.fused_ops.linear": [[369, 1, 1, "", "Linear"]], "ttnn": [[369, 0, 1, "", "BcastOpDim"], [369, 0, 1, "", "BcastOpMath"], [8, 1, 1, "", "GetDefaultDevice"], [368, 0, 1, "", "MemoryConfig"], [9, 1, 1, "", "SetDefaultDevice"], [374, 0, 1, "", "Shape"], [368, 0, 1, "", "Tensor"], [10, 1, 1, "", "abs"], [11, 1, 1, "", "abs_bw"], [12, 1, 1, "", "acos"], [13, 1, 1, "", "acos_bw"], [14, 1, 1, "", "acosh"], [15, 1, 1, "", "acosh_bw"], [16, 1, 1, "", "add"], [17, 1, 1, "", "add_bw"], [18, 1, 1, "", "addalpha"], [19, 1, 1, "", "addalpha_bw"], [20, 1, 1, "", "addcdiv"], [21, 1, 1, "", "addcdiv_bw"], [22, 1, 1, "", "addcmul"], [23, 1, 1, "", "addcmul_bw"], [24, 1, 1, "", "all_gather"], [25, 1, 1, "", "angle"], [26, 1, 1, "", "angle_bw"], [27, 1, 1, "", "arange"], [28, 1, 1, "", "argmax"], [29, 1, 1, "", "as_tensor"], [30, 1, 1, "", "asin"], [31, 1, 1, "", "asin_bw"], [32, 1, 1, "", "asinh"], [33, 1, 1, "", "asinh_bw"], [34, 1, 1, "", "assign_bw"], [35, 1, 1, "", "atan"], [36, 1, 1, "", "atan2"], [37, 1, 1, "", "atan2_bw"], [38, 1, 1, "", "atan_bw"], [39, 1, 1, "", "atanh"], [40, 1, 1, "", "atanh_bw"], [41, 1, 1, "", "batch_norm"], [42, 1, 1, "", "bias_gelu_bw"], [43, 1, 1, "", "bitwise_and"], [44, 1, 1, "", "bitwise_left_shift"], [45, 1, 1, "", "bitwise_not"], [46, 1, 1, "", "bitwise_or"], [47, 1, 1, "", "bitwise_right_shift"], [48, 1, 1, "", "bitwise_xor"], [49, 1, 1, "", "cbrt"], [50, 1, 1, "", "ceil"], [51, 1, 1, "", "ceil_bw"], [52, 1, 1, "", "celu"], [53, 1, 1, "", "celu_bw"], [54, 1, 1, "", "clamp"], [55, 1, 1, "", "clamp_bw"], [56, 1, 1, "", "clip"], [57, 1, 1, "", "clip_bw"], [58, 1, 1, "", "clone"], [59, 1, 1, "", "close_device"], [60, 1, 1, "", "concat"], [61, 1, 1, "", "concat_bw"], [62, 1, 1, "", "conj"], [63, 1, 1, "", "conj_bw"], [64, 1, 1, "", "cos"], [65, 1, 1, "", "cos_bw"], [66, 1, 1, "", "cosh"], [67, 1, 1, "", "cosh_bw"], [68, 1, 1, "", "create_sharded_memory_config"], [69, 1, 1, "", "deallocate"], [70, 1, 1, "", "deg2rad"], [71, 1, 1, "", "deg2rad_bw"], [72, 1, 1, "", "digamma"], [73, 1, 1, "", "digamma_bw"], [74, 1, 1, "", "div"], [75, 1, 1, "", "div_bw"], [76, 1, 1, "", "div_no_nan"], [77, 1, 1, "", "div_no_nan_bw"], [78, 1, 1, "", "downsample"], [79, 1, 1, "", "dump_tensor"], [80, 1, 1, "", "elu"], [81, 1, 1, "", "elu_bw"], [82, 1, 1, "", "embedding"], [83, 1, 1, "", "embedding_bw"], [84, 1, 1, "", "empty"], [85, 1, 1, "", "empty_like"], [86, 1, 1, "", "eq"], [87, 1, 1, "", "eq_"], [88, 1, 1, "", "eqz"], [89, 1, 1, "", "erf"], [90, 1, 1, "", "erf_bw"], [91, 1, 1, "", "erfc"], [92, 1, 1, "", "erfc_bw"], [93, 1, 1, "", "erfinv"], [94, 1, 1, "", "erfinv_bw"], [95, 1, 1, "", "exp"], [96, 1, 1, "", "exp2"], [97, 1, 1, "", "exp2_bw"], [98, 1, 1, "", "exp_bw"], [105, 1, 1, "", "expm1"], [106, 1, 1, "", "expm1_bw"], [107, 1, 1, "", "fill"], [108, 1, 1, "", "fill_bw"], [109, 1, 1, "", "fill_ones_rm"], [110, 1, 1, "", "fill_rm"], [111, 1, 1, "", "fill_zero_bw"], [112, 1, 1, "", "floor"], [113, 1, 1, "", "floor_bw"], [114, 1, 1, "", "floor_div"], [115, 1, 1, "", "fmod"], [116, 1, 1, "", "fmod_bw"], [117, 1, 1, "", "format_input_tensor"], [118, 1, 1, "", "format_output_tensor"], [119, 1, 1, "", "frac"], [120, 1, 1, "", "frac_bw"], [121, 1, 1, "", "from_device"], [122, 1, 1, "", "from_torch"], [123, 1, 1, "", "full"], [124, 1, 1, "", "full_like"], [125, 1, 1, "", "gcd"], [126, 1, 1, "", "ge"], [127, 1, 1, "", "ge_"], [128, 1, 1, "", "geglu"], [129, 1, 1, "", "gelu"], [130, 1, 1, "", "gelu_bw"], [131, 1, 1, "", "gez"], [132, 1, 1, "", "global_avg_pool2d"], [133, 1, 1, "", "glu"], [134, 1, 1, "", "group_norm"], [135, 1, 1, "", "gt"], [136, 1, 1, "", "gt_"], [137, 1, 1, "", "gtz"], [138, 1, 1, "", "hardshrink"], [139, 1, 1, "", "hardshrink_bw"], [140, 1, 1, "", "hardsigmoid"], [141, 1, 1, "", "hardsigmoid_bw"], [142, 1, 1, "", "hardswish"], [143, 1, 1, "", "hardswish_bw"], [144, 1, 1, "", "hardtanh"], [145, 1, 1, "", "hardtanh_bw"], [146, 1, 1, "", "heaviside"], [147, 1, 1, "", "hypot"], [148, 1, 1, "", "hypot_bw"], [149, 1, 1, "", "i0"], [150, 1, 1, "", "i0_bw"], [151, 1, 1, "", "identity"], [152, 1, 1, "", "imag"], [153, 1, 1, "", "imag_bw"], [154, 1, 1, "", "indexed_fill"], [155, 1, 1, "", "is_imag"], [156, 1, 1, "", "is_real"], [157, 1, 1, "", "isclose"], [158, 1, 1, "", "isfinite"], [159, 1, 1, "", "isinf"], [160, 1, 1, "", "isnan"], [161, 1, 1, "", "isneginf"], [162, 1, 1, "", "isposinf"], [165, 1, 1, "", "l1_loss"], [166, 1, 1, "", "layer_norm"], [167, 1, 1, "", "lcm"], [168, 1, 1, "", "ldexp"], [169, 1, 1, "", "ldexp_bw"], [170, 1, 1, "", "le"], [171, 1, 1, "", "le_"], [172, 1, 1, "", "leaky_relu"], [173, 1, 1, "", "leaky_relu_bw"], [174, 1, 1, "", "lerp"], [175, 1, 1, "", "lerp_bw"], [176, 1, 1, "", "lez"], [177, 1, 1, "", "lgamma"], [178, 1, 1, "", "lgamma_bw"], [179, 1, 1, "", "linear"], [180, 1, 1, "", "load_tensor"], [181, 1, 1, "", "log"], [182, 1, 1, "", "log10"], [183, 1, 1, "", "log10_bw"], [184, 1, 1, "", "log1p"], [185, 1, 1, "", "log1p_bw"], [186, 1, 1, "", "log2"], [187, 1, 1, "", "log2_bw"], [188, 1, 1, "", "log_bw"], [189, 1, 1, "", "log_sigmoid"], [190, 1, 1, "", "log_sigmoid_bw"], [191, 1, 1, "", "logaddexp"], [192, 1, 1, "", "logaddexp2"], [193, 1, 1, "", "logaddexp2_bw"], [194, 1, 1, "", "logaddexp_bw"], [195, 1, 1, "", "logical_and"], [196, 1, 1, "", "logical_and_"], [197, 1, 1, "", "logical_not"], [198, 1, 1, "", "logical_not_"], [199, 1, 1, "", "logical_or"], [200, 1, 1, "", "logical_or_"], [201, 1, 1, "", "logical_xor"], [202, 1, 1, "", "logical_xor_"], [203, 1, 1, "", "logit"], [204, 1, 1, "", "logit_bw"], [205, 1, 1, "", "logiteps_bw"], [206, 1, 1, "", "lt"], [207, 1, 1, "", "lt_"], [208, 1, 1, "", "ltz"], [209, 1, 1, "", "mac"], [210, 1, 1, "", "manage_device"], [211, 1, 1, "", "matmul"], [212, 1, 1, "", "max"], [213, 1, 1, "", "max_bw"], [214, 1, 1, "", "max_pool2d"], [215, 1, 1, "", "maximum"], [216, 1, 1, "", "mean"], [217, 1, 1, "", "min"], [218, 1, 1, "", "min_bw"], [219, 1, 1, "", "minimum"], [220, 1, 1, "", "mish"], [223, 1, 1, "", "moreh_sum"], [224, 1, 1, "", "mse_loss"], [225, 1, 1, "", "mul_bw"], [226, 1, 1, "", "multigammaln"], [227, 1, 1, "", "multigammaln_bw"], [228, 1, 1, "", "multiply"], [229, 1, 1, "", "ne"], [230, 1, 1, "", "ne_"], [231, 1, 1, "", "neg"], [232, 1, 1, "", "neg_bw"], [233, 1, 1, "", "nextafter"], [234, 1, 1, "", "nez"], [235, 1, 1, "", "nonzero"], [236, 1, 1, "", "normalize_global"], [237, 1, 1, "", "normalize_hw"], [238, 1, 1, "", "ones"], [239, 1, 1, "", "ones_like"], [240, 1, 1, "", "open_device"], [241, 1, 1, "", "outer"], [242, 1, 1, "", "pad"], [243, 1, 1, "", "pad_to_tile_shape"], [244, 1, 1, "", "permute"], [245, 1, 1, "", "polar"], [246, 1, 1, "", "polar_bw"], [247, 1, 1, "", "polygamma"], [248, 1, 1, "", "polygamma_bw"], [249, 1, 1, "", "polyval"], [250, 1, 1, "", "pow"], [251, 1, 1, "", "pow_bw"], [252, 1, 1, "", "prelu"], [253, 1, 1, "", "prod"], [254, 1, 1, "", "prod_bw"], [255, 1, 1, "", "rad2deg"], [256, 1, 1, "", "rad2deg_bw"], [257, 1, 1, "", "rdiv"], [258, 1, 1, "", "rdiv_bw"], [259, 1, 1, "", "real"], [260, 1, 1, "", "real_bw"], [261, 1, 1, "", "reallocate"], [262, 1, 1, "", "reciprocal"], [263, 1, 1, "", "reciprocal_bw"], [264, 1, 1, "", "reduce_scatter"], [265, 1, 1, "", "register_post_operation_hook"], [266, 1, 1, "", "register_pre_operation_hook"], [267, 1, 1, "", "reglu"], [268, 1, 1, "", "relu"], [269, 1, 1, "", "relu6"], [270, 1, 1, "", "relu6_bw"], [271, 1, 1, "", "relu_bw"], [272, 1, 1, "", "relu_max"], [273, 1, 1, "", "relu_min"], [274, 1, 1, "", "remainder"], [275, 1, 1, "", "remainder_bw"], [276, 1, 1, "", "repeat"], [277, 1, 1, "", "repeat_bw"], [278, 1, 1, "", "repeat_interleave"], [279, 1, 1, "", "reshape"], [280, 1, 1, "", "rms_norm"], [281, 1, 1, "", "round"], [282, 1, 1, "", "round_bw"], [283, 1, 1, "", "rpow"], [284, 1, 1, "", "rpow_bw"], [285, 1, 1, "", "rsqrt"], [286, 1, 1, "", "rsqrt_bw"], [287, 1, 1, "", "rsub"], [288, 1, 1, "", "rsub_bw"], [289, 1, 1, "", "scatter"], [290, 1, 1, "", "selu"], [291, 1, 1, "", "selu_bw"], [292, 1, 1, "", "set_printoptions"], [293, 1, 1, "", "sigmoid"], [294, 1, 1, "", "sigmoid_accurate"], [295, 1, 1, "", "sigmoid_bw"], [296, 1, 1, "", "sign"], [297, 1, 1, "", "sign_bw"], [298, 1, 1, "", "signbit"], [299, 1, 1, "", "silu"], [300, 1, 1, "", "silu_bw"], [301, 1, 1, "", "sin"], [302, 1, 1, "", "sin_bw"], [303, 1, 1, "", "sinh"], [304, 1, 1, "", "sinh_bw"], [305, 1, 1, "", "slice"], [306, 1, 1, "", "softmax"], [307, 1, 1, "", "softplus"], [308, 1, 1, "", "softplus_bw"], [309, 1, 1, "", "softshrink"], [310, 1, 1, "", "softshrink_bw"], [311, 1, 1, "", "softsign"], [312, 1, 1, "", "softsign_bw"], [313, 1, 1, "", "sqrt"], [314, 1, 1, "", "sqrt_bw"], [315, 1, 1, "", "square"], [316, 1, 1, "", "square_bw"], [317, 1, 1, "", "squared_difference"], [318, 1, 1, "", "squared_difference_bw"], [319, 1, 1, "", "std"], [320, 1, 1, "", "sub_bw"], [321, 1, 1, "", "subalpha"], [322, 1, 1, "", "subalpha_bw"], [323, 1, 1, "", "subtract"], [324, 1, 1, "", "sum"], [325, 1, 1, "", "swiglu"], [326, 1, 1, "", "swish"], [327, 1, 1, "", "synchronize_device"], [328, 1, 1, "", "tan"], [329, 1, 1, "", "tan_bw"], [330, 1, 1, "", "tanh"], [331, 1, 1, "", "tanh_bw"], [332, 1, 1, "", "tanhshrink"], [333, 1, 1, "", "tanhshrink_bw"], [334, 1, 1, "", "threshold"], [335, 1, 1, "", "threshold_bw"], [336, 1, 1, "", "tilize"], [337, 1, 1, "", "tilize_with_val_padding"], [338, 1, 1, "", "to_device"], [339, 1, 1, "", "to_layout"], [340, 1, 1, "", "to_memory_config"], [341, 1, 1, "", "to_torch"], [342, 1, 1, "", "topk"], [349, 1, 1, "", "tril"], [350, 1, 1, "", "triu"], [351, 1, 1, "", "trunc"], [352, 1, 1, "", "trunc_bw"], [353, 1, 1, "", "unary_chain"], [354, 1, 1, "", "untilize"], [355, 1, 1, "", "untilize_with_unpadding"], [356, 1, 1, "", "upsample"], [357, 1, 1, "", "var"], [358, 1, 1, "", "where"], [359, 1, 1, "", "where_bw"], [360, 1, 1, "", "xlogy"], [361, 1, 1, "", "xlogy_bw"], [362, 1, 1, "", "zeros"], [363, 1, 1, "", "zeros_like"]], "ttnn.MemoryConfig": [[368, 2, 1, "", "__init__"]], "ttnn.Shape": [[374, 3, 1, "", "rank"], [374, 2, 1, "", "to_rank"]], "ttnn.Tensor": [[368, 2, 1, "", "__init__"], [368, 2, 1, "", "buffer"], [369, 1, 1, "", "cpu"], [368, 2, 1, "", "device"], [368, 2, 1, "", "get_dtype"], [368, 2, 1, "", "get_layout"], [368, 2, 1, "", "pad"], [368, 2, 1, "", "pad_to_tile"], [368, 2, 1, "", "storage_type"], [368, 2, 1, "", "to"], [368, 2, 1, "", "unpad"], [368, 2, 1, "", "unpad_from_tile"]], "ttnn.experimental": [[99, 1, 1, "", "all_reduce"], [100, 1, 1, "", "cumprod"], [101, 1, 1, "", "dropout"], [102, 1, 1, "", "gelu_bw"], [103, 1, 1, "", "rotary_embedding"], [104, 1, 1, "", "sort"]], "ttnn.kv_cache": [[163, 1, 1, "", "fill_cache_for_user_"], [164, 1, 1, "", "update_cache_for_token_"]], "ttnn.model_preprocessing": [[221, 1, 1, "", "preprocess_model"], [222, 1, 1, "", "preprocess_model_parameters"]], "ttnn.operations.moreh": [[369, 1, 1, "", "group_norm"], [369, 1, 1, "", "group_norm_backward"], [369, 1, 1, "", "logsoftmax"], [369, 1, 1, "", "logsoftmax_backward"], [369, 1, 1, "", "mean"], [369, 1, 1, "", "mean_backward"], [369, 1, 1, "", "norm"], [369, 1, 1, "", "norm_backward"], [369, 1, 1, "", "softmax"], [369, 1, 1, "", "softmax_backward"], [369, 1, 1, "", "softmin"], [369, 1, 1, "", "softmin_backward"]], "ttnn.transformer": [[343, 1, 1, "", "attention_softmax"], [344, 1, 1, "", "attention_softmax_"], [345, 1, 1, "", "concatenate_heads"], [346, 1, 1, "", "scaled_dot_product_attention"], [347, 1, 1, "", "scaled_dot_product_attention_decode"], [348, 1, 1, "", "split_query_key_value_and_split_heads"]]}, "objtypes": {"0": "py:class", "1": "py:function", "2": "py:method", "3": "py:property"}, "objnames": {"0": ["py", "class", "Python class"], "1": ["py", "function", "Python function"], "2": ["py", "method", "Python method"], "3": ["py", "property", "Python property"]}, "titleterms": {"welcom": 0, "tt": [0, 5, 6, 364, 366, 368, 369, 370, 371, 373, 390], "nn": [0, 5, 6, 364, 370, 371, 373, 390], "document": 0, "ttnn": [0, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 376, 379, 382, 383, 385, 386, 387, 388], "model": [0, 4, 7, 364, 370, 371, 385, 386, 389], "resourc": 0, "indic": 0, "tabl": 0, "contribut": [1, 371], "develop": 1, "support": [2, 390], "report": [2, 7, 373], "bug": 2, "featur": 2, "propos": 2, "request": 2, "troubleshoot": 2, "debug": [2, 390], "tip": 2, "commun": 2, "perform": [3, 384], "prerequisit": [3, 4, 371], "run": [3, 4, 366, 385, 388, 390], "perf": [3, 373], "file": 3, "all": [3, 371, 390], "get": [4, 370, 388], "start": [4, 370], "an": [4, 390], "exampl": [4, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 105, 106, 107, 108, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 165, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 243, 245, 246, 247, 248, 249, 250, 251, 252, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 279, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 328, 329, 330, 331, 332, 333, 334, 335, 339, 340, 341, 349, 350, 351, 352, 353, 357, 358, 359, 360, 361, 362, 363, 364, 366, 368, 371, 390], "next": 4, "step": [4, 6, 364, 371], "what": [5, 6], "i": [5, 6], "ad": 6, "new": [6, 369, 372], "oper": [6, 7, 364, 369, 373, 377, 381, 383, 386, 387, 390], "faq": 6, "ar": [6, 371], "need": 6, "add": [6, 16, 381, 383], "c": [6, 390], "python": [6, 390], "devic": [6, 7, 366, 369, 371, 383, 384, 385, 390], "implement": [6, 385, 388], "1": [6, 364, 370, 371, 390], "2": [6, 364, 369, 370, 371, 389, 390], "bind": 6, "option": [6, 369, 371], "golden": 6, "function": [6, 372, 386, 390], "api": [7, 368, 369, 374], "memori": [7, 374], "config": [7, 374, 384], "core": 7, "tensor": [7, 366, 368, 369, 374, 381, 383, 384, 390], "creation": 7, "matrix": [7, 384], "multipl": [7, 374, 384], "pointwis": 7, "unari": 7, "binari": 7, "ternari": 7, "loss": 7, "reduct": 7, "data": [7, 374, 383], "movement": 7, "normal": 7, "moreh": 7, "transform": [7, 343, 344, 345, 346, 347, 348], "ccl": 7, "embed": [7, 82], "pool": 7, "vision": 7, "kv": 7, "cach": [7, 369, 384, 385, 390], "convers": 7, "hook": [7, 390], "getdefaultdevic": 8, "setdefaultdevic": 9, "ab": 10, "abs_bw": 11, "aco": 12, "acos_bw": 13, "acosh": 14, "acosh_bw": 15, "add_bw": 17, "addalpha": 18, "addalpha_bw": 19, "addcdiv": 20, "addcdiv_bw": 21, "addcmul": 22, "addcmul_bw": 23, "all_gath": 24, "angl": 25, "angle_bw": 26, "arang": 27, "argmax": 28, "as_tensor": 29, "asin": 30, "asin_bw": 31, "asinh": 32, "asinh_bw": 33, "assign_bw": 34, "atan": 35, "atan2": 36, "atan2_bw": 37, "atan_bw": 38, "atanh": 39, "atanh_bw": 40, "batch_norm": 41, "bias_gelu_bw": 42, "bitwise_and": 43, "bitwise_left_shift": 44, "bitwise_not": 45, "bitwise_or": 46, "bitwise_right_shift": 47, "bitwise_xor": 48, "cbrt": 49, "ceil": 50, "ceil_bw": 51, "celu": 52, "celu_bw": 53, "clamp": 54, "clamp_bw": 55, "clip": 56, "clip_bw": 57, "clone": [58, 371, 389], "close_devic": 59, "concat": 60, "concat_bw": 61, "conj": 62, "conj_bw": 63, "co": 64, "cos_bw": 65, "cosh": 66, "cosh_bw": 67, "create_sharded_memory_config": 68, "dealloc": 69, "deg2rad": 70, "deg2rad_bw": 71, "digamma": 72, "digamma_bw": 73, "div": 74, "div_bw": 75, "div_no_nan": 76, "div_no_nan_bw": 77, "downsampl": 78, "dump_tensor": 79, "elu": 80, "elu_bw": 81, "embedding_bw": 83, "empti": 84, "empty_lik": 85, "eq": 86, "eq_": 87, "eqz": 88, "erf": 89, "erf_bw": 90, "erfc": 91, "erfc_bw": 92, "erfinv": 93, "erfinv_bw": 94, "exp": 95, "exp2": 96, "exp2_bw": 97, "exp_bw": 98, "experiment": [99, 100, 101, 102, 103, 104, 369], "all_reduc": 99, "cumprod": 100, "dropout": 101, "gelu_bw": [102, 130], "rotary_embed": 103, "sort": 104, "expm1": 105, "expm1_bw": 106, "fill": 107, "fill_bw": 108, "fill_ones_rm": 109, "fill_rm": 110, "fill_zero_bw": 111, "floor": 112, "floor_bw": 113, "floor_div": 114, "fmod": 115, "fmod_bw": 116, "format_input_tensor": 117, "format_output_tensor": 118, "frac": 119, "frac_bw": 120, "from_devic": 121, "from_torch": 122, "full": 123, "full_lik": 124, "gcd": 125, "ge": 126, "ge_": 127, "geglu": 128, "gelu": 129, "gez": 131, "global_avg_pool2d": 132, "glu": 133, "group_norm": 134, "gt": 135, "gt_": 136, "gtz": 137, "hardshrink": 138, "hardshrink_bw": 139, "hardsigmoid": 140, "hardsigmoid_bw": 141, "hardswish": 142, "hardswish_bw": 143, "hardtanh": 144, "hardtanh_bw": 145, "heavisid": 146, "hypot": 147, "hypot_bw": 148, "i0": 149, "i0_bw": 150, "ident": 151, "imag": [152, 371], "imag_bw": 153, "indexed_fil": 154, "is_imag": 155, "is_real": 156, "isclos": 157, "isfinit": 158, "isinf": 159, "isnan": 160, "isneginf": 161, "isposinf": 162, "kv_cach": [163, 164], "fill_cache_for_user_": 163, "update_cache_for_token_": 164, "l1_loss": 165, "layer_norm": 166, "lcm": 167, "ldexp": 168, "ldexp_bw": 169, "le": 170, "le_": 171, "leaky_relu": 172, "leaky_relu_bw": 173, "lerp": 174, "lerp_bw": 175, "lez": 176, "lgamma": 177, "lgamma_bw": 178, "linear": 179, "load_tensor": 180, "log": [181, 369, 390], "log10": 182, "log10_bw": 183, "log1p": 184, "log1p_bw": 185, "log2": 186, "log2_bw": 187, "log_bw": 188, "log_sigmoid": 189, "log_sigmoid_bw": 190, "logaddexp": 191, "logaddexp2": 192, "logaddexp2_bw": 193, "logaddexp_bw": 194, "logical_and": 195, "logical_and_": 196, "logical_not": 197, "logical_not_": 198, "logical_or": 199, "logical_or_": 200, "logical_xor": 201, "logical_xor_": 202, "logit": 203, "logit_bw": 204, "logiteps_bw": 205, "lt": 206, "lt_": 207, "ltz": 208, "mac": 209, "manage_devic": 210, "matmul": [211, 377], "max": 212, "max_bw": 213, "max_pool2d": 214, "maximum": 215, "mean": 216, "min": 217, "min_bw": 218, "minimum": 219, "mish": 220, "model_preprocess": [221, 222], "preprocess_model": 221, "preprocess_model_paramet": 222, "moreh_sum": 223, "mse_loss": 224, "mul_bw": 225, "multigammaln": 226, "multigammaln_bw": 227, "multipli": [228, 384], "ne": 229, "ne_": 230, "neg": 231, "neg_bw": 232, "nextaft": 233, "nez": 234, "nonzero": 235, "normalize_glob": 236, "normalize_hw": 237, "ones": 238, "ones_lik": 239, "open_devic": 240, "outer": 241, "pad": 242, "pad_to_tile_shap": 243, "permut": 244, "polar": 245, "polar_bw": 246, "polygamma": 247, "polygamma_bw": 248, "polyv": 249, "pow": 250, "pow_bw": 251, "prelu": 252, "prod": 253, "prod_bw": 254, "rad2deg": 255, "rad2deg_bw": 256, "rdiv": 257, "rdiv_bw": 258, "real": 259, "real_bw": 260, "realloc": 261, "reciproc": 262, "reciprocal_bw": 263, "reduce_scatt": 264, "register_post_operation_hook": 265, "register_pre_operation_hook": 266, "reglu": 267, "relu": 268, "relu6": 269, "relu6_bw": 270, "relu_bw": 271, "relu_max": 272, "relu_min": 273, "remaind": 274, "remainder_bw": 275, "repeat": 276, "repeat_bw": 277, "repeat_interleav": 278, "reshap": 279, "rms_norm": 280, "round": 281, "round_bw": 282, "rpow": 283, "rpow_bw": 284, "rsqrt": 285, "rsqrt_bw": 286, "rsub": 287, "rsub_bw": 288, "scatter": 289, "selu": 290, "selu_bw": 291, "set_printopt": 292, "sigmoid": 293, "sigmoid_accur": 294, "sigmoid_bw": 295, "sign": 296, "sign_bw": 297, "signbit": 298, "silu": 299, "silu_bw": 300, "sin": 301, "sin_bw": 302, "sinh": 303, "sinh_bw": 304, "slice": [305, 390], "softmax": 306, "softplu": 307, "softplus_bw": 308, "softshrink": 309, "softshrink_bw": 310, "softsign": 311, "softsign_bw": 312, "sqrt": 313, "sqrt_bw": 314, "squar": 315, "square_bw": 316, "squared_differ": 317, "squared_difference_bw": 318, "std": 319, "sub_bw": 320, "subalpha": 321, "subalpha_bw": 322, "subtract": 323, "sum": 324, "swiglu": 325, "swish": 326, "synchronize_devic": 327, "tan": 328, "tan_bw": 329, "tanh": 330, "tanh_bw": 331, "tanhshrink": 332, "tanhshrink_bw": 333, "threshold": 334, "threshold_bw": 335, "tiliz": 336, "tilize_with_val_pad": 337, "to_devic": 338, "to_layout": 339, "to_memory_config": 340, "to_torch": 341, "topk": 342, "attention_softmax": 343, "attention_softmax_": 344, "concatenate_head": 345, "scaled_dot_product_attent": 346, "scaled_dot_product_attention_decod": 347, "split_query_key_value_and_split_head": 348, "tril": 349, "triu": 350, "trunc": 351, "trunc_bw": 352, "unary_chain": 353, "until": 354, "untilize_with_unpad": 355, "upsampl": 356, "var": 357, "where": [358, 370], "where_bw": 359, "xlogi": 360, "xlogy_bw": 361, "zero": 362, "zeros_lik": 363, "convert": [364, 368, 383, 385, 390], "pytorch": [364, 366, 368, 389], "rewrit": 364, "switch": 364, "3": [364, 370, 371, 390], "optim": [364, 370, 385], "more": [364, 384], "build": [365, 370, 371, 389], "uplift": 365, "demo": [365, 370], "lib": [366, 369], "us": [366, 383, 384, 385, 386, 390], "one": 366, "op": 366, "from": [366, 369, 370, 371, 388, 389, 390], "acceler": 366, "odd": 366, "size": 366, "last": 366, "dim": 366, "depend": [367, 371], "overview": [368, 369], "storag": [368, 374, 383], "memoryconfig": 368, "between": 368, "infrastructur": 369, "member": 369, "input": 369, "output": [369, 383, 384, 385], "profil": [369, 373, 379, 387], "fast": 369, "dispatch": 369, "program": [369, 371, 384, 385, 390], "through": 369, "tt_lib": [369, 390], "primari": 369, "enum": 369, "fallback": 369, "fuse": 369, "mini": 369, "graph": [369, 376, 388, 389, 390], "librari": [369, 389], "complex": 369, "type": [369, 374, 383], "instal": [370, 371], "explor": 370, "our": [370, 371], "tutori": [370, 375], "multi": [370, 371, 378, 385], "head": [370, 378, 385], "attent": [370, 378, 385], "simpl": 370, "4": [370, 390], "To": [370, 371], "go": 370, "here": 370, "set": 371, "up": 371, "hardwar": 371, "driver": 371, "firmwar": 371, "system": 371, "level": 371, "kmd": 371, "updat": 371, "flash": 371, "manag": 371, "interfac": 371, "smi": 371, "card": 371, "configur": [371, 384, 385], "topologi": 371, "metalium": 371, "There": 371, "three": 371, "sourc": 371, "repositori": 371, "invok": 371, "script": 371, "docker": 371, "releas": 371, "wheel": 371, "download": [371, 389], "latest": 371, "For": 371, "user": 371, "onli": 371, "environ": 371, "you": 371, "verifi": 371, "your": 371, "try": 371, "execut": 371, "interest": 371, "onboard": 372, "header": 373, "profile_thi": 373, "descript": 373, "shape": 374, "layout": [374, 383, 384], "requir": 374, "width": 374, "limit": 374, "bfloat8_b": 374, "shard": 374, "torch": [376, 383, 384, 385, 386, 388, 390], "dit_xl_2": 376, "With": 376, "resnet": [380, 388], "basic": [380, 390], "block": [380, 388], "tracer": 382, "creat": [383, 388], "host": 383, "borrow": 383, "v": 383, "own": 383, "open": 383, "initi": [383, 384, 385], "b": [383, 384], "random": [383, 384], "valu": [383, 384], "inspect": [383, 384], "attribut": 383, "close": [383, 384, 385], "enabl": [384, 385, 390], "result": 384, "write": 385, "activ": 385, "weight": 385, "first": 385, "iter": 385, "subsequ": 385, "version": [385, 388], "pre": [385, 389, 390], "process": 385, "paramet": [385, 388], "check": 385, "match": 385, "origin": 385, "trace": [386, 388, 390], "modul": [386, 388], "written": 386, "torchvis": 388, "preprocess": 388, "displai": [388, 389], "pass": 388, "constructor": 388, "base": 389, "http": 389, "github": 389, "com": 389, "facebookresearch": 389, "dit": 389, "git": 389, "xl": 389, "sampl": 389, "train": 389, "__getitem__": 390, "5": 390, "intermedi": 390, "6": 390, "7": 390, "8": 390, "9": 390, "10": 390, "chang": 390, "string": 390, "represent": 390, "11": 390, "visual": 390, "web": 390, "browser": 390, "12": 390, "regist": 390, "post": 390, "13": 390, "queri": 390, "14": 390, "fall": 390, "back": 390, "15": 390, "captur": 390, "buffer": 390, "alloc": 390, "etc": 390}, "envversion": {"sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "nbsphinx": 4, "sphinx": 58}, "alltitles": {"Welcome to TT-NN documentation!": [[0, "welcome-to-tt-nn-documentation"]], "TTNN": [[0, null]], "Models": [[0, null]], "Resources": [[0, null]], "Indices and tables": [[0, "indices-and-tables"]], "Contributing as a developer": [[1, "contributing-as-a-developer"]], "Support": [[2, "support"]], "Reporting bugs, feature proposals, or support requests": [[2, "reporting-bugs-feature-proposals-or-support-requests"]], "Troubleshooting and debugging tips": [[2, "troubleshooting-and-debugging-tips"]], "Community": [[2, "community"]], "Performance": [[3, "performance"]], "Prerequisites": [[3, "prerequisites"], [4, "prerequisites"]], "Running a perf file": [[3, "running-a-perf-file"]], "Running all the perf files": [[3, "running-all-the-perf-files"]], "Getting Started": [[4, "getting-started"], [370, "getting-started"]], "Running an example model": [[4, "running-an-example-model"]], "Next steps": [[4, "next-steps"]], "What is TT-NN?": [[5, "what-is-tt-nn"]], "Adding New TT-NN Operation": [[6, "adding-new-tt-nn-operation"]], "FAQ": [[6, "faq"]], "What is a TT-NN operation?": [[6, "what-is-a-tt-nn-operation"]], "What steps are needed to add TT-NN operation in C++?": [[6, "what-steps-are-needed-to-add-tt-nn-operation-in-c"]], "What steps are needed to add TT-NN operation in Python?": [[6, "what-steps-are-needed-to-add-tt-nn-operation-in-python"]], "Example of Adding a new Device Operation": [[6, "example-of-adding-a-new-device-operation"]], "C++ Implementation": [[6, "c-implementation"]], "Step 1: Implement device operation": [[6, "step-1-implement-device-operation"]], "Step 2: Implement the operation in C++": [[6, "step-2-implement-the-operation-in-c"]], "Python Implementation": [[6, "python-implementation"]], "Step 1: Add Python binding": [[6, "step-1-add-python-binding"]], "Step 2: (Optional) Add golden function for the operation in Python": [[6, "step-2-optional-add-golden-function-for-the-operation-in-python"]], "APIs": [[7, "apis"], [374, "apis"]], "Device": [[7, "device"]], "Memory Config": [[7, "memory-config"], [374, "memory-config"]], "Operations": [[7, "operations"]], "Core": [[7, "core"]], "Tensor Creation": [[7, "tensor-creation"]], "Matrix Multiplication": [[7, "matrix-multiplication"], [384, "Matrix-Multiplication"]], "Pointwise Unary": [[7, "pointwise-unary"]], "Pointwise Binary": [[7, "pointwise-binary"]], "Pointwise Ternary": [[7, "pointwise-ternary"]], "Losses": [[7, "losses"]], "Reduction": [[7, "reduction"]], "Data Movement": [[7, "data-movement"]], "Normalization": [[7, "normalization"]], "Moreh Operations": [[7, "moreh-operations"]], "Transformer": [[7, "transformer"]], "CCL": [[7, "ccl"]], "Embedding": [[7, "embedding"]], "Pooling": [[7, "pooling"]], "Vision": [[7, "vision"]], "KV Cache": [[7, "kv-cache"]], "Model Conversion": [[7, "model-conversion"]], "Reports": [[7, "reports"]], "Operation Hooks": [[7, "operation-hooks"]], "ttnn.GetDefaultDevice": [[8, "ttnn-getdefaultdevice"]], "Example": [[8, null], [9, null], [10, null], [11, null], [12, null], [13, null], [14, null], [15, null], [16, null], [17, null], [18, null], [19, null], [20, null], [21, null], [22, null], [23, null], [24, null], [25, null], [26, null], [27, null], [30, null], [31, null], [32, null], [33, null], [34, null], [35, null], [36, null], [37, null], [38, null], [39, null], [40, null], [41, null], [42, null], [43, null], [44, null], [45, null], [46, null], [47, null], [48, null], [49, null], [50, null], [51, null], [52, null], [53, null], [54, null], [55, null], [56, null], [57, null], [59, null], [60, null], [61, null], [62, null], [63, null], [64, null], [65, null], [66, null], [67, null], [68, null], [69, null], [70, null], [71, null], [72, null], [73, null], [74, null], [75, null], [76, null], [77, null], [79, null], [80, null], [81, null], [82, null], [83, null], [84, null], [85, null], [86, null], [87, null], [88, null], [89, null], [90, null], [91, null], [92, null], [93, null], [94, null], [95, null], [96, null], [97, null], [98, null], [99, null], [100, null], [101, null], [102, null], [105, null], [106, null], [107, null], [108, null], [111, null], [112, null], [113, null], [114, null], [115, null], [116, null], [117, null], [118, null], [119, null], [120, null], [121, null], [122, null], [123, null], [124, null], [125, null], [126, null], [127, null], [128, null], [129, null], [130, null], [131, null], [132, null], [133, null], [135, null], [136, null], [137, null], [138, null], [139, null], [140, null], [141, null], [142, null], [143, null], [144, null], [145, null], [146, null], [147, null], [148, null], [149, null], [150, null], [151, null], [152, null], [153, null], [154, null], [155, null], [156, null], [157, null], [158, null], [159, null], [160, null], [161, null], [162, null], [165, null], [167, null], [168, null], [169, null], [170, null], [171, null], [172, null], [173, null], [174, null], [175, null], [176, null], [177, null], [178, null], [179, null], [180, null], [181, null], [182, null], [183, null], [184, null], [185, null], [186, null], [187, null], [188, null], [189, null], [190, null], [191, null], [192, null], [193, null], [194, null], [195, null], [196, null], [197, null], [198, null], [199, null], [200, null], [201, null], [202, null], [203, null], [204, null], [205, null], [206, null], [207, null], [208, null], [209, null], [210, null], [211, null], [212, null], [213, null], [214, null], [215, null], [216, null], [217, null], [218, null], [219, null], [220, null], [224, null], [225, null], [226, null], [227, null], [228, null], [229, null], [230, null], [231, null], [232, null], [233, null], [234, null], [235, null], [236, null], [237, null], [238, null], [239, null], [240, null], [241, null], [243, null], [245, null], [246, null], [247, null], [248, null], [249, null], [250, null], [251, null], [252, null], [254, null], [255, null], [256, null], [257, null], [258, null], [259, null], [260, null], [261, null], [262, null], [263, null], [264, null], [267, null], [268, null], [269, null], [270, null], [271, null], [272, null], [273, null], [274, null], [275, null], [276, null], [277, null], [279, null], [281, null], [282, null], [283, null], [284, null], [285, null], [286, null], [287, null], [288, null], [289, null], [290, null], [291, null], [293, null], [294, null], [295, null], [296, null], [297, null], [298, null], [299, null], [300, null], [301, null], [302, null], [303, null], [304, null], [305, null], [306, null], [307, null], [308, null], [309, null], [310, null], [311, null], [312, null], [313, null], [314, null], [315, null], [316, null], [317, null], [318, null], [319, null], [320, null], [321, null], [322, null], [323, null], [324, null], [325, null], [326, null], [328, null], [329, null], [330, null], [331, null], [332, null], [333, null], [334, null], [335, null], [339, null], [340, null], [341, null], [349, null], [350, null], [351, null], [352, null], [353, null], [357, null], [358, null], [359, null], [360, null], [361, null], [362, null], [363, null]], "ttnn.SetDefaultDevice": [[9, "ttnn-setdefaultdevice"]], "ttnn.abs": [[10, "ttnn-abs"]], "ttnn.abs_bw": [[11, "ttnn-abs-bw"]], "ttnn.acos": [[12, "ttnn-acos"]], "ttnn.acos_bw": [[13, "ttnn-acos-bw"]], "ttnn.acosh": [[14, "ttnn-acosh"]], "ttnn.acosh_bw": [[15, "ttnn-acosh-bw"]], "ttnn.add": [[16, "ttnn-add"]], "ttnn.add_bw": [[17, "ttnn-add-bw"]], "ttnn.addalpha": [[18, "ttnn-addalpha"]], "ttnn.addalpha_bw": [[19, "ttnn-addalpha-bw"]], "ttnn.addcdiv": [[20, "ttnn-addcdiv"]], "ttnn.addcdiv_bw": [[21, "ttnn-addcdiv-bw"]], "ttnn.addcmul": [[22, "ttnn-addcmul"]], "ttnn.addcmul_bw": [[23, "ttnn-addcmul-bw"]], "ttnn.all_gather": [[24, "ttnn-all-gather"]], "ttnn.angle": [[25, "ttnn-angle"]], "ttnn.angle_bw": [[26, "ttnn-angle-bw"]], "ttnn.arange": [[27, "ttnn-arange"]], "ttnn.argmax": [[28, "ttnn-argmax"]], "ttnn.as_tensor": [[29, "ttnn-as-tensor"]], "Examples": [[29, null], [292, null]], "ttnn.asin": [[30, "ttnn-asin"]], "ttnn.asin_bw": [[31, "ttnn-asin-bw"]], "ttnn.asinh": [[32, "ttnn-asinh"]], "ttnn.asinh_bw": [[33, "ttnn-asinh-bw"]], "ttnn.assign_bw": [[34, "ttnn-assign-bw"]], "ttnn.atan": [[35, "ttnn-atan"]], "ttnn.atan2": [[36, "ttnn-atan2"]], "ttnn.atan2_bw": [[37, "ttnn-atan2-bw"]], "ttnn.atan_bw": [[38, "ttnn-atan-bw"]], "ttnn.atanh": [[39, "ttnn-atanh"]], "ttnn.atanh_bw": [[40, "ttnn-atanh-bw"]], "ttnn.batch_norm": [[41, "ttnn-batch-norm"]], "ttnn.bias_gelu_bw": [[42, "ttnn-bias-gelu-bw"]], "ttnn.bitwise_and": [[43, "ttnn-bitwise-and"]], "ttnn.bitwise_left_shift": [[44, "ttnn-bitwise-left-shift"]], "ttnn.bitwise_not": [[45, "ttnn-bitwise-not"]], "ttnn.bitwise_or": [[46, "ttnn-bitwise-or"]], "ttnn.bitwise_right_shift": [[47, "ttnn-bitwise-right-shift"]], "ttnn.bitwise_xor": [[48, "ttnn-bitwise-xor"]], "ttnn.cbrt": [[49, "ttnn-cbrt"]], "ttnn.ceil": [[50, "ttnn-ceil"]], "ttnn.ceil_bw": [[51, "ttnn-ceil-bw"]], "ttnn.celu": [[52, "ttnn-celu"]], "ttnn.celu_bw": [[53, "ttnn-celu-bw"]], "ttnn.clamp": [[54, "ttnn-clamp"]], "ttnn.clamp_bw": [[55, "ttnn-clamp-bw"]], "ttnn.clip": [[56, "ttnn-clip"]], "ttnn.clip_bw": [[57, "ttnn-clip-bw"]], "ttnn.clone": [[58, "ttnn-clone"]], "ttnn.close_device": [[59, "ttnn-close-device"]], "ttnn.concat": [[60, "ttnn-concat"]], "ttnn.concat_bw": [[61, "ttnn-concat-bw"]], "ttnn.conj": [[62, "ttnn-conj"]], "ttnn.conj_bw": [[63, "ttnn-conj-bw"]], "ttnn.cos": [[64, "ttnn-cos"]], "ttnn.cos_bw": [[65, "ttnn-cos-bw"]], "ttnn.cosh": [[66, "ttnn-cosh"]], "ttnn.cosh_bw": [[67, "ttnn-cosh-bw"]], "ttnn.create_sharded_memory_config": [[68, "ttnn-create-sharded-memory-config"]], "ttnn.deallocate": [[69, "ttnn-deallocate"]], "ttnn.deg2rad": [[70, "ttnn-deg2rad"]], "ttnn.deg2rad_bw": [[71, "ttnn-deg2rad-bw"]], "ttnn.digamma": [[72, "ttnn-digamma"]], "ttnn.digamma_bw": [[73, "ttnn-digamma-bw"]], "ttnn.div": [[74, "ttnn-div"]], "ttnn.div_bw": [[75, "ttnn-div-bw"]], "ttnn.div_no_nan": [[76, "ttnn-div-no-nan"]], "ttnn.div_no_nan_bw": [[77, "ttnn-div-no-nan-bw"]], "ttnn.downsample": [[78, "ttnn-downsample"]], "ttnn.dump_tensor": [[79, "ttnn-dump-tensor"]], "ttnn.elu": [[80, "ttnn-elu"]], "ttnn.elu_bw": [[81, "ttnn-elu-bw"]], "ttnn.embedding": [[82, "ttnn-embedding"]], "ttnn.embedding_bw": [[83, "ttnn-embedding-bw"]], "ttnn.empty": [[84, "ttnn-empty"]], "ttnn.empty_like": [[85, "ttnn-empty-like"]], "ttnn.eq": [[86, "ttnn-eq"]], "ttnn.eq_": [[87, "ttnn-eq"]], "ttnn.eqz": [[88, "ttnn-eqz"]], "ttnn.erf": [[89, "ttnn-erf"]], "ttnn.erf_bw": [[90, "ttnn-erf-bw"]], "ttnn.erfc": [[91, "ttnn-erfc"]], "ttnn.erfc_bw": [[92, "ttnn-erfc-bw"]], "ttnn.erfinv": [[93, "ttnn-erfinv"]], "ttnn.erfinv_bw": [[94, "ttnn-erfinv-bw"]], "ttnn.exp": [[95, "ttnn-exp"]], "ttnn.exp2": [[96, "ttnn-exp2"]], "ttnn.exp2_bw": [[97, "ttnn-exp2-bw"]], "ttnn.exp_bw": [[98, "ttnn-exp-bw"]], "ttnn.experimental.all_reduce": [[99, "ttnn-experimental-all-reduce"]], "ttnn.experimental.cumprod": [[100, "ttnn-experimental-cumprod"]], "ttnn.experimental.dropout": [[101, "ttnn-experimental-dropout"]], "ttnn.experimental.gelu_bw": [[102, "ttnn-experimental-gelu-bw"]], "ttnn.experimental.rotary_embedding": [[103, "ttnn-experimental-rotary-embedding"]], "ttnn.experimental.sort": [[104, "ttnn-experimental-sort"]], "ttnn.expm1": [[105, "ttnn-expm1"]], "ttnn.expm1_bw": [[106, "ttnn-expm1-bw"]], "ttnn.fill": [[107, "ttnn-fill"]], "ttnn.fill_bw": [[108, "ttnn-fill-bw"]], "ttnn.fill_ones_rm": [[109, "ttnn-fill-ones-rm"]], "ttnn.fill_rm": [[110, "ttnn-fill-rm"]], "ttnn.fill_zero_bw": [[111, "ttnn-fill-zero-bw"]], "ttnn.floor": [[112, "ttnn-floor"]], "ttnn.floor_bw": [[113, "ttnn-floor-bw"]], "ttnn.floor_div": [[114, "ttnn-floor-div"]], "ttnn.fmod": [[115, "ttnn-fmod"]], "ttnn.fmod_bw": [[116, "ttnn-fmod-bw"]], "ttnn.format_input_tensor": [[117, "ttnn-format-input-tensor"]], "ttnn.format_output_tensor": [[118, "ttnn-format-output-tensor"]], "ttnn.frac": [[119, "ttnn-frac"]], "ttnn.frac_bw": [[120, "ttnn-frac-bw"]], "ttnn.from_device": [[121, "ttnn-from-device"]], "ttnn.from_torch": [[122, "ttnn-from-torch"]], "ttnn.full": [[123, "ttnn-full"]], "ttnn.full_like": [[124, "ttnn-full-like"]], "ttnn.gcd": [[125, "ttnn-gcd"]], "ttnn.ge": [[126, "ttnn-ge"]], "ttnn.ge_": [[127, "ttnn-ge"]], "ttnn.geglu": [[128, "ttnn-geglu"]], "ttnn.gelu": [[129, "ttnn-gelu"]], "ttnn.gelu_bw": [[130, "ttnn-gelu-bw"]], "ttnn.gez": [[131, "ttnn-gez"]], "ttnn.global_avg_pool2d": [[132, "ttnn-global-avg-pool2d"]], "ttnn.glu": [[133, "ttnn-glu"]], "ttnn.group_norm": [[134, "ttnn-group-norm"]], "ttnn.gt": [[135, "ttnn-gt"]], "ttnn.gt_": [[136, "ttnn-gt"]], "ttnn.gtz": [[137, "ttnn-gtz"]], "ttnn.hardshrink": [[138, "ttnn-hardshrink"]], "ttnn.hardshrink_bw": [[139, "ttnn-hardshrink-bw"]], "ttnn.hardsigmoid": [[140, "ttnn-hardsigmoid"]], "ttnn.hardsigmoid_bw": [[141, "ttnn-hardsigmoid-bw"]], "ttnn.hardswish": [[142, "ttnn-hardswish"]], "ttnn.hardswish_bw": [[143, "ttnn-hardswish-bw"]], "ttnn.hardtanh": [[144, "ttnn-hardtanh"]], "ttnn.hardtanh_bw": [[145, "ttnn-hardtanh-bw"]], "ttnn.heaviside": [[146, "ttnn-heaviside"]], "ttnn.hypot": [[147, "ttnn-hypot"]], "ttnn.hypot_bw": [[148, "ttnn-hypot-bw"]], "ttnn.i0": [[149, "ttnn-i0"]], "ttnn.i0_bw": [[150, "ttnn-i0-bw"]], "ttnn.identity": [[151, "ttnn-identity"]], "ttnn.imag": [[152, "ttnn-imag"]], "ttnn.imag_bw": [[153, "ttnn-imag-bw"]], "ttnn.indexed_fill": [[154, "ttnn-indexed-fill"]], "ttnn.is_imag": [[155, "ttnn-is-imag"]], "ttnn.is_real": [[156, "ttnn-is-real"]], "ttnn.isclose": [[157, "ttnn-isclose"]], "ttnn.isfinite": [[158, "ttnn-isfinite"]], "ttnn.isinf": [[159, "ttnn-isinf"]], "ttnn.isnan": [[160, "ttnn-isnan"]], "ttnn.isneginf": [[161, "ttnn-isneginf"]], "ttnn.isposinf": [[162, "ttnn-isposinf"]], "ttnn.kv_cache.fill_cache_for_user_": [[163, "ttnn-kv-cache-fill-cache-for-user"]], "ttnn.kv_cache.update_cache_for_token_": [[164, "ttnn-kv-cache-update-cache-for-token"]], "ttnn.l1_loss": [[165, "ttnn-l1-loss"]], "ttnn.layer_norm": [[166, "ttnn-layer-norm"]], "ttnn.lcm": [[167, "ttnn-lcm"]], "ttnn.ldexp": [[168, "ttnn-ldexp"]], "ttnn.ldexp_bw": [[169, "ttnn-ldexp-bw"]], "ttnn.le": [[170, "ttnn-le"]], "ttnn.le_": [[171, "ttnn-le"]], "ttnn.leaky_relu": [[172, "ttnn-leaky-relu"]], "ttnn.leaky_relu_bw": [[173, "ttnn-leaky-relu-bw"]], "ttnn.lerp": [[174, "ttnn-lerp"]], "ttnn.lerp_bw": [[175, "ttnn-lerp-bw"]], "ttnn.lez": [[176, "ttnn-lez"]], "ttnn.lgamma": [[177, "ttnn-lgamma"]], "ttnn.lgamma_bw": [[178, "ttnn-lgamma-bw"]], "ttnn.linear": [[179, "ttnn-linear"]], "ttnn.load_tensor": [[180, "ttnn-load-tensor"]], "ttnn.log": [[181, "ttnn-log"]], "ttnn.log10": [[182, "ttnn-log10"]], "ttnn.log10_bw": [[183, "ttnn-log10-bw"]], "ttnn.log1p": [[184, "ttnn-log1p"]], "ttnn.log1p_bw": [[185, "ttnn-log1p-bw"]], "ttnn.log2": [[186, "ttnn-log2"]], "ttnn.log2_bw": [[187, "ttnn-log2-bw"]], "ttnn.log_bw": [[188, "ttnn-log-bw"]], "ttnn.log_sigmoid": [[189, "ttnn-log-sigmoid"]], "ttnn.log_sigmoid_bw": [[190, "ttnn-log-sigmoid-bw"]], "ttnn.logaddexp": [[191, "ttnn-logaddexp"]], "ttnn.logaddexp2": [[192, "ttnn-logaddexp2"]], "ttnn.logaddexp2_bw": [[193, "ttnn-logaddexp2-bw"]], "ttnn.logaddexp_bw": [[194, "ttnn-logaddexp-bw"]], "ttnn.logical_and": [[195, "ttnn-logical-and"]], "ttnn.logical_and_": [[196, "ttnn-logical-and"]], "ttnn.logical_not": [[197, "ttnn-logical-not"]], "ttnn.logical_not_": [[198, "ttnn-logical-not"]], "ttnn.logical_or": [[199, "ttnn-logical-or"]], "ttnn.logical_or_": [[200, "ttnn-logical-or"]], "ttnn.logical_xor": [[201, "ttnn-logical-xor"]], "ttnn.logical_xor_": [[202, "ttnn-logical-xor"]], "ttnn.logit": [[203, "ttnn-logit"]], "ttnn.logit_bw": [[204, "ttnn-logit-bw"]], "ttnn.logiteps_bw": [[205, "ttnn-logiteps-bw"]], "ttnn.lt": [[206, "ttnn-lt"]], "ttnn.lt_": [[207, "ttnn-lt"]], "ttnn.ltz": [[208, "ttnn-ltz"]], "ttnn.mac": [[209, "ttnn-mac"]], "ttnn.manage_device": [[210, "ttnn-manage-device"]], "ttnn.matmul": [[211, "ttnn-matmul"]], "ttnn.max": [[212, "ttnn-max"]], "ttnn.max_bw": [[213, "ttnn-max-bw"]], "ttnn.max_pool2d": [[214, "ttnn-max-pool2d"]], "ttnn.maximum": [[215, "ttnn-maximum"]], "ttnn.mean": [[216, "ttnn-mean"]], "ttnn.min": [[217, "ttnn-min"]], "ttnn.min_bw": [[218, "ttnn-min-bw"]], "ttnn.minimum": [[219, "ttnn-minimum"]], "ttnn.mish": [[220, "ttnn-mish"]], "ttnn.model_preprocessing.preprocess_model": [[221, "ttnn-model-preprocessing-preprocess-model"]], "ttnn.model_preprocessing.preprocess_model_parameters": [[222, "ttnn-model-preprocessing-preprocess-model-parameters"]], "ttnn.moreh_sum": [[223, "ttnn-moreh-sum"]], "ttnn.mse_loss": [[224, "ttnn-mse-loss"]], "ttnn.mul_bw": [[225, "ttnn-mul-bw"]], "ttnn.multigammaln": [[226, "ttnn-multigammaln"]], "ttnn.multigammaln_bw": [[227, "ttnn-multigammaln-bw"]], "ttnn.multiply": [[228, "ttnn-multiply"]], "ttnn.ne": [[229, "ttnn-ne"]], "ttnn.ne_": [[230, "ttnn-ne"]], "ttnn.neg": [[231, "ttnn-neg"]], "ttnn.neg_bw": [[232, "ttnn-neg-bw"]], "ttnn.nextafter": [[233, "ttnn-nextafter"]], "ttnn.nez": [[234, "ttnn-nez"]], "ttnn.nonzero": [[235, "ttnn-nonzero"]], "ttnn.normalize_global": [[236, "ttnn-normalize-global"]], "ttnn.normalize_hw": [[237, "ttnn-normalize-hw"]], "ttnn.ones": [[238, "ttnn-ones"]], "ttnn.ones_like": [[239, "ttnn-ones-like"]], "ttnn.open_device": [[240, "ttnn-open-device"]], "ttnn.outer": [[241, "ttnn-outer"]], "ttnn.pad": [[242, "ttnn-pad"]], "ttnn.pad_to_tile_shape": [[243, "ttnn-pad-to-tile-shape"]], "ttnn.permute": [[244, "ttnn-permute"]], "ttnn.polar": [[245, "ttnn-polar"]], "ttnn.polar_bw": [[246, "ttnn-polar-bw"]], "ttnn.polygamma": [[247, "ttnn-polygamma"]], "ttnn.polygamma_bw": [[248, "ttnn-polygamma-bw"]], "ttnn.polyval": [[249, "ttnn-polyval"]], "ttnn.pow": [[250, "ttnn-pow"]], "ttnn.pow_bw": [[251, "ttnn-pow-bw"]], "ttnn.prelu": [[252, "ttnn-prelu"]], "ttnn.prod": [[253, "ttnn-prod"]], "ttnn.prod_bw": [[254, "ttnn-prod-bw"]], "ttnn.rad2deg": [[255, "ttnn-rad2deg"]], "ttnn.rad2deg_bw": [[256, "ttnn-rad2deg-bw"]], "ttnn.rdiv": [[257, "ttnn-rdiv"]], "ttnn.rdiv_bw": [[258, "ttnn-rdiv-bw"]], "ttnn.real": [[259, "ttnn-real"]], "ttnn.real_bw": [[260, "ttnn-real-bw"]], "ttnn.reallocate": [[261, "ttnn-reallocate"]], "ttnn.reciprocal": [[262, "ttnn-reciprocal"]], "ttnn.reciprocal_bw": [[263, "ttnn-reciprocal-bw"]], "ttnn.reduce_scatter": [[264, "ttnn-reduce-scatter"]], "ttnn.register_post_operation_hook": [[265, "ttnn-register-post-operation-hook"]], "ttnn.register_pre_operation_hook": [[266, "ttnn-register-pre-operation-hook"]], "ttnn.reglu": [[267, "ttnn-reglu"]], "ttnn.relu": [[268, "ttnn-relu"]], "ttnn.relu6": [[269, "ttnn-relu6"]], "ttnn.relu6_bw": [[270, "ttnn-relu6-bw"]], "ttnn.relu_bw": [[271, "ttnn-relu-bw"]], "ttnn.relu_max": [[272, "ttnn-relu-max"]], "ttnn.relu_min": [[273, "ttnn-relu-min"]], "ttnn.remainder": [[274, "ttnn-remainder"]], "ttnn.remainder_bw": [[275, "ttnn-remainder-bw"]], "ttnn.repeat": [[276, "ttnn-repeat"]], "ttnn.repeat_bw": [[277, "ttnn-repeat-bw"]], "ttnn.repeat_interleave": [[278, "ttnn-repeat-interleave"]], "ttnn.reshape": [[279, "ttnn-reshape"]], "ttnn.rms_norm": [[280, "ttnn-rms-norm"]], "ttnn.round": [[281, "ttnn-round"]], "ttnn.round_bw": [[282, "ttnn-round-bw"]], "ttnn.rpow": [[283, "ttnn-rpow"]], "ttnn.rpow_bw": [[284, "ttnn-rpow-bw"]], "ttnn.rsqrt": [[285, "ttnn-rsqrt"]], "ttnn.rsqrt_bw": [[286, "ttnn-rsqrt-bw"]], "ttnn.rsub": [[287, "ttnn-rsub"]], "ttnn.rsub_bw": [[288, "ttnn-rsub-bw"]], "ttnn.scatter": [[289, "ttnn-scatter"]], "ttnn.selu": [[290, "ttnn-selu"]], "ttnn.selu_bw": [[291, "ttnn-selu-bw"]], "ttnn.set_printoptions": [[292, "ttnn-set-printoptions"]], "ttnn.sigmoid": [[293, "ttnn-sigmoid"]], "ttnn.sigmoid_accurate": [[294, "ttnn-sigmoid-accurate"]], "ttnn.sigmoid_bw": [[295, "ttnn-sigmoid-bw"]], "ttnn.sign": [[296, "ttnn-sign"]], "ttnn.sign_bw": [[297, "ttnn-sign-bw"]], "ttnn.signbit": [[298, "ttnn-signbit"]], "ttnn.silu": [[299, "ttnn-silu"]], "ttnn.silu_bw": [[300, "ttnn-silu-bw"]], "ttnn.sin": [[301, "ttnn-sin"]], "ttnn.sin_bw": [[302, "ttnn-sin-bw"]], "ttnn.sinh": [[303, "ttnn-sinh"]], "ttnn.sinh_bw": [[304, "ttnn-sinh-bw"]], "ttnn.slice": [[305, "ttnn-slice"]], "ttnn.softmax": [[306, "ttnn-softmax"]], "ttnn.softplus": [[307, "ttnn-softplus"]], "ttnn.softplus_bw": [[308, "ttnn-softplus-bw"]], "ttnn.softshrink": [[309, "ttnn-softshrink"]], "ttnn.softshrink_bw": [[310, "ttnn-softshrink-bw"]], "ttnn.softsign": [[311, "ttnn-softsign"]], "ttnn.softsign_bw": [[312, "ttnn-softsign-bw"]], "ttnn.sqrt": [[313, "ttnn-sqrt"]], "ttnn.sqrt_bw": [[314, "ttnn-sqrt-bw"]], "ttnn.square": [[315, "ttnn-square"]], "ttnn.square_bw": [[316, "ttnn-square-bw"]], "ttnn.squared_difference": [[317, "ttnn-squared-difference"]], "ttnn.squared_difference_bw": [[318, "ttnn-squared-difference-bw"]], "ttnn.std": [[319, "ttnn-std"]], "ttnn.sub_bw": [[320, "ttnn-sub-bw"]], "ttnn.subalpha": [[321, "ttnn-subalpha"]], "ttnn.subalpha_bw": [[322, "ttnn-subalpha-bw"]], "ttnn.subtract": [[323, "ttnn-subtract"]], "ttnn.sum": [[324, "ttnn-sum"]], "ttnn.swiglu": [[325, "ttnn-swiglu"]], "ttnn.swish": [[326, "ttnn-swish"]], "ttnn.synchronize_device": [[327, "ttnn-synchronize-device"]], "ttnn.tan": [[328, "ttnn-tan"]], "ttnn.tan_bw": [[329, "ttnn-tan-bw"]], "ttnn.tanh": [[330, "ttnn-tanh"]], "ttnn.tanh_bw": [[331, "ttnn-tanh-bw"]], "ttnn.tanhshrink": [[332, "ttnn-tanhshrink"]], "ttnn.tanhshrink_bw": [[333, "ttnn-tanhshrink-bw"]], "ttnn.threshold": [[334, "ttnn-threshold"]], "ttnn.threshold_bw": [[335, "ttnn-threshold-bw"]], "ttnn.tilize": [[336, "ttnn-tilize"]], "ttnn.tilize_with_val_padding": [[337, "ttnn-tilize-with-val-padding"]], "ttnn.to_device": [[338, "ttnn-to-device"]], "ttnn.to_layout": [[339, "ttnn-to-layout"]], "ttnn.to_memory_config": [[340, "ttnn-to-memory-config"]], "ttnn.to_torch": [[341, "ttnn-to-torch"]], "ttnn.topk": [[342, "ttnn-topk"]], "ttnn.transformer.attention_softmax": [[343, "ttnn-transformer-attention-softmax"]], "ttnn.transformer.attention_softmax_": [[344, "ttnn-transformer-attention-softmax"]], "ttnn.transformer.concatenate_heads": [[345, "ttnn-transformer-concatenate-heads"]], "ttnn.transformer.scaled_dot_product_attention": [[346, "ttnn-transformer-scaled-dot-product-attention"]], "ttnn.transformer.scaled_dot_product_attention_decode": [[347, "ttnn-transformer-scaled-dot-product-attention-decode"]], "ttnn.transformer.split_query_key_value_and_split_heads": [[348, "ttnn-transformer-split-query-key-value-and-split-heads"]], "ttnn.tril": [[349, "ttnn-tril"]], "ttnn.triu": [[350, "ttnn-triu"]], "ttnn.trunc": [[351, "ttnn-trunc"]], "ttnn.trunc_bw": [[352, "ttnn-trunc-bw"]], "ttnn.unary_chain": [[353, "ttnn-unary-chain"]], "ttnn.untilize": [[354, "ttnn-untilize"]], "ttnn.untilize_with_unpadding": [[355, "ttnn-untilize-with-unpadding"]], "ttnn.upsample": [[356, "ttnn-upsample"]], "ttnn.var": [[357, "ttnn-var"]], "ttnn.where": [[358, "ttnn-where"]], "ttnn.where_bw": [[359, "ttnn-where-bw"]], "ttnn.xlogy": [[360, "ttnn-xlogy"]], "ttnn.xlogy_bw": [[361, "ttnn-xlogy-bw"]], "ttnn.zeros": [[362, "ttnn-zeros"]], "ttnn.zeros_like": [[363, "ttnn-zeros-like"]], "Converting PyTorch Model to TT-NN": [[364, "converting-pytorch-model-to-tt-nn"]], "Step 1 - Rewriting the Model": [[364, "step-1-rewriting-the-model"]], "Step 2 - Switching to ttnn Operations": [[364, "step-2-switching-to-ttnn-operations"]], "Step 3 - Optimizing the Model": [[364, "step-3-optimizing-the-model"]], "More examples": [[364, "more-examples"]], "Building and Uplifting Demos": [[365, "building-and-uplifting-demos"]], "Examples of Tensor and TT-LIB Use": [[366, "examples-of-tensor-and-tt-lib-use"]], "Run one OP from TT-LIB on TT Accelerator device": [[366, "run-one-op-from-tt-lib-on-tt-accelerator-device"]], "Run TT-LIB and PyTorch OPs": [[366, "run-tt-lib-and-pytorch-ops"]], "Tensors with odd size of last dim": [[366, "tensors-with-odd-size-of-last-dim"]], "Dependencies": [[367, "dependencies"]], "Tensor": [[368, "tensor"], [374, "tensor"]], "Overview": [[368, "overview"], [369, "overview"]], "Tensor Storage": [[368, "tensor-storage"]], "Tensor API": [[368, "tensor-api"]], "MemoryConfig": [[368, "memoryconfig"]], "Examples of converting between PyTorch Tensor and TT Tensor": [[368, "examples-of-converting-between-pytorch-tensor-and-tt-tensor"]], "Converting a PyTorch Tensor to a TT Tensor": [[368, "converting-a-pytorch-tensor-to-a-tt-tensor"]], "Converting a TT Tensor to a PyTorch Tensor": [[368, "converting-a-tt-tensor-to-a-pytorch-tensor"]], "TT-LIB": [[369, "tt-lib"]], "Operation Infrastructure": [[369, "operation-infrastructure"]], "New Device Operation": [[369, "new-device-operation"]], "New Device Operation with a member": [[369, "new-device-operation-with-a-member"]], "New Device Operation with Optional Input Tensors": [[369, "new-device-operation-with-optional-input-tensors"]], "New Device Operation with Optional Output Tensors": [[369, "new-device-operation-with-optional-output-tensors"]], "Profiler": [[369, "profiler"]], "Fast Dispatch": [[369, "fast-dispatch"]], "Program Caching": [[369, "program-caching"]], "Logs": [[369, "logs"]], "TT-LIB API through tt_lib": [[369, "tt-lib-api-through-tt-lib"]], "Primary Operations": [[369, "primary-operations"]], "Enums": [[369, "enums"]], "Fallback Operations": [[369, "fallback-operations"]], "Experimental Operations": [[369, "experimental-operations"]], "Fused Operations from tt_lib Mini-Graph Library": [[369, "fused-operations-from-tt-lib-mini-graph-library"]], "Complex Operations (Type 2)": [[369, "complex-operations-type-2"]], "1. Install and Build": [[370, "install-and-build"]], "2. Explore Our Model Demos": [[370, "explore-our-model-demos"]], "3. TT-NN Tutorial: Multi-Head Attention (Simple)": [[370, "tt-nn-tutorial-multi-head-attention-simple"]], "4. TT-NN Tutorial: Multi-Head Attention (Optimized)": [[370, "tt-nn-tutorial-multi-head-attention-optimized"]], "Where To Go From Here": [[370, "where-to-go-from-here"]], "Install": [[371, "install"]], "Prerequisites:": [[371, "prerequisites"]], "1: Set Up the Hardware": [[371, "set-up-the-hardware"]], "2: Install Driver & Firmware": [[371, "install-driver-firmware"]], "Install System-level Dependencies": [[371, "install-system-level-dependencies"]], "Install the Driver (TT-KMD)": [[371, "install-the-driver-tt-kmd"]], "Update Device TT-Firmware with TT-Flash": [[371, "update-device-tt-firmware-with-tt-flash"]], "Install System Management Interface (TT-SMI)": [[371, "install-system-management-interface-tt-smi"]], "(Optional) Multi-Card Configuration (TT-Topology)": [[371, "optional-multi-card-configuration-tt-topology"]], "TT-NN / TT-Metalium Installation": [[371, "tt-nn-tt-metalium-installation"]], "There are three options for installing TT-Metalium:": [[371, "there-are-three-options-for-installing-tt-metalium"]], "Option 1: From Source": [[371, "option-1-from-source"]], "Step 1. Clone the Repository:": [[371, "step-1-clone-the-repository"]], "Step 2. Invoke our Build Scripts:": [[371, "step-2-invoke-our-build-scripts"]], "Option 2: From Docker Release Image": [[371, "option-2-from-docker-release-image"]], "Option 3: From Wheel": [[371, "option-3-from-wheel"]], "Step 1. Download and Install the Latest Wheel:": [[371, "step-1-download-and-install-the-latest-wheel"]], "Step 2. (For models users only) Set Up Environment for Models:": [[371, "step-2-for-models-users-only-set-up-environment-for-models"]], "You are All Set!": [[371, "you-are-all-set"]], "To verify your installation, try executing a programming example:": [[371, "to-verify-your-installation-try-executing-a-programming-example"]], "Interested in Contributing?": [[371, "interested-in-contributing"]], "Onboarding New Functionality": [[372, "onboarding-new-functionality"]], "Profiling TT-NN Operations": [[373, "profiling-tt-nn-operations"]], "Perf Report Headers": [[373, "perf-report-headers"]], "profile_this description": [[373, "profile-this-description"]], "Shape": [[374, "shape"]], "Layout": [[374, "layout"], [383, "Layout"]], "Data Type": [[374, "data-type"], [383, "Data-Type"]], "Required Width Multiples for Data Types": [[374, "id4"]], "Limitation of BFLOAT8_B": [[374, "limitation-of-bfloat8-b"]], "Storage": [[374, "storage"]], "Tensor Sharding": [[374, "tensor-sharding"]], "Tutorials": [[375, "id1"]], "Graphing Torch DiT_XL_2 With TTNN": [[376, "graphing-torch-dit-xl-2-with-ttnn"]], "Matmul Operation": [[377, "matmul-operation"]], "Multi-Head Attention": [[378, "multi-head-attention"], [385, "Multi-Head-Attention"]], "ttnn Profiling": [[379, "ttnn-profiling"]], "Resnet Basic Block": [[380, "resnet-basic-block"]], "Tensor and Add Operation": [[381, "tensor-and-add-operation"], [383, "Tensor-and-Add-Operation"]], "ttnn Tracer": [[382, "ttnn-tracer"]], "Creating a tensor": [[383, "Creating-a-tensor"]], "Host Storage: Borrowed vs Owned": [[383, "Host-Storage:-Borrowed-vs-Owned"]], "Device storage": [[383, "Device-storage"]], "Open the device": [[383, "Open-the-device"]], "Initialize tensors a and b with random values using torch": [[383, "Initialize-tensors-a-and-b-with-random-values-using-torch"], [384, "Initialize-tensors-a-and-b-with-random-values-using-torch"]], "Add tensor a and b": [[383, "Add-tensor-a-and-b"]], "Inspect the output tensor of the add in ttnn": [[383, "Inspect-the-output-tensor-of-the-add-in-ttnn"]], "Convert to torch and inspect the attributes of the torch tensor": [[383, "Convert-to-torch-and-inspect-the-attributes-of-the-torch-tensor"]], "Close the device": [[383, "Close-the-device"], [384, "Close-the-device"], [385, "Close-the-device"]], "Enable program cache": [[384, "Enable-program-cache"], [385, "Enable-program-cache"]], "Configuration": [[384, "Configuration"], [385, "Configuration"]], "Matrix multiply tensor a and b": [[384, "Matrix-multiply-tensor-a-and-b"]], "Inspect the layout of matrix multiplication output": [[384, "Inspect-the-layout-of-matrix-multiplication-output"]], "Inspect the result of the matrix multiplication": [[384, "Inspect-the-result-of-the-matrix-multiplication"]], "Matrix multiply tensor a and b by using more performant config": [[384, "Matrix-multiply-tensor-a-and-b-by-using-more-performant-config"]], "Write Multi-Head Attention using ttnn": [[385, "Write-Multi-Head-Attention-using-ttnn"]], "Initialize activations and weights using torch": [[385, "Initialize-activations-and-weights-using-torch"]], "Convert activations and weights to ttnn": [[385, "Convert-activations-and-weights-to-ttnn"]], "Run the first iteration of Multi-Head Attention": [[385, "Run-the-first-iteration-of-Multi-Head-Attention"]], "Run a subsequent iteration of Multi-Head Attention": [[385, "Run-a-subsequent-iteration-of-Multi-Head-Attention"]], "Write optimized version of Multi-Head Attention": [[385, "Write-optimized-version-of-Multi-Head-Attention"]], "Pre-process the parameters of the optimized model": [[385, "Pre-process-the-parameters-of-the-optimized-model"]], "Run the first iteration of the optimized Multi-Head Attention": [[385, "Run-the-first-iteration-of-the-optimized-Multi-Head-Attention"]], "Run a subsequent iteration of the optimized Multi-Head Attention": [[385, "Run-a-subsequent-iteration-of-the-optimized-Multi-Head-Attention"]], "Check that the output of the optimized version matches the output of the original implementation": [[385, "Check-that-the-output-of-the-optimized-version-matches-the-output-of-the-original-implementation"]], "Tracing ttnn operations and torch modules/functions": [[386, "Tracing-ttnn-operations-and-torch-modules/functions"]], "Trace torch functions": [[386, "Trace-torch-functions"]], "Trace torch functions and ttnn operations": [[386, "Trace-torch-functions-and-ttnn-operations"]], "Trace torch functions, torch modules and ttnn operations": [[386, "Trace-torch-functions,-torch-modules-and-ttnn-operations"]], "Trace models written using ttnn": [[386, "Trace-models-written-using-ttnn"]], "Profiling ttnn operations": [[387, "Profiling-ttnn-operations"]], "Resnet Block": [[388, "Resnet-Block"]], "Torch Module (from torchvision)": [[388, "Torch-Module-(from-torchvision)"]], "Create torch module and preprocess it to get ttnn parameters": [[388, "Create-torch-module-and-preprocess-it-to-get-ttnn-parameters"]], "Display the parameters of the module": [[388, "Display-the-parameters-of-the-module"]], "Display the traced torch graph": [[388, "Display-the-traced-torch-graph"]], "Implement ttnn version of the module. Pass in the parameters into the constructor.": [[388, "Implement-ttnn-version-of-the-module.-Pass-in-the-parameters-into-the-constructor."]], "Run ttnn module and display the traced graph": [[388, "Run-ttnn-module-and-display-the-traced-graph"]], "Build a graph of a pytorch based model": [[389, "Build-a-graph-of-a-pytorch-based-model"]], "Clone the library from https://github.com/facebookresearch/DiT.git": [[389, "Clone-the-library-from-https://github.com/facebookresearch/DiT.git"]], "Download DiT-XL/2 Models": [[389, "Download-DiT-XL/2-Models"]], "Sample from Pre-trained DiT Models and build the graph": [[389, "Sample-from-Pre-trained-DiT-Models-and-build-the-graph"]], "Display the graph": [[389, "Display-the-graph"]], "Using TT-NN": [[390, "using-tt-nn"]], "Basic Examples": [[390, "basic-examples"]], "1. Converting from and to torch tensor": [[390, "converting-from-and-to-torch-tensor"]], "2. Running an operation on the device": [[390, "running-an-operation-on-the-device"]], "3. Using __getitem__ to slice the tensor": [[390, "using-getitem-to-slice-the-tensor"]], "4. Enabling program cache": [[390, "enabling-program-cache"]], "5. Debugging intermediate tensors": [[390, "debugging-intermediate-tensors"]], "6. Tracing the graph of operations": [[390, "tracing-the-graph-of-operations"]], "7. Using tt_lib operation in TT-NN": [[390, "using-tt-lib-operation-in-tt-nn"]], "8. Enabling Logging": [[390, "enabling-logging"]], "9. Supported Python Operators": [[390, "supported-python-operators"]], "10. Changing the string representation of the tensor": [[390, "changing-the-string-representation-of-the-tensor"]], "11. Visualize using Web Browser": [[390, "visualize-using-web-browser"]], "12. Register pre- and/or post-operation hooks": [[390, "register-pre-and-or-post-operation-hooks"]], "13. Query all operations": [[390, "query-all-operations"]], "14. Falling back to torch": [[390, "falling-back-to-torch"]], "15. Capturing graph of C++ functions, buffer allocations, etc": [[390, "capturing-graph-of-c-functions-buffer-allocations-etc"]]}, "indexentries": {"getdefaultdevice() (in module ttnn)": [[8, "ttnn.GetDefaultDevice"]], "setdefaultdevice() (in module ttnn)": [[9, "ttnn.SetDefaultDevice"]], "abs() (in module ttnn)": [[10, "ttnn.abs"]], "abs_bw() (in module ttnn)": [[11, "ttnn.abs_bw"]], "acos() (in module ttnn)": [[12, "ttnn.acos"]], "acos_bw() (in module ttnn)": [[13, "ttnn.acos_bw"]], "acosh() (in module ttnn)": [[14, "ttnn.acosh"]], "acosh_bw() (in module ttnn)": [[15, "ttnn.acosh_bw"]], "add() (in module ttnn)": [[16, "ttnn.add"]], "add_bw() (in module ttnn)": [[17, "ttnn.add_bw"]], "addalpha() (in module ttnn)": [[18, "ttnn.addalpha"]], "addalpha_bw() (in module ttnn)": [[19, "ttnn.addalpha_bw"]], "addcdiv() (in module ttnn)": [[20, "ttnn.addcdiv"]], "addcdiv_bw() (in module ttnn)": [[21, "ttnn.addcdiv_bw"]], "addcmul() (in module ttnn)": [[22, "ttnn.addcmul"]], "addcmul_bw() (in module ttnn)": [[23, "ttnn.addcmul_bw"]], "all_gather() (in module ttnn)": [[24, "ttnn.all_gather"]], "angle() (in module ttnn)": [[25, "ttnn.angle"]], "angle_bw() (in module ttnn)": [[26, "ttnn.angle_bw"]], "arange() (in module ttnn)": [[27, "ttnn.arange"]], "argmax() (in module ttnn)": [[28, "ttnn.argmax"]], "as_tensor() (in module ttnn)": [[29, "ttnn.as_tensor"]], "asin() (in module ttnn)": [[30, "ttnn.asin"]], "asin_bw() (in module ttnn)": [[31, "ttnn.asin_bw"]], "asinh() (in module ttnn)": [[32, "ttnn.asinh"]], "asinh_bw() (in module ttnn)": [[33, "ttnn.asinh_bw"]], "assign_bw() (in module ttnn)": [[34, "ttnn.assign_bw"]], "atan() (in module ttnn)": [[35, "ttnn.atan"]], "atan2() (in module ttnn)": [[36, "ttnn.atan2"]], "atan2_bw() (in module ttnn)": [[37, "ttnn.atan2_bw"]], "atan_bw() (in module ttnn)": [[38, "ttnn.atan_bw"]], "atanh() (in module ttnn)": [[39, "ttnn.atanh"]], "atanh_bw() (in module ttnn)": [[40, "ttnn.atanh_bw"]], "batch_norm() (in module ttnn)": [[41, "ttnn.batch_norm"]], "bias_gelu_bw() (in module ttnn)": [[42, "ttnn.bias_gelu_bw"]], "bitwise_and() (in module ttnn)": [[43, "ttnn.bitwise_and"]], "bitwise_left_shift() (in module ttnn)": [[44, "ttnn.bitwise_left_shift"]], "bitwise_not() (in module ttnn)": [[45, "ttnn.bitwise_not"]], "bitwise_or() (in module ttnn)": [[46, "ttnn.bitwise_or"]], "bitwise_right_shift() (in module ttnn)": [[47, "ttnn.bitwise_right_shift"]], "bitwise_xor() (in module ttnn)": [[48, "ttnn.bitwise_xor"]], "cbrt() (in module ttnn)": [[49, "ttnn.cbrt"]], "ceil() (in module ttnn)": [[50, "ttnn.ceil"]], "ceil_bw() (in module ttnn)": [[51, "ttnn.ceil_bw"]], "celu() (in module ttnn)": [[52, "ttnn.celu"]], "celu_bw() (in module ttnn)": [[53, "ttnn.celu_bw"]], "clamp() (in module ttnn)": [[54, "ttnn.clamp"]], "clamp_bw() (in module ttnn)": [[55, "ttnn.clamp_bw"]], "clip() (in module ttnn)": [[56, "ttnn.clip"]], "clip_bw() (in module ttnn)": [[57, "ttnn.clip_bw"]], "clone() (in module ttnn)": [[58, "ttnn.clone"]], "close_device() (in module ttnn)": [[59, "ttnn.close_device"]], "concat() (in module ttnn)": [[60, "ttnn.concat"]], "concat_bw() (in module ttnn)": [[61, "ttnn.concat_bw"]], "conj() (in module ttnn)": [[62, "ttnn.conj"]], "conj_bw() (in module ttnn)": [[63, "ttnn.conj_bw"]], "cos() (in module ttnn)": [[64, "ttnn.cos"]], "cos_bw() (in module ttnn)": [[65, "ttnn.cos_bw"]], "cosh() (in module ttnn)": [[66, "ttnn.cosh"]], "cosh_bw() (in module ttnn)": [[67, "ttnn.cosh_bw"]], "create_sharded_memory_config() (in module ttnn)": [[68, "ttnn.create_sharded_memory_config"]], "deallocate() (in module ttnn)": [[69, "ttnn.deallocate"]], "deg2rad() (in module ttnn)": [[70, "ttnn.deg2rad"]], "deg2rad_bw() (in module ttnn)": [[71, "ttnn.deg2rad_bw"]], "digamma() (in module ttnn)": [[72, "ttnn.digamma"]], "digamma_bw() (in module ttnn)": [[73, "ttnn.digamma_bw"]], "div() (in module ttnn)": [[74, "ttnn.div"]], "div_bw() (in module ttnn)": [[75, "ttnn.div_bw"]], "div_no_nan() (in module ttnn)": [[76, "ttnn.div_no_nan"]], "div_no_nan_bw() (in module ttnn)": [[77, "ttnn.div_no_nan_bw"]], "downsample() (in module ttnn)": [[78, "ttnn.downsample"]], "dump_tensor() (in module ttnn)": [[79, "ttnn.dump_tensor"]], "elu() (in module ttnn)": [[80, "ttnn.elu"]], "elu_bw() (in module ttnn)": [[81, "ttnn.elu_bw"]], "embedding() (in module ttnn)": [[82, "ttnn.embedding"]], "embedding_bw() (in module ttnn)": [[83, "ttnn.embedding_bw"]], "empty() (in module ttnn)": [[84, "ttnn.empty"]], "empty_like() (in module ttnn)": [[85, "ttnn.empty_like"]], "eq() (in module ttnn)": [[86, "ttnn.eq"]], "eq_() (in module ttnn)": [[87, "ttnn.eq_"]], "eqz() (in module ttnn)": [[88, "ttnn.eqz"]], "erf() (in module ttnn)": [[89, "ttnn.erf"]], "erf_bw() (in module ttnn)": [[90, "ttnn.erf_bw"]], "erfc() (in module ttnn)": [[91, "ttnn.erfc"]], "erfc_bw() (in module ttnn)": [[92, "ttnn.erfc_bw"]], "erfinv() (in module ttnn)": [[93, "ttnn.erfinv"]], "erfinv_bw() (in module ttnn)": [[94, "ttnn.erfinv_bw"]], "exp() (in module ttnn)": [[95, "ttnn.exp"]], "exp2() (in module ttnn)": [[96, "ttnn.exp2"]], "exp2_bw() (in module ttnn)": [[97, "ttnn.exp2_bw"]], "exp_bw() (in module ttnn)": [[98, "ttnn.exp_bw"]], "all_reduce() (in module ttnn.experimental)": [[99, "ttnn.experimental.all_reduce"]], "cumprod() (in module ttnn.experimental)": [[100, "ttnn.experimental.cumprod"]], "dropout() (in module ttnn.experimental)": [[101, "ttnn.experimental.dropout"]], "gelu_bw() (in module ttnn.experimental)": [[102, "ttnn.experimental.gelu_bw"]], "rotary_embedding() (in module ttnn.experimental)": [[103, "ttnn.experimental.rotary_embedding"]], "sort() (in module ttnn.experimental)": [[104, "ttnn.experimental.sort"]], "expm1() (in module ttnn)": [[105, "ttnn.expm1"]], "expm1_bw() (in module ttnn)": [[106, "ttnn.expm1_bw"]], "fill() (in module ttnn)": [[107, "ttnn.fill"]], "fill_bw() (in module ttnn)": [[108, "ttnn.fill_bw"]], "fill_ones_rm() (in module ttnn)": [[109, "ttnn.fill_ones_rm"]], "fill_rm() (in module ttnn)": [[110, "ttnn.fill_rm"]], "fill_zero_bw() (in module ttnn)": [[111, "ttnn.fill_zero_bw"]], "floor() (in module ttnn)": [[112, "ttnn.floor"]], "floor_bw() (in module ttnn)": [[113, "ttnn.floor_bw"]], "floor_div() (in module ttnn)": [[114, "ttnn.floor_div"]], "fmod() (in module ttnn)": [[115, "ttnn.fmod"]], "fmod_bw() (in module ttnn)": [[116, "ttnn.fmod_bw"]], "format_input_tensor() (in module ttnn)": [[117, "ttnn.format_input_tensor"]], "format_output_tensor() (in module ttnn)": [[118, "ttnn.format_output_tensor"]], "frac() (in module ttnn)": [[119, "ttnn.frac"]], "frac_bw() (in module ttnn)": [[120, "ttnn.frac_bw"]], "from_device() (in module ttnn)": [[121, "ttnn.from_device"]], "from_torch() (in module ttnn)": [[122, "ttnn.from_torch"]], "full() (in module ttnn)": [[123, "ttnn.full"]], "full_like() (in module ttnn)": [[124, "ttnn.full_like"]], "gcd() (in module ttnn)": [[125, "ttnn.gcd"]], "ge() (in module ttnn)": [[126, "ttnn.ge"]], "ge_() (in module ttnn)": [[127, "ttnn.ge_"]], "geglu() (in module ttnn)": [[128, "ttnn.geglu"]], "gelu() (in module ttnn)": [[129, "ttnn.gelu"]], "gelu_bw() (in module ttnn)": [[130, "ttnn.gelu_bw"]], "gez() (in module ttnn)": [[131, "ttnn.gez"]], "global_avg_pool2d() (in module ttnn)": [[132, "ttnn.global_avg_pool2d"]], "glu() (in module ttnn)": [[133, "ttnn.glu"]], "group_norm() (in module ttnn)": [[134, "ttnn.group_norm"]], "gt() (in module ttnn)": [[135, "ttnn.gt"]], "gt_() (in module ttnn)": [[136, "ttnn.gt_"]], "gtz() (in module ttnn)": [[137, "ttnn.gtz"]], "hardshrink() (in module ttnn)": [[138, "ttnn.hardshrink"]], "hardshrink_bw() (in module ttnn)": [[139, "ttnn.hardshrink_bw"]], "hardsigmoid() (in module ttnn)": [[140, "ttnn.hardsigmoid"]], "hardsigmoid_bw() (in module ttnn)": [[141, "ttnn.hardsigmoid_bw"]], "hardswish() (in module ttnn)": [[142, "ttnn.hardswish"]], "hardswish_bw() (in module ttnn)": [[143, "ttnn.hardswish_bw"]], "hardtanh() (in module ttnn)": [[144, "ttnn.hardtanh"]], "hardtanh_bw() (in module ttnn)": [[145, "ttnn.hardtanh_bw"]], "heaviside() (in module ttnn)": [[146, "ttnn.heaviside"]], "hypot() (in module ttnn)": [[147, "ttnn.hypot"]], "hypot_bw() (in module ttnn)": [[148, "ttnn.hypot_bw"]], "i0() (in module ttnn)": [[149, "ttnn.i0"]], "i0_bw() (in module ttnn)": [[150, "ttnn.i0_bw"]], "identity() (in module ttnn)": [[151, "ttnn.identity"]], "imag() (in module ttnn)": [[152, "ttnn.imag"]], "imag_bw() (in module ttnn)": [[153, "ttnn.imag_bw"]], "indexed_fill() (in module ttnn)": [[154, "ttnn.indexed_fill"]], "is_imag() (in module ttnn)": [[155, "ttnn.is_imag"]], "is_real() (in module ttnn)": [[156, "ttnn.is_real"]], "isclose() (in module ttnn)": [[157, "ttnn.isclose"]], "isfinite() (in module ttnn)": [[158, "ttnn.isfinite"]], "isinf() (in module ttnn)": [[159, "ttnn.isinf"]], "isnan() (in module ttnn)": [[160, "ttnn.isnan"]], "isneginf() (in module ttnn)": [[161, "ttnn.isneginf"]], "isposinf() (in module ttnn)": [[162, "ttnn.isposinf"]], "fill_cache_for_user_() (in module ttnn.kv_cache)": [[163, "ttnn.kv_cache.fill_cache_for_user_"]], "update_cache_for_token_() (in module ttnn.kv_cache)": [[164, "ttnn.kv_cache.update_cache_for_token_"]], "l1_loss() (in module ttnn)": [[165, "ttnn.l1_loss"]], "layer_norm() (in module ttnn)": [[166, "ttnn.layer_norm"]], "lcm() (in module ttnn)": [[167, "ttnn.lcm"]], "ldexp() (in module ttnn)": [[168, "ttnn.ldexp"]], "ldexp_bw() (in module ttnn)": [[169, "ttnn.ldexp_bw"]], "le() (in module ttnn)": [[170, "ttnn.le"]], "le_() (in module ttnn)": [[171, "ttnn.le_"]], "leaky_relu() (in module ttnn)": [[172, "ttnn.leaky_relu"]], "leaky_relu_bw() (in module ttnn)": [[173, "ttnn.leaky_relu_bw"]], "lerp() (in module ttnn)": [[174, "ttnn.lerp"]], "lerp_bw() (in module ttnn)": [[175, "ttnn.lerp_bw"]], "lez() (in module ttnn)": [[176, "ttnn.lez"]], "lgamma() (in module ttnn)": [[177, "ttnn.lgamma"]], "lgamma_bw() (in module ttnn)": [[178, "ttnn.lgamma_bw"]], "linear() (in module ttnn)": [[179, "ttnn.linear"]], "load_tensor() (in module ttnn)": [[180, "ttnn.load_tensor"]], "log() (in module ttnn)": [[181, "ttnn.log"]], "log10() (in module ttnn)": [[182, "ttnn.log10"]], "log10_bw() (in module ttnn)": [[183, "ttnn.log10_bw"]], "log1p() (in module ttnn)": [[184, "ttnn.log1p"]], "log1p_bw() (in module ttnn)": [[185, "ttnn.log1p_bw"]], "log2() (in module ttnn)": [[186, "ttnn.log2"]], "log2_bw() (in module ttnn)": [[187, "ttnn.log2_bw"]], "log_bw() (in module ttnn)": [[188, "ttnn.log_bw"]], "log_sigmoid() (in module ttnn)": [[189, "ttnn.log_sigmoid"]], "log_sigmoid_bw() (in module ttnn)": [[190, "ttnn.log_sigmoid_bw"]], "logaddexp() (in module ttnn)": [[191, "ttnn.logaddexp"]], "logaddexp2() (in module ttnn)": [[192, "ttnn.logaddexp2"]], "logaddexp2_bw() (in module ttnn)": [[193, "ttnn.logaddexp2_bw"]], "logaddexp_bw() (in module ttnn)": [[194, "ttnn.logaddexp_bw"]], "logical_and() (in module ttnn)": [[195, "ttnn.logical_and"]], "logical_and_() (in module ttnn)": [[196, "ttnn.logical_and_"]], "logical_not() (in module ttnn)": [[197, "ttnn.logical_not"]], "logical_not_() (in module ttnn)": [[198, "ttnn.logical_not_"]], "logical_or() (in module ttnn)": [[199, "ttnn.logical_or"]], "logical_or_() (in module ttnn)": [[200, "ttnn.logical_or_"]], "logical_xor() (in module ttnn)": [[201, "ttnn.logical_xor"]], "logical_xor_() (in module ttnn)": [[202, "ttnn.logical_xor_"]], "logit() (in module ttnn)": [[203, "ttnn.logit"]], "logit_bw() (in module ttnn)": [[204, "ttnn.logit_bw"]], "logiteps_bw() (in module ttnn)": [[205, "ttnn.logiteps_bw"]], "lt() (in module ttnn)": [[206, "ttnn.lt"]], "lt_() (in module ttnn)": [[207, "ttnn.lt_"]], "ltz() (in module ttnn)": [[208, "ttnn.ltz"]], "mac() (in module ttnn)": [[209, "ttnn.mac"]], "manage_device() (in module ttnn)": [[210, "ttnn.manage_device"]], "matmul() (in module ttnn)": [[211, "ttnn.matmul"]], "max() (in module ttnn)": [[212, "ttnn.max"]], "max_bw() (in module ttnn)": [[213, "ttnn.max_bw"]], "max_pool2d() (in module ttnn)": [[214, "ttnn.max_pool2d"]], "maximum() (in module ttnn)": [[215, "ttnn.maximum"]], "mean() (in module ttnn)": [[216, "ttnn.mean"]], "min() (in module ttnn)": [[217, "ttnn.min"]], "min_bw() (in module ttnn)": [[218, "ttnn.min_bw"]], "minimum() (in module ttnn)": [[219, "ttnn.minimum"]], "mish() (in module ttnn)": [[220, "ttnn.mish"]], "preprocess_model() (in module ttnn.model_preprocessing)": [[221, "ttnn.model_preprocessing.preprocess_model"]], "preprocess_model_parameters() (in module ttnn.model_preprocessing)": [[222, "ttnn.model_preprocessing.preprocess_model_parameters"]], "moreh_sum() (in module ttnn)": [[223, "ttnn.moreh_sum"]], "mse_loss() (in module ttnn)": [[224, "ttnn.mse_loss"]], "mul_bw() (in module ttnn)": [[225, "ttnn.mul_bw"]], "multigammaln() (in module ttnn)": [[226, "ttnn.multigammaln"]], "multigammaln_bw() (in module ttnn)": [[227, "ttnn.multigammaln_bw"]], "multiply() (in module ttnn)": [[228, "ttnn.multiply"]], "ne() (in module ttnn)": [[229, "ttnn.ne"]], "ne_() (in module ttnn)": [[230, "ttnn.ne_"]], "neg() (in module ttnn)": [[231, "ttnn.neg"]], "neg_bw() (in module ttnn)": [[232, "ttnn.neg_bw"]], "nextafter() (in module ttnn)": [[233, "ttnn.nextafter"]], "nez() (in module ttnn)": [[234, "ttnn.nez"]], "nonzero() (in module ttnn)": [[235, "ttnn.nonzero"]], "normalize_global() (in module ttnn)": [[236, "ttnn.normalize_global"]], "normalize_hw() (in module ttnn)": [[237, "ttnn.normalize_hw"]], "ones() (in module ttnn)": [[238, "ttnn.ones"]], "ones_like() (in module ttnn)": [[239, "ttnn.ones_like"]], "open_device() (in module ttnn)": [[240, "ttnn.open_device"]], "outer() (in module ttnn)": [[241, "ttnn.outer"]], "pad() (in module ttnn)": [[242, "ttnn.pad"]], "pad_to_tile_shape() (in module ttnn)": [[243, "ttnn.pad_to_tile_shape"]], "permute() (in module ttnn)": [[244, "ttnn.permute"]], "polar() (in module ttnn)": [[245, "ttnn.polar"]], "polar_bw() (in module ttnn)": [[246, "ttnn.polar_bw"]], "polygamma() (in module ttnn)": [[247, "ttnn.polygamma"]], "polygamma_bw() (in module ttnn)": [[248, "ttnn.polygamma_bw"]], "polyval() (in module ttnn)": [[249, "ttnn.polyval"]], "pow() (in module ttnn)": [[250, "ttnn.pow"]], "pow_bw() (in module ttnn)": [[251, "ttnn.pow_bw"]], "prelu() (in module ttnn)": [[252, "ttnn.prelu"]], "prod() (in module ttnn)": [[253, "ttnn.prod"]], "prod_bw() (in module ttnn)": [[254, "ttnn.prod_bw"]], "rad2deg() (in module ttnn)": [[255, "ttnn.rad2deg"]], "rad2deg_bw() (in module ttnn)": [[256, "ttnn.rad2deg_bw"]], "rdiv() (in module ttnn)": [[257, "ttnn.rdiv"]], "rdiv_bw() (in module ttnn)": [[258, "ttnn.rdiv_bw"]], "real() (in module ttnn)": [[259, "ttnn.real"]], "real_bw() (in module ttnn)": [[260, "ttnn.real_bw"]], "reallocate() (in module ttnn)": [[261, "ttnn.reallocate"]], "reciprocal() (in module ttnn)": [[262, "ttnn.reciprocal"]], "reciprocal_bw() (in module ttnn)": [[263, "ttnn.reciprocal_bw"]], "reduce_scatter() (in module ttnn)": [[264, "ttnn.reduce_scatter"]], "register_post_operation_hook() (in module ttnn)": [[265, "ttnn.register_post_operation_hook"]], "register_pre_operation_hook() (in module ttnn)": [[266, "ttnn.register_pre_operation_hook"]], "reglu() (in module ttnn)": [[267, "ttnn.reglu"]], "relu() (in module ttnn)": [[268, "ttnn.relu"]], "relu6() (in module ttnn)": [[269, "ttnn.relu6"]], "relu6_bw() (in module ttnn)": [[270, "ttnn.relu6_bw"]], "relu_bw() (in module ttnn)": [[271, "ttnn.relu_bw"]], "relu_max() (in module ttnn)": [[272, "ttnn.relu_max"]], "relu_min() (in module ttnn)": [[273, "ttnn.relu_min"]], "remainder() (in module ttnn)": [[274, "ttnn.remainder"]], "remainder_bw() (in module ttnn)": [[275, "ttnn.remainder_bw"]], "repeat() (in module ttnn)": [[276, "ttnn.repeat"]], "repeat_bw() (in module ttnn)": [[277, "ttnn.repeat_bw"]], "repeat_interleave() (in module ttnn)": [[278, "ttnn.repeat_interleave"]], "reshape() (in module ttnn)": [[279, "ttnn.reshape"]], "rms_norm() (in module ttnn)": [[280, "ttnn.rms_norm"]], "round() (in module ttnn)": [[281, "ttnn.round"]], "round_bw() (in module ttnn)": [[282, "ttnn.round_bw"]], "rpow() (in module ttnn)": [[283, "ttnn.rpow"]], "rpow_bw() (in module ttnn)": [[284, "ttnn.rpow_bw"]], "rsqrt() (in module ttnn)": [[285, "ttnn.rsqrt"]], "rsqrt_bw() (in module ttnn)": [[286, "ttnn.rsqrt_bw"]], "rsub() (in module ttnn)": [[287, "ttnn.rsub"]], "rsub_bw() (in module ttnn)": [[288, "ttnn.rsub_bw"]], "scatter() (in module ttnn)": [[289, "ttnn.scatter"]], "selu() (in module ttnn)": [[290, "ttnn.selu"]], "selu_bw() (in module ttnn)": [[291, "ttnn.selu_bw"]], "set_printoptions() (in module ttnn)": [[292, "ttnn.set_printoptions"]], "sigmoid() (in module ttnn)": [[293, "ttnn.sigmoid"]], "sigmoid_accurate() (in module ttnn)": [[294, "ttnn.sigmoid_accurate"]], "sigmoid_bw() (in module ttnn)": [[295, "ttnn.sigmoid_bw"]], "sign() (in module ttnn)": [[296, "ttnn.sign"]], "sign_bw() (in module ttnn)": [[297, "ttnn.sign_bw"]], "signbit() (in module ttnn)": [[298, "ttnn.signbit"]], "silu() (in module ttnn)": [[299, "ttnn.silu"]], "silu_bw() (in module ttnn)": [[300, "ttnn.silu_bw"]], "sin() (in module ttnn)": [[301, "ttnn.sin"]], "sin_bw() (in module ttnn)": [[302, "ttnn.sin_bw"]], "sinh() (in module ttnn)": [[303, "ttnn.sinh"]], "sinh_bw() (in module ttnn)": [[304, "ttnn.sinh_bw"]], "slice() (in module ttnn)": [[305, "ttnn.slice"]], "softmax() (in module ttnn)": [[306, "ttnn.softmax"]], "softplus() (in module ttnn)": [[307, "ttnn.softplus"]], "softplus_bw() (in module ttnn)": [[308, "ttnn.softplus_bw"]], "softshrink() (in module ttnn)": [[309, "ttnn.softshrink"]], "softshrink_bw() (in module ttnn)": [[310, "ttnn.softshrink_bw"]], "softsign() (in module ttnn)": [[311, "ttnn.softsign"]], "softsign_bw() (in module ttnn)": [[312, "ttnn.softsign_bw"]], "sqrt() (in module ttnn)": [[313, "ttnn.sqrt"]], "sqrt_bw() (in module ttnn)": [[314, "ttnn.sqrt_bw"]], "square() (in module ttnn)": [[315, "ttnn.square"]], "square_bw() (in module ttnn)": [[316, "ttnn.square_bw"]], "squared_difference() (in module ttnn)": [[317, "ttnn.squared_difference"]], "squared_difference_bw() (in module ttnn)": [[318, "ttnn.squared_difference_bw"]], "std() (in module ttnn)": [[319, "ttnn.std"]], "sub_bw() (in module ttnn)": [[320, "ttnn.sub_bw"]], "subalpha() (in module ttnn)": [[321, "ttnn.subalpha"]], "subalpha_bw() (in module ttnn)": [[322, "ttnn.subalpha_bw"]], "subtract() (in module ttnn)": [[323, "ttnn.subtract"]], "sum() (in module ttnn)": [[324, "ttnn.sum"]], "swiglu() (in module ttnn)": [[325, "ttnn.swiglu"]], "swish() (in module ttnn)": [[326, "ttnn.swish"]], "synchronize_device() (in module ttnn)": [[327, "ttnn.synchronize_device"]], "tan() (in module ttnn)": [[328, "ttnn.tan"]], "tan_bw() (in module ttnn)": [[329, "ttnn.tan_bw"]], "tanh() (in module ttnn)": [[330, "ttnn.tanh"]], "tanh_bw() (in module ttnn)": [[331, "ttnn.tanh_bw"]], "tanhshrink() (in module ttnn)": [[332, "ttnn.tanhshrink"]], "tanhshrink_bw() (in module ttnn)": [[333, "ttnn.tanhshrink_bw"]], "threshold() (in module ttnn)": [[334, "ttnn.threshold"]], "threshold_bw() (in module ttnn)": [[335, "ttnn.threshold_bw"]], "tilize() (in module ttnn)": [[336, "ttnn.tilize"]], "tilize_with_val_padding() (in module ttnn)": [[337, "ttnn.tilize_with_val_padding"]], "to_device() (in module ttnn)": [[338, "ttnn.to_device"]], "to_layout() (in module ttnn)": [[339, "ttnn.to_layout"]], "to_memory_config() (in module ttnn)": [[340, "ttnn.to_memory_config"]], "to_torch() (in module ttnn)": [[341, "ttnn.to_torch"]], "topk() (in module ttnn)": [[342, "ttnn.topk"]], "attention_softmax() (in module ttnn.transformer)": [[343, "ttnn.transformer.attention_softmax"]], "attention_softmax_() (in module ttnn.transformer)": [[344, "ttnn.transformer.attention_softmax_"]], "concatenate_heads() (in module ttnn.transformer)": [[345, "ttnn.transformer.concatenate_heads"]], "scaled_dot_product_attention() (in module ttnn.transformer)": [[346, "ttnn.transformer.scaled_dot_product_attention"]], "scaled_dot_product_attention_decode() (in module ttnn.transformer)": [[347, "ttnn.transformer.scaled_dot_product_attention_decode"]], "split_query_key_value_and_split_heads() (in module ttnn.transformer)": [[348, "ttnn.transformer.split_query_key_value_and_split_heads"]], "tril() (in module ttnn)": [[349, "ttnn.tril"]], "triu() (in module ttnn)": [[350, "ttnn.triu"]], "trunc() (in module ttnn)": [[351, "ttnn.trunc"]], "trunc_bw() (in module ttnn)": [[352, "ttnn.trunc_bw"]], "unary_chain() (in module ttnn)": [[353, "ttnn.unary_chain"]], "untilize() (in module ttnn)": [[354, "ttnn.untilize"]], "untilize_with_unpadding() (in module ttnn)": [[355, "ttnn.untilize_with_unpadding"]], "upsample() (in module ttnn)": [[356, "ttnn.upsample"]], "var() (in module ttnn)": [[357, "ttnn.var"]], "where() (in module ttnn)": [[358, "ttnn.where"]], "where_bw() (in module ttnn)": [[359, "ttnn.where_bw"]], "xlogy() (in module ttnn)": [[360, "ttnn.xlogy"]], "xlogy_bw() (in module ttnn)": [[361, "ttnn.xlogy_bw"]], "zeros() (in module ttnn)": [[362, "ttnn.zeros"]], "zeros_like() (in module ttnn)": [[363, "ttnn.zeros_like"]], "memoryconfig (class in ttnn)": [[368, "ttnn.MemoryConfig"]], "tensor (class in ttnn)": [[368, "ttnn.Tensor"]], "__init__() (ttnn.memoryconfig method)": [[368, "ttnn.MemoryConfig.__init__"]], "__init__() (ttnn.tensor method)": [[368, "ttnn.Tensor.__init__"]], "buffer() (ttnn.tensor method)": [[368, "ttnn.Tensor.buffer"]], "device() (ttnn.tensor method)": [[368, "ttnn.Tensor.device"]], "get_dtype() (ttnn.tensor method)": [[368, "ttnn.Tensor.get_dtype"]], "get_layout() (ttnn.tensor method)": [[368, "ttnn.Tensor.get_layout"]], "pad() (ttnn.tensor method)": [[368, "ttnn.Tensor.pad"]], "pad_to_tile() (ttnn.tensor method)": [[368, "ttnn.Tensor.pad_to_tile"]], "storage_type() (ttnn.tensor method)": [[368, "ttnn.Tensor.storage_type"]], "to() (ttnn.tensor method)": [[368, "ttnn.Tensor.to"]], "unpad() (ttnn.tensor method)": [[368, "ttnn.Tensor.unpad"]], "unpad_from_tile() (ttnn.tensor method)": [[368, "ttnn.Tensor.unpad_from_tile"]], "adaptiveavgpool2d (class in tt_lib.fallback_ops)": [[369, "tt_lib.fallback_ops.AdaptiveAvgPool2d"]], "addandnorm() (in module tt_lib.fused_ops.add_and_norm)": [[369, "tt_lib.fused_ops.add_and_norm.AddAndNorm"]], "batchnorm2d (class in tt_lib.fallback_ops)": [[369, "tt_lib.fallback_ops.BatchNorm2d"]], "bcastopdim (class in ttnn)": [[369, "ttnn.BcastOpDim"]], "bcastopmath (class in ttnn)": [[369, "ttnn.BcastOpMath"]], "conv2d (class in tt_lib.fallback_ops)": [[369, "tt_lib.fallback_ops.Conv2d"]], "groupnorm (class in tt_lib.fallback_ops)": [[369, "tt_lib.fallback_ops.GroupNorm"]], "layernorm (class in tt_lib.fallback_ops)": [[369, "tt_lib.fallback_ops.LayerNorm"]], "layernorm() (in module tt_lib.fused_ops.layernorm)": [[369, "tt_lib.fused_ops.layernorm.Layernorm"]], "linear() (in module tt_lib.fused_ops.linear)": [[369, "tt_lib.fused_ops.linear.Linear"]], "maxpool2d (class in tt_lib.fallback_ops)": [[369, "tt_lib.fallback_ops.MaxPool2d"]], "binary_bitwise_and (class in tt_lib.fallback_ops)": [[369, "tt_lib.fallback_ops.binary_bitwise_and"]], "binary_bitwise_left_shift (class in tt_lib.fallback_ops)": [[369, "tt_lib.fallback_ops.binary_bitwise_left_shift"]], "binary_bitwise_or (class in tt_lib.fallback_ops)": [[369, "tt_lib.fallback_ops.binary_bitwise_or"]], "binary_bitwise_right_shift (class in tt_lib.fallback_ops)": [[369, "tt_lib.fallback_ops.binary_bitwise_right_shift"]], "binary_bitwise_xor (class in tt_lib.fallback_ops)": [[369, "tt_lib.fallback_ops.binary_bitwise_xor"]], "binary_fmod (class in tt_lib.fallback_ops)": [[369, "tt_lib.fallback_ops.binary_fmod"]], "bitwise_not (class in tt_lib.fallback_ops)": [[369, "tt_lib.fallback_ops.bitwise_not"]], "ceil (class in tt_lib.fallback_ops)": [[369, "tt_lib.fallback_ops.ceil"]], "chunk() (in module tt_lib.fallback_ops)": [[369, "tt_lib.fallback_ops.chunk"]], "concat() (in module tt_lib.fallback_ops)": [[369, "tt_lib.fallback_ops.concat"]], "conv2d() (in module tt_lib.fallback_ops)": [[369, "tt_lib.fallback_ops.conv2d"]], "cpu() (in module ttnn.tensor)": [[369, "ttnn.Tensor.cpu"]], "floor (class in tt_lib.fallback_ops)": [[369, "tt_lib.fallback_ops.floor"]], "full() (in module tt_lib.fallback_ops)": [[369, "tt_lib.fallback_ops.full"]], "group_norm() (in module tt_lib.fallback_ops)": [[369, "tt_lib.fallback_ops.group_norm"]], "group_norm() (in module ttnn.operations.moreh)": [[369, "ttnn.operations.moreh.group_norm"]], "group_norm_backward() (in module ttnn.operations.moreh)": [[369, "ttnn.operations.moreh.group_norm_backward"]], "interpolate() (in module tt_lib.fallback_ops)": [[369, "tt_lib.fallback_ops.interpolate"]], "layer_norm() (in module tt_lib.fallback_ops)": [[369, "tt_lib.fallback_ops.layer_norm"]], "logsoftmax() (in module ttnn.operations.moreh)": [[369, "ttnn.operations.moreh.logsoftmax"]], "logsoftmax_backward() (in module ttnn.operations.moreh)": [[369, "ttnn.operations.moreh.logsoftmax_backward"]], "mean() (in module ttnn.operations.moreh)": [[369, "ttnn.operations.moreh.mean"]], "mean_backward() (in module ttnn.operations.moreh)": [[369, "ttnn.operations.moreh.mean_backward"]], "norm() (in module ttnn.operations.moreh)": [[369, "ttnn.operations.moreh.norm"]], "norm_backward() (in module ttnn.operations.moreh)": [[369, "ttnn.operations.moreh.norm_backward"]], "pad() (in module tt_lib.fallback_ops)": [[369, "tt_lib.fallback_ops.pad"]], "repeat() (in module tt_lib.fallback_ops)": [[369, "tt_lib.fallback_ops.repeat"]], "repeat_interleave() (in module tt_lib.fallback_ops)": [[369, "tt_lib.fallback_ops.repeat_interleave"]], "reshape() (in module tt_lib.fallback_ops)": [[369, "tt_lib.fallback_ops.reshape"]], "silu() (in module tt_lib.fallback_ops)": [[369, "tt_lib.fallback_ops.silu"]], "softmax() (in module tt_lib.fallback_ops)": [[369, "tt_lib.fallback_ops.softmax"]], "softmax() (in module ttnn.operations.moreh)": [[369, "ttnn.operations.moreh.softmax"]], "softmax_backward() (in module ttnn.operations.moreh)": [[369, "ttnn.operations.moreh.softmax_backward"]], "softmin() (in module ttnn.operations.moreh)": [[369, "ttnn.operations.moreh.softmin"]], "softmin_backward() (in module ttnn.operations.moreh)": [[369, "ttnn.operations.moreh.softmin_backward"]], "tensor_slice() (in module tt_lib.fallback_ops)": [[369, "tt_lib.fallback_ops.tensor_slice"]], "torch_argmax (class in tt_lib.fallback_ops)": [[369, "tt_lib.fallback_ops.torch_argmax"]], "torch_argmin (class in tt_lib.fallback_ops)": [[369, "tt_lib.fallback_ops.torch_argmin"]], "trunc (class in tt_lib.fallback_ops)": [[369, "tt_lib.fallback_ops.trunc"]], "unary_bitwise_and (class in tt_lib.fallback_ops)": [[369, "tt_lib.fallback_ops.unary_bitwise_and"]], "unary_bitwise_left_shift (class in tt_lib.fallback_ops)": [[369, "tt_lib.fallback_ops.unary_bitwise_left_shift"]], "unary_bitwise_or (class in tt_lib.fallback_ops)": [[369, "tt_lib.fallback_ops.unary_bitwise_or"]], "unary_bitwise_right_shift (class in tt_lib.fallback_ops)": [[369, "tt_lib.fallback_ops.unary_bitwise_right_shift"]], "unary_bitwise_xor (class in tt_lib.fallback_ops)": [[369, "tt_lib.fallback_ops.unary_bitwise_xor"]], "unary_fmod (class in tt_lib.fallback_ops)": [[369, "tt_lib.fallback_ops.unary_fmod"]], "shape (class in ttnn)": [[374, "ttnn.Shape"]], "rank (ttnn.shape property)": [[374, "ttnn.Shape.rank"]], "to_rank() (ttnn.shape method)": [[374, "ttnn.Shape.to_rank"]]}})